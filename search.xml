<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[如何优雅的落地一件事]]></title>
    <url>%2Fmanagement-execute-plan%2F</url>
    <content type="text"><![CDATA[PORTRAIT OF LION STANDING IN WIND 在工作中，经常会遇见上级交代的事情，布置下去底下没有执行；自己定好的方案，交给别人执行得不够好每当这个时候，需要反思一下 目标是什么只有理解了目标，才能知道这个方案的好坏，如果不能说服自己这个方案是达成目标的好方案，自然也没法去说服别人，落地效果当然大打折扣。 所以目标是一切的起点 亲手打样明晰了目标，确认好方案后，就需要亲手打样了 打样，是东北话，在东北农村里手工做鞋时都会先照着双脚的大小在纸上描个样子，然后把描着脚样子的纸和鞋底子的料粘在一起，照着纸上的样子纳鞋底，我们把在纸上描脚的样子叫“打样”。打样也就是开模具：设计出了产品，必须按照定型的产品开个模具，才能大规模生产。打样在具体实施的时候大致分为三步： 第一步“亲自抓”打样是对已经经得住推演的方案进行小规模验证和完善，这个过程是一个巨大的挑战。任何方案在实施过程中都会遇到计划时没有预见到的问题，需要随机应变解决问题。如果方案是向东，但是向东的路上有山有水有沟有坎，就必须有人逢山开路、遇水搭桥，如果能够开辟出道路、搭起桥梁，此路就走得通，否则此路就是走不通的。而能否开辟出道路以及能否搭得起桥梁和负责人的水平密切相关，也和能够调动的资源密切相关。而你作为方案得 owner 代表的是组织中对资源的最大调动能力，是最有可能解决问题的。如果不是你亲自负责而是派出组员去打样，很大概率就是无法落地了 第二步“写菜谱”一个高明的大厨如何能够教会很多人成为高明的厨师呢？核心是写出菜谱，标明：原材料是什么，每种原材料分量多少，切成什么形状，做菜的时候，先放什么后放什么，火候要多久，何时添加调料，添加什么添加多少。这就是菜谱。有了菜谱，任何厨师都可以做出和大厨一样的饭菜。而打样，就是一个写菜谱的过程。大厨按照自己的理解把一桌菜做出来，看看味道是否符合预期，如果符合，再把如何做的过程写成菜谱；如果不符合，就重新做，调整做法，直到做出符合预期的味道，然后把如何做的过程写成菜谱。任何一个方案，如果打样成功了，一定要像写菜谱一样写出执行案”，把成功的战法定性、定量，写出操作手册，以便推广。 第三步“复制”打样的目的是为了复制，打样只是工作的开始不是结束，一旦打样 成功，就要复制 以身作则“亲自打样”后，你的方案看起来开始执行了，这个时候千万不能掉以轻心，你还需要做到“以身作则”。因为你即是方案得制定者也是方案得参与者，你的一言一行，很大程度影响到了落地效果。在这点上我们可以借鉴一下古代的政治家在落地一个政策上的套路，比如赵武灵王胡服骑射，北魏孝文帝汉化等。对应的在英文中对这种行为也有一个说法叫“Lead by Example”，通常用来形容团队领导以身作则的行事风格。 定期考核“以身作则”还不够，你还需要“定期考核”。很简单的道理，如果老师布置了作业不批改你会做吗？没有考核的 action 最终都会被大家遗忘，在开始执行前就要约定考核时间，给大家紧迫感；执行中定期考核，收集反馈，持续迭代；到最后久而久之慢慢形成习惯，方案才算是优雅地被落地了 综上，这四步法，希望可以带给你和你的团队一些帮助~]]></content>
      <categories>
        <category>Management</category>
      </categories>
      <tags>
        <tag>scrum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[穿越者王安石]]></title>
    <url>%2Fhistory-wanganshi%2F</url>
    <content type="text"><![CDATA[Dead camelthorn trees 王荆公其人王安石是旧属五代南唐领内临川县人，是为宋朝政治文化吹入新风的“南人”群体中的一员。曾在仁宗时上书天子，陈述政治改革的必要，该万言书《上仁宗皇帝言事书》被称为天下名文 王安石是诗人，为唐宋八大家之一，同时也是哲学家。但其哲学并非意识形态武装的理论思想，而是一眼就能分析把握事物真相的直观主义。他不喜欢受意识形态约束的理论，批评说那是被“理”遮蔽的东西。因此，他的政治改革不为描绘遥远未来的虚空影像所吸引，而是始终直视现实，究明其中的不平衡，匡正其中的不合理，将政治引入合理化的轨道。 他从儒教经典中选出号称记载周公所定政治机构的《周礼》，为之添加新注释，名曰《周官新义》。这并不是描写遥不可及的理想国，而是认为，在任何时候，古代的政治原理只要付诸实践，都可以成为行动的典范。 他的政治并非理想主义，而是合理主义 富国之法提到王安石，就不得不提在宋神宗时期开展的轰轰烈烈的熙宁变法，由于是在王安石主导下的变法，故亦称王安石变法。王安石变法在政治经济各个方面有诸多改进，下面摘取经济方面的三个办法作简要介绍 青苗法农民在作物播种后，需要购买和修理农具，以及存储新的收获到来之前所需的食盐和谷物，这些都是最需要金钱的。向债主借贷，利息异常高，从春到秋以一年计算，利率竟可达五六成。青苗法就是在这样的情况下实行的融资办法。想融资的农民集十户以上为一保，承担连带责任，可以不用抵押，从官府借钱。还债时，可按借债时的价格以谷物清偿，如果当时谷价腾高，也可用金钱偿还，但利息不超过二成。一般认为青苗法是二成利息的借贷，这是不恰当的。 均输法一般而言，物价的起伏有两种，一种是时间的变动，另一种是地域的差异，两种对民生都有很大影响。物价涨跌剧烈时，政府在物资贬值时买入，腾高时卖出，这叫平准法，从前汉武帝和王莽新朝都曾试行。均输与此不同，在交通尚未发达、信息不流通的时代，如果相同的物资在一地有多余，在别处则脱销，价格就会有很大差别，此时就以政府之力将物资从富余之处调往不足之地。 市易法在政府内部逐渐确立地位之后，王安石开始公开明确态度，对当时盘踞在国都和地方大都市支配经济领域的财阀寡头垄断体制，推出匡正的政策，这就是市易法。 首先在国都置市易务，在地方大都市设立市易司，收购滞销货，抵押卖出后又可以向民间放贷。这样既限制大商人对市场的控制，有利于稳定物价和商品交流，也增加了政府的财政收入。 评述除上述之外，王安石的新法在各个方面都有开展。这些新法当然不是王安石一个人想出来的，每项改革都有各自的提案者，其中很多是连名字都不知道的民间人士。他们根据经验提出改良方案并进言，王安石认真听取意见，上与天子、下与官僚谈话，这是王安石作为政治家的杰出之处，即便现在看来也有诸多民主进步的成分。 王安石的故事是中国历史的一大题目，几世纪以来对他作褒贬者不知凡几，他的功业也是争论不休。但是站在 21 世纪提及王安石，我们更感到惊异：在我们之前九百年，王安石企图以金融管制的办法操纵国事，其范围与深度不曾在同时代其他地方提出。 当王安石对神宗说“不加税而国用足”，他或许已知道可以利用信用借款的办法刺激经济成长。当生产增加货物充足流通时，即使使用同一税率也能在高额流通的状态里收到增税之成果，颇有几分当今盛行的凯恩斯主义经济刺激政策。 当我们完全从经济的角度来看得话，青苗法相当于国有银行提供低息贷款，均输法相当于国有企业负责重要物资的调配并参与市场买卖，市易法相当于国家控制定价权。看来在遥远的宋朝，我们已经进行过了一次“计划经济实验”。 王安石变法彰显了他与现代读者近，与同时代人物远，让我们不禁遐想，他会不会是一个穿越者。]]></content>
      <categories>
        <category>History</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[前端私有库实践]]></title>
    <url>%2Fdesign-private-npm%2F</url>
    <content type="text"><![CDATA[EYE TO EYE 最近 UI 在推公司的统一设计规范，定义了一个移动端统一的 加载样式，需要替换到移动端所有的页面当中去 但是移动端涉及的项目有4个，意思是我们需要在4个项目中都要实现这个 加载样式，作为程序员的我们表示这样一点也不 DRY 是时候需要搭建私有组件库了 我们将这个过程分为两个部分：一是跟组件跟私有库相关的工具的完善二是定义好组件的发布流程 工具verdaccio 私有npm proxy registry 的搭建先安装 docker装好后拉取 verdaccio 镜像并启动12docker pull verdaccio/verdaccio //拉取镜像docker run -it -d --rm --name verdaccio -p 4873:4873 verdaccio/verdaccio //启动verdaccio 然后浏览器键入地址：host:4873 就会看到如下界面，表示私有 npm 库搭建成功 开发发布工具有了组件库，如何开发和发布组件也需要一个标准的模板详情见 npm-package-template 发布流程组件从创造到上线到私有库的生命周期为：开发、评审和发布 组件开发 开发前务必记得先去 私有库 看看有没有现成的组件，然后再决定是否继续开发 clone npm-package-template 工具，然后按照 README 的介绍进行开发 注意每个组件都需要 README，来介绍用途、传参以及使用方式 组件评审增加这个环节的目的： 把控私有库的组件质量 信息共享，让大家知道最近都产生了哪些组件 创造一个大家交流前端技术的场景 具体评审的形式可以用会议的方式邀请感兴趣的同事参加，评审的内容主要为： 新组件是否必要，应用场景在哪 组件的传参是否规范，是否合适 代码结构是否符合标准（每个团队应该有自己的标准） 实际上就是大家一起从产品层面到技术层面来给你 review 一下，顺便闹闹磕最后要注意的是： PC组件 和 移动端组件最好拆为两个组件 移动端组件的布局最好用 rem 的方式 组件发布参考 npm-package-template README 进行发布 发布后记得邮件通知大家哦~ 后续迭代 建立一个类似 antd 的组件展示库，支持开发和发布，可以直接在 antd 的源码上修改 单独打包配置 组件需增加单元测试]]></content>
      <categories>
        <category>设计与架构</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何带新人]]></title>
    <url>%2Fmanagement-tobe-a-mentor%2F</url>
    <content type="text"><![CDATA[Earthrise 作为一个曾经也是新人的过来人，被放养过，也被调教过，兜兜转转走到现在，可谓是一把辛酸泪 最近陆陆续续带了一些新人，结合自己以前的经历，加上一些学习和请教，有了一些总结和思考 带新人不外乎为了达到两个目的：融入团队和有能力胜任工作 融入团队大部分人进入一个陌生的环境，遇到一群陌生的人，都会感到紧张，放不开，表现如下： 遇到问题，不好意思问同事，自己闭门造车，很久都搞不定，最后忍不住问了同事，才发现这个问题原来是这么回事 做一个任务，由于意外情况多了很多工作量，又不好意思把问题抛出来，请求协助，自己默默承受，加了班，最后结果也不好 遇到问题找不到人交流，感觉到孤立无助 以上种种都会给新人带来特别多的困扰，甚至开始怀疑自己，严重干扰了个人成长和发挥而这样的结果也是团队不稳定的开端。 如果我们把团队中的每个人看做一个点，人和人的交际看做线，那么这张网中的线越多越密，那么团队就越稳固越有凝聚力 融入团队，从 “画线” 开始！ 需要一个 mentor当然这个mentor很可能是你，告诉新人，有任何问题，找mentor 新人和mentor的这条线是条”明线“ 需要一次团队内聚餐线的建立应该从生活开始，工作的地方太正式，只有大家放松坐下来，聊聊自己的过去，爱好，不经意间”暗线”就建立了，更有甚者，从闲聊中找到了共同爱好，找到了火花，会潜移默化的让新人认为自己找到了组织，并且其他成员也会把其当成“自己人”；当然不要忘了开始的时候介绍介绍团队其他成员 开始一周需要每天去问三个问题 今天干了什么 有什么问题吗 我有什么能帮到你吗 最开始的一周，是新人的过渡期，这个时期是新人需要适应新环境变化的焦虑期，并且对技术来讲，很可能会遇到新的技术要学习，大量的问题肯定会接踵而至。主动的询问新人，并根据得到的信息判断出他们的困境，并给出引导，这个帮助对于他们来说就是雪中送炭。当然最重要的是让新人感受到他不是孤独的一个人，有人在关心帮助他 鼓励把遇到的问题，解决后给大家分享这算是融入团队的一个大杀器把 举个经历过的例子，组员 A 被一个 Bug 卡住了，迟迟未能解决，究其原因，原来是对我们前后端分离的认证方式不熟，然后mentor给他讲了一遍，把 Bug 解决了，并鼓励他去详细的了解我们的认证流程，给大家做分享。后来在分享的过程中，其他人提了很多问题，他一部分解答了，一部分未解答，下来之后把未解答的部分通过自己查资料和寻求别人帮助一一解决了，并发了邮件，沉淀出了文档。一方面 A 自己成长了，别人也对他产生认可；另一方面，其他人也得以了解了认证流程。当然更重要的是 A 不知不觉融入了团队 回顾整个过程，多少条“线”被建立我也数不清了 有能力胜任工作其实这个跟新人的能力，经验的不同有很大的不同，这里我只说几点注意事项 实践中学习才是最快到一个新的团队，会学习新的技术，一般来讲，给他一周时间学习，然后就开始派任务了事 但实际上这样的效果并不好，没有目标的学习就是分散自己的精力，降低自己的效率 最佳实践应该是先跟他明确目标，再以这个目标拆解下来看要学习什么 举个例子，有个新人进来会接手React项目，但他之前没有任何React经验；mentor首先会跟他安排一个目标任务，就是实现一个简单的Todolist，不需要UI，实现功能就行。而要实现这个功能就要学习React，并且需要用Webpack简单的起一个前端项目，实现之后需要用Git提交到我们代码库，让mentor review。 这一套流程下来，基本就是我们平时的一个简单的开发流程，里面涉及到的能力都是之后必须的。有了目标后，新人其实也明确了他要做什么，以及最终的目的是什么，这样学起来也就更聚焦了 安排任务要循序渐进为什么游戏这么吸引人，原因是游戏的反馈机制。你完成了某个任务，你就会升级，你就会更厉害；你不断得到正反馈，你就越愿意玩这个游戏 安排任务也是，前期一定不要安排超过能力的任务，给新人一些时间，给新人一些正反馈 我的经验一般是先让他修一些 Bug，能 handle 之后就可以做个小需求，之后可以做个小功能，也 ok 的话可以试着做个模块 总之就是一步一步加担子，同时根据他的反馈来掌握节奏 任务分配给了新人切勿当甩手掌柜新人由于刚进入团队，缺失了很多的上下文，大概率做的方向或者实现的方式会不太对，所以mentor要试着去引导 先问他会怎么做，然后提出问题，看他能不能回答，不能回答就发表下你自己的想法，一步步去完善他的方案，帮助其梳理思路 而对于拿到任务完全束手无策的新人来说，只要掌握下面的口诀： 我说你听 你说我听 我做你看 你做我看 除了前期的梳理外，中期频繁跟进进度，以及最后的代码 review 也是必不可少的环节相信按这样的流程跑几次之后你就会发现”新“人也慢慢变成”老“人了 最后带新人其实关注点还是在人上，明确自己作为一个mentor的责任，带上些许神圣感，去完成自己的使命 因为很可能，你的不经意会影响到他的人生轨迹 对命运保持一些敬畏]]></content>
  </entry>
  <entry>
    <title><![CDATA[Value Objects in Ruby on Rails]]></title>
    <url>%2Fdesign-value-objects-in-rails%2F</url>
    <content type="text"><![CDATA[What is a Value Object?As stated by Martin Fowler a value object is: A small simple object, like money or a date range, whose equality isn’t based on identity. two points: A value object represents a simple entity whose equality is based on its value, Value objects should be immutable Why do we need them? the separation of concerns it allows you to combine behaviour with the data and add functionality to data without polluting the model by isolating functionality your code is easier to test removes duplication improves the understanding and organisation of code. Operations on particular data are now gathered in a single place, instead of disperse throughout the code use cases Arguments together all the time One attribute with behaviour Two inseparable attributes value and unit Class enumerable Arguments together all the timeThe first situation happens when we have two or more arguments that are passed and used together all the time, often known as a “Data Clump” (code smell). A date range is a common example, when start_date and end_date are passed together all the time in our methods. We can create a class DateRange with the attributes start_date and end_date and this class should be responsible for the start_date and end_date columns of a given ActiveRecord object and accommodate all the related behaviour. We could include methods like include_date?(date), include_date_range?(date_range), overlap_date_range?(date_range) and to_s. This class can look something like this:1234567891011121314151617181920212223class DateRange attr_reader :start_date, :end_date def initialize(start_date, end_date) @start_date, @end_date = start_date, end_date end def include_date?(date) date &gt;= start_date &amp;&amp; date &lt;= end_date end def include_date_range?(date_range) start_date &lt;= date_range.start_date &amp;&amp; end_date &gt;= date_range.end_date end def overlap_date_range?(date_range) start_date &lt;= date_range.end_date &amp;&amp; end_date &gt;= date_range.start_date end def to_s &quot;from #&#123;start_date.strftime(&apos;%d-%B-%Y&apos;)&#125; to #&#123;end_date.strftime(&apos;%d-%B-%Y&apos;)&#125;&quot; endend This is just a standard Ruby object that does not inherit from ActiveRecord::Base. This class can be used, for example, with an Event model with the following columns: name, description, address_city, address_state, starts_at, ends_at. The Event model could look something like this:12345678910class Event &lt; ActiveRecord::Base def date_range DateRange.new(start_date, end_date) end def date_range=(date_range) self.start_date = date_range.start_date self.end_date = date_range.end_date endend With all this in place, we get the following usage:123456789&gt; event = Event.create(name: &apos;Ruby conf&apos;, start_date: Date.today, end_date: Date.today + 1.days)&gt; event.date_range=&gt; #&lt;DateRange:0x007fd8760c2690 @start_date=Tue, 06 Jun 2017, @end_date=Fri, 16 Jun 2017&gt;&gt; event.date_range.include_date?(Date.today)=&gt; true&gt; event.date_range.include_date_range?(DateRange.new(Date.today, Date.today + 2.days))=&gt; false&gt; event.date_range.include_date_range?(DateRange.new(Date.today, Date.today + 1.days))=&gt; true As I mentioned previously, one of the advantages of extracting code that usually goes in the model and create value objects is that you can reuse them and here is an example of that since we could use the DateRange object in the other model as well. One attribute with behaviourAnother situation where a value object can be useful is when you have one simple attribute that needs some associated behaviour and such behaviour does not belong in the model. Imagine that you have a model Room that inherits from ActiveRecord::Base with a degrees attribute and then you add a Temperature class to answer some questions that your system may need related with the temperature value:123456789101112131415161718192021222324252627282930class Temperature include Comparable attr_reader :degrees COLD = 20 HOT = 25 def initialize(degrees) @degrees = degrees end def cold? self &lt; COLD end def hot? self &gt; HOT end def &lt;=&gt;(other) degrees &lt;=&gt; other.degrees end def hash degrees.hash end def to_s &quot;#&#123;degrees&#125; °C&quot; endend We get the following usage:1234567891011&gt; room_1 = Room.create(degrees: 10)&gt; room_2 = Room.create(degrees: 20)&gt; room_3 = Room.create(degrees: 30)&gt; room_1.temperature.cold?=&gt; true&gt; room_1.temperature.hot?=&gt; false&gt; [room_1.temperature, Temperature.new(20), room_3.temperature, room_2.temperature].sort=&gt; [#&lt;Temperature:0x007fe194378840 @degrees=10&gt;, #&lt;Temperature:0x007fe194378818 @degrees=20&gt;, #&lt;Temperature:0x007fe1943787c8 @degrees=20&gt;, #&lt;Temperature:0x007fe1943787f0 @degrees=30&gt;]&gt; [room_1.temperature, Temperature.new(20), room_3.temperature, room_2.temperature].uniq=&gt; [#&lt;Temperature:0x007fe194361e88 @degrees=10&gt;, #&lt;Temperature:0x007fe194361e60 @degrees=20&gt;, #&lt;Temperature:0x007fe194361e38 @degrees=30&gt;] Two inseparable attributes value and unitA very popular one is the money gem, which helps you deal with money and currency conventions by providing a Money class that encapsulates all information about a certain amount of money such as its currency and value. The gem readme file is very thorough and self-explanatory so if you are interested go ahead and take a look. You can use it in a model class like Product:12345678910class Product &lt; ActiveRecord::Base def cost Money.new(cents, currency) end def cost=(cost) self.cents = cost.cents self.currency = cost.currency.to_s endend In this case when asking for or setting the cost of a product, we would use a Money instance.12345678&gt; product = Product.create(cost: Money.new(500, &quot;EUR&quot;))&gt; product.cost=&gt; #&lt;Money fractional:500 currency:EUR&gt;&gt; product.cost.cents=&gt; 500&gt; product.currency=&gt; &quot;EUR&quot; Class enumerableIt is common practice to define a value object in Rails models by creating an array class like this:1234567class Event &lt; ActiveRecord::Base SIZE = %w( small medium big )end This practice is not good because the array values may be used in the model attributes but they have nothing to do directly with the model domain. Defining the value object like this has a few disadvantages like impossibility to add functionality to the value object without polluting the model and does not allow to reuse the object. So we can create an object to accommodate the data of that array and also add some useful methods if we need them. The value object class for the example above could look something like this:1234567891011121314151617181920class Size SIZES = %w(small medium big) attr_reader :size def initialize(size) @size = size end def self.to_select SIZES.map&#123;|c| [c.capitalize, c]&#125; end def valid? SIZES.include?(size) end def to_s size.capitalize endend We can get the set of possible sizes and have a method that can be used in select form fields. I think this is useful if you have more than one model that has the attribute size, if your model has a lot of those arrays and you want to slim your model or if you have logic associated with it. Another common example of this type is Color, when we need to have a set of colors that can be used in some persisted models. Referenceshttps://github.com/tcrayford/Values/tree/master]]></content>
      <categories>
        <category>设计与架构</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[事务 Transaction]]></title>
    <url>%2Fdatabase-transaction%2F</url>
    <content type="text"><![CDATA[ACID的含义原子性（Atomicity）如果一个线程执行一个原子操作，这意味着另一个线程无法看到该操作的一半结果。系统只能处于操作之前或操作之后的状态，而不是介于两者之间的状态 一致性（Consistency）对数据的一组特定陈述必须始终成立。即不变量（invariants） 隔离性（Isolation）ACID意义上的隔离性意味着，同时执行的事务是相互隔离的：它们不能相互冒犯。传统的数据库教科书将隔离性形式化为可序列化（Serializability），这意味着每个事务可以假装它是唯一在整个数据库上运行的事务。数据库确保当事务已经提交时，结果与它们按顺序运行（一个接一个）是一样的，尽管实际上它们可能是并发运行的 然而实践中很少会使用可序列化隔离，因为它有性能损失。一些流行的数据库如Oracle 11g，甚至没有实现它。在Oracle中有一个名为“可序列化”的隔离级别，但实际上它实现了一种叫做快照隔离（snapshot isolation）的功能，这是一种比可序列化更弱的保证 持久性（Durability）数据库系统的目的是，提供一个安全的地方存储数据，而不用担心丢失。持久性是一个承诺，即一旦事务成功完成，即使发生硬件故障或数据库崩溃，写入的任何数据也不会丢失 概念（concepts）脏读一个客户端读取到另一个客户端尚未提交的写入。读已提交或更强的隔离级别可以防止脏读 脏写一个客户端覆盖写入了另一个客户端尚未提交的写入。几乎所有的事务实现都可以防止脏写 read skew 读取偏差（不可重复读）例如爱丽丝在银行有1000美元的储蓄，分为两个账户，每个500美元。现在一笔事务从她的一个账户中转移了100美元到另一个账户。如果她在事务处理的同时查看其账户余额列表，不幸地在转账事务完成前看到收款账户余额（余额为500美元），而在转账完成后看到另一个转出账户（已经转出100美元，余额400美元）。对爱丽丝来说，现在她的账户似乎只有900美元——看起来100美元已经消失了 在同一个事务中，客户端在不同的时间点会看见数据库的不同状态。快照隔离经常用于解决这个问题，它允许事务从一个特定时间点的一致性快照中读取数据。快照隔离通常使用多版本并发控制（MVCC） 来实现 lost update不同事务对同一个对象写两个客户端同时执行读取-修改-写入序列。其中一个写操作，在没有合并另一个写入变更情况下，直接覆盖了另一个写操作的结果。所以导致数据丢失。快照隔离的一些实现可以自动防止这种异常，而另一些实现则需要手动锁定（SELECT FOR UPDATE） write skew不同事务对不同对象写一个事务读取一些东西，根据它所看到的值作出决定，并将决定写入数据库。但是，写作的时候，决定的前提不再是真实的。只有可序列化的隔离才能防止这种异常 幻读一个事务中的写入改变另一个事务的搜索查询的结果，被称为幻读幻读会造成 lost update 或者 write skew 字面意义上的串行执行如果每个事务的执行速度非常快，并且事务吞吐量足够低，足以在单个CPU核上处理，这是一个简单而有效的选择 两阶段锁定（2PL，two-phase locking）数十年来，两阶段锁定一直是实现可序列化的标准方式，但是许多应用出于性能问题的考虑避免使用它 可串行化快照隔离（SSI）一个相当新的算法，避免了先前方法的大部分缺点。它使用乐观的方法，允许事务执行而无需阻塞。当一个事务想要提交时，它会进行检查，如果执行不可序列化，事务就会被中止。 弱隔离级别(Weak Isolation Levels)如果两个事务不触及相同的数据，它们可以安全地并行（parallel）运行，因为两者都不依赖于另一个。当一个事务读取由另一个事务同时修改的数据时，或者当两个事务试图同时修改相同的数据时，并发问题（竞争条件）才会出现。数据库一直试图通过提供事务隔离（transaction isolation）来隐藏应用程序开发者的并发问题。从理论上讲，隔离可以通过假装没有并发发生，让你的生活更加轻松：可序列化（serializable）的隔离等级意味着数据库保证事务的效果与连续运行（即一次一个，没有任何并发）是一样的。实际上不幸的是：隔离并没有那么简单。可序列化会有性能损失，许多数据库不愿意支付这个代价。因此，系统通常使用较弱的隔离级别来防止一部分，而不是全部的并发问题。这些隔离级别难以理解，并且会导致微妙的错误，但是它们仍然在实践中被使用。 读已提交（Read Committed）最基本的事务隔离级别是读已提交，它提供了两个保证： 从数据库读时，只能看到已提交的数据（没有脏读（dirty reads）） 写入数据库时，只会覆盖已经写入的数据（没有脏写（dirty writes）） 没有脏读设想一个事务已经将一些数据写入数据库，但事务还没有提交或中止。另一个事务可以看到未提交的数据吗？如果是的话，那就叫做脏读（dirty reads） 在读已提交隔离级别运行的事务必须防止脏读。这意味着事务的任何写入操作只有在该事务提交时才能被其他人看到（然后所有的写入操作都会立即变得可见） 没有脏读：用户2只有在用户1的事务已经提交后才能看到x的新值。 没有脏写如果两个事务同时尝试更新数据库中的相同对象，会发生什么情况？我们不知道写入的顺序是怎样的，但是我们通常认为后面的写入会覆盖前面的写入。 但是，如果先前的写入是尚未提交事务的一部分，又会发生什么情况，后面的写入会覆盖一个尚未提交的值？这被称作脏写（dirty write）在读已提交的隔离级别上运行的事务必须防止脏写，通常是延迟第二次写入，直到第一次写入事务提交或中止为止 Alice和Bob两个人同时试图购买同一辆车。购买汽车需要两次数据库写入：网站上的商品列表需要更新，以反映买家的购买，销售发票需要发送给买家。根据下图，销售是属于Bob的（因为他成功更新了商品列表），但发票却寄送给了爱丽丝（因为她成功更新了发票表）。读已提交会阻止这样这样的事故。 实现读已提交防止脏写数据库通过使用行锁（row-level lock）来防止脏写：当事务想要修改特定对象（行或文档）时，它必须首先获得该对象的锁。然后必须持有该锁直到事务被提交或中止。一次只有一个事务可持有任何给定对象的锁；如果另一个事务要写入同一个对象，则必须等到第一个事务提交或中止后，才能获取该锁并继续。这种锁定是读已提交模式（或更强的隔离级别）的数据库自动完成的。 防止脏读一种选择是使用读锁，并要求任何想要读取对象的事务来简单地获取该锁，然后在读取之后立即再次释放该锁。但是要求读锁的办法在实践中效果并不好。因为一个长时间运行的写入事务会迫使许多只读事务等到这个慢写入事务完成。这会损失只读事务的响应时间，并且不利于可操作性：因为等待锁，应用某个部分的迟缓可能由于连锁效应，导致其他部分出现问题出于这个原因，大多数数据库使用图7-4的方式防止脏读：对于写入的每个对象，数据库都会记住旧的已提交值，和由当前持有写入锁的事务设置的新值。 当事务正在进行时，任何其他读取对象的事务都会拿到旧值。 只有当新值提交后，事务才会切换到读取新值。 快照隔离和可重复读（Snapshot Isolation and Repeatable Read）不可重复读（nonrepeatable read）或读取偏差（read skew）爱丽丝在银行有1000美元的储蓄，分为两个账户，每个500美元。现在一笔事务从她的一个账户中转移了100美元到另一个账户。如果她在事务处理的同时查看其账户余额列表，不幸地在转账事务完成前看到收款账户余额（余额为500美元），而在转账完成后看到另一个转出账户（已经转出100美元，余额400美元）。对爱丽丝来说，现在她的账户似乎只有900美元——看起来100美元已经消失了。 快照隔离（snapshot isolation）【28】是这个问题最常见的解决方案。想法是，每个事务都从数据库的一致快照（consistent snapshot）中读取——也就是说，事务可以看到事务开始时在数据库中提交的所有数据。即使这些数据随后被另一个事务更改，每个事务也只能看到该特定时间点的旧数据。 实现快照隔离与读取提交的隔离类似，快照隔离的实现通常使用写锁来防止脏写，这意味着进行写入的事务会阻止另一个事务修改同一个对象。但是读取不需要任何锁定。从性能的角度来看，快照隔离的一个关键原则是：读不阻塞写，写不阻塞读。这允许数据库在处理一致性快照上的长时间查询时，可以正常地同时处理写入操作。且两者间没有任何锁定争用 为了实现快照隔离，数据库使用了我们看到的用于防止图7-4中的脏读的机制的一般化。数据库必须可能保留一个对象的几个不同的提交版本，因为各种正在进行的事务可能需要看到数据库在不同的时间点的状态。因为它并排维护着多个版本的对象，所以这种技术被称为多版本并发控制（MVCC, multi-version concurrentcy control）。 表中的每一行都有一个 created_by 字段，其中包含将该行插入到表中的的事务ID。此外，每行都有一个 deleted_by 字段，最初是空的。如果某个事务删除了一行，那么该行实际上并未从数据库中删除，而是通过将 deleted_by 字段设置为请求删除的事务的ID来标记为删除。在稍后的时间，当确定没有事务可以再访问已删除的数据时，数据库中的垃圾收集过程会将所有带有删除标记的行移除，并释放其空间 观察一致性快照的可见性规则当一个事务从数据库中读取时，事务ID用于决定它可以看见哪些对象，看不见哪些对象。通过仔细定义可见性规则，数据库可以向应用程序呈现一致的数据库快照。工作如下： 在每次事务开始时，数据库列出当时所有其他（尚未提交或中止）的事务清单，即使之后提交了，这些事务的写入也都会被忽略 被中止事务所执行的任何写入都将被忽略 由具有较晚事务ID（即，在当前事务开始之后开始的）的事务所做的任何写入都被忽略，而不管这些事务是否已经提交 所有其他写入，对应用都是可见的 这些规则适用于创建和删除对象。在上图中，当事务12 从账户2 读取时，它会看到 500 余额的删除是由事务13 完成的（根据规则3，事务12 看不到事务13 执行的删除），且400美元记录的创建也是不可见的（按照相同的规则）。换句话说，如果以下两个条件都成立，则可见一个对象： 读事务开始时，创建该对象的事务已经提交 对象未被标记为删除，或如果被标记为删除，请求删除的事务在读事务开始时尚未提交。长时间运行的事务可能会长时间使用快照，并继续读取（从其他事务的角度来看）早已被覆盖或删除的值。由于从来不更新值，而是每次值改变时创建一个新的版本，数据库可以在提供一致快照的同时只产生很小的额外开销 防止丢失更新(Preventing Lost Updates)到目前为止已经讨论的读已提交和快照隔离级别，主要保证了只读事务在并发写入时可以看到什么。却忽略了两个事务并发写入的问题 并发的写入事务之间还有其他几种有趣的冲突。其中最着名的是丢失更新（lost update）问题，如图7-1所示，以两个并发计数器增量为例。 如果应用从数据库中读取一些值，修改它并写回修改的值（读取-修改-写入序列），则可能会发生丢失更新的问题。如果两个事务同时执行，则其中一个的修改可能会丢失，因为第二个写入的内容并没有包括第一个事务的修改 原子写(Atomic write operations)原子操作通常通过在读取对象时，获取其上的排它锁来实现。以便更新完成之前没有其他事务可以读取它。这种技术有时被称为游标稳定性（cursor stability）。另一个选择是简单地强制所有的原子操作在单一线程上执行。 不幸的是，ORM框架很容易意外地执行不安全的读取-修改-写入序列，而不是使用数据库提供的原子操作。如果你知道自己在做什么那当然不是问题，但它经常产生那种很难测出来的微妙Bug。 显式锁定(Explicit locking)如果数据库的内置原子操作没有提供必要的功能，防止丢失更新的另一个选择是让应用程序显式地锁定将要更新的对象。然后应用程序可以执行读取-修改-写入序列，如果任何其他事务尝试同时读取同一个对象，则强制等待，直到第一个读取-修改-写入序列完成。12345678BEGIN TRANSACTION;SELECT * FROM figures WHERE name = &apos;robot&apos; AND game_id = 222FOR UPDATE;-- 检查玩家的操作是否有效，然后更新先前SELECT返回棋子的位置。UPDATE figures SET position = &apos;c4&apos; WHERE id = 1234;COMMIT; FOR UPDATE子句告诉数据库应该对该查询返回的所有行加锁。 自动检测丢失的更新(Automatically detecting lost updates)原子操作和锁是通过强制读取-修改-写入序列按顺序发生，来防止丢失更新的方法。另一种方法是允许它们并行执行，如果事务管理器检测到丢失更新，则中止事务并强制它们重试其读取-修改-写入序列。这种方法的一个优点是，数据库可以结合快照隔离高效地执行此检查。事实上，PostgreSQL的可重复读，Oracle的可串行化和SQL Server的快照隔离级别，都会自动检测到丢失更新，并中止惹麻烦的事务。但是，MySQL/InnoDB的可重复读并不会检测丢失更新【23】。一些作者【28,30】认为，数据库必须能防止丢失更新才称得上是提供了快照隔离，所以在这个定义下，MySQL下不提供快照隔离。丢失更新检测是一个很好的功能，因为它不需要应用代码使用任何特殊的数据库功能，你可能会忘记使用锁或原子操作，从而引入错误；但丢失更新的检测是自动发生的，因此不太容易出错。 比较并设置（CAS, Compare And Set）此操作的目的是为了避免丢失更新：只有当前值从上次读取时一直未改变，才允许更新发生。如果当前值与先前读取的值不匹配，则更新不起作用，且必须重试读取-修改-写入序列。例如，为了防止两个用户同时更新同一个wiki页面，可以尝试类似这样的方式，只有当用户开始编辑页面内容时，才会发生更新：123-- 根据数据库的实现情况，这可能也可能不安全UPDATE wiki_pages SET content = &apos;新内容&apos; WHERE id = 1234 AND content = &apos;旧内容&apos;; 如果内容已经更改并且不再与“旧内容”相匹配，则此更新将不起作用，因此您需要检查更新是否生效，必要时重试。但是，如果数据库允许WHERE子句从旧快照中读取，则此语句可能无法防止丢失更新，因为即使发生了另一个并发写入，WHERE条件也可能为真。在依赖数据库的CAS操作前要检查其是否安全。 写入偏差与幻读(Write Skew and Phantoms)前面的章节中，我们看到了脏写和丢失更新，当不同的事务并发地尝试写入相同的对象时，会出现两种竞争条件。为了避免数据损坏，这些竞争条件需要被阻止——既可以由数据库自动执行，也可以通过锁和原子写操作这类手动安全措施来防止。 首先，想象一下这个例子：你正在为医院写一个医生轮班管理程序。医院通常会同时要求几位医生待命，但底线是至少有一位医生在待命。医生可以放弃他们的班次（例如，如果他们自己生病了），只要至少有一个同事在这一班中继续工作。现在想象一下，Alice和Bob是两位值班医生。两人都感到不适，所以他们都决定请假。不幸的是，他们恰好在同一时间点击按钮下班。图7-8说明了接下来的事情。 在两个事务中，应用首先检查是否有两个或以上的医生正在值班；如果是的话，它就假定一名医生可以安全地休班。由于数据库使用快照隔离，两次检查都返回 2 ，所以两个事务都进入下一个阶段。Alice更新自己的记录休班了，而Bob也做了一样的事情。两个事务都成功提交了，现在没有医生值班了。违反了至少有一名医生在值班的要求。 写偏差的特征这种异常称为写偏差。它既不是脏写，也不是丢失更新，因为这两个事务正在更新两个不同的对象（Alice和Bob各自的待命记录）。在这里发生的冲突并不是那么明显，但是这显然是一个竞争条件：如果两个事务一个接一个地运行，那么第二个医生就不能歇班了。异常行为只有在事务并发进行时才有可能。可以将写入偏差视为丢失更新问题的一般化。如果两个事务读取相同的对象，然后更新其中一些对象（不同的事务可能更新不同的对象），则可能发生写入偏差。在多个事务更新同一个对象的特殊情况下，就会发生脏写或丢失更新（取决于时机）。 导致写入偏差的幻读所有这些例子都遵循类似的模式： 一个SELECT查询找出符合条件的行，并检查是否符合一些要求。（例如：至少有两名医生在值班；不存在对该会议室同一时段的预定；棋盘上的位置没有被其他棋子占据；用户名还没有被抢注；账户里还有足够余额） 按照第一个查询的结果，应用代码决定是否继续。（可能会继续操作，也可能中止并报错） 如果应用决定继续操作，就执行写入（插入、更新或删除），并提交事务。这个写入的效果改变了步骤2 中的先决条件。换句话说，如果在提交写入后，重复执行一次步骤1 的SELECT查询，将会得到不同的结果。因为写入改变符合搜索条件的行集（现在少了一个医生值班，那时候的会议室现在已经被预订了，棋盘上的这个位置已经被占据了，用户名已经被抢注，账户余额不够了） 这种效应：一个事务中的写入改变另一个事务的搜索查询的结果，被称为幻读【3】。快照隔离避免了只读查询中幻读，但是在像我们讨论的例子那样的读写事务中，幻影会导致特别棘手的写歪斜(write skew)情况 可序列化（Serializable）读已提交和快照隔离级别会阻止某些竞争条件，但不会阻止另一些。我们遇到了一些特别棘手的例子，写入偏差和幻读。 可序列化（Serializability）隔离通常被认为是最强的隔离级别。它保证即使事务可以并行执行，最终的结果也是一样的，就好像它们没有任何并发性，连续挨个执行一样。因此数据库保证，如果事务在单独运行时正常运行，则它们在并发运行时继续保持正确 —— 换句话说，数据库可以防止所有可能的竞争条件。 真的串行执行避免并发问题的最简单方法就是完全不要并发：在单个线程上按顺序一次只执行一个事务。这样做就完全绕开了检测/防止事务间冲突的问题，由此产生的隔离，正是可序列化的定义。 在特定约束条件下，真的串行执行事务，已经成为一种实现可序列化隔离等级的可行办法： 每个事务都必须小而快，只要有一个缓慢的事务，就会拖慢所有事务处理 仅限于活跃数据集可以放入内存的情况。很少访问的数据可能会被移动到磁盘，但如果需要在单线程执行的事务中访问，系统就会变得非常慢 写入吞吐量必须低到能在单个CPU核上处理，如若不然，事务需要能划分至单个分区，且不需要跨分区协调 跨分区事务是可能的，但是它们的使用程度有很大的限制 两阶段锁定（2PL）读阻塞写，写阻塞读两阶段锁定的巨大缺点，以及70年代以来没有被所有人使用的原因，是其性能问题。两阶段锁定下的事务吞吐量与查询响应时间要比弱隔离级别下要差得多 这一部分是由于获取和释放所有这些锁的开销，但更重要的是由于并发性的降低。按照设计，如果两个并发事务试图做任何可能导致竞争条件的事情，那么必须等待另一个完成 传统的关系数据库不限制事务的持续时间，因为它们是为等待人类输入的交互式应用而设计的。因此，当一个事务需要等待另一个事务时，等待的时长并没有限制。即使你保证所有的事务都很短，如果有多个事务想要访问同一个对象，那么可能会形成一个队列，所以事务可能需要等待几个其他事务才能完成。因此，运行2PL的数据库可能具有相当不稳定的延迟，如果在工作负载中存在争用，那么可能高百分位点处的响应会非常的慢（参阅“描述性能”）。可能只需要一个缓慢的事务，或者一个访问大量数据并获取许多锁的事务，就能把系统的其他部分拖慢，甚至迫使系统停机。当需要稳健的操作时，这种不稳定性是有问题的 基于锁实现的读已提交隔离级别可能发生死锁，但在基于2PL实现的可序列化隔离级别中，它们会出现的频繁的多（取决于事务的访问模式）。这可能是一个额外的性能问题：当事务由于死锁而被中止并被重试时，它需要从头重做它的工作。如果死锁很频繁，这可能意味着巨大的浪费 序列化快照隔离（SSI）两阶段锁是一种所谓的悲观并发控制机制（pessimistic） ：它是基于这样的原则：如果有事情可能出错（如另一个事务所持有的锁所表示的），最好等到情况安全后再做任何事情。这就像互斥，用于保护多线程编程中的数据结构。从某种意义上说，串行执行可以称为悲观到了极致：在事务持续期间，每个事务对整个数据库（或数据库的一个分区）具有排它锁，作为对悲观的补偿，我们让每笔事务执行得非常快，所以只需要短时间持有“锁” 相比之下，序列化快照隔离是一种乐观（optimistic） 的并发控制技术。在这种情况下，乐观意味着，如果存在潜在的危险也不阻止事务，而是继续执行事务，希望一切都会好起来。当一个事务想要提交时，数据库检查是否有什么不好的事情发生（即隔离是否被违反）；如果是的话，事务将被中止，并且必须重试。只有可序列化的事务才被允许提交 乐观并发控制是一个古老的想法，其优点和缺点已经争论了很长时间。如果存在很多争用（contention）（很多事务试图访问相同的对象），则表现不佳，因为这会导致很大一部分事务需要中止。如果系统已经接近最大吞吐量，来自重试事务的额外负载可能会使性能变差 但是，如果有足够的备用容量，并且事务之间的争用不是太高，乐观的并发控制技术往往比悲观的要好。可交换的原子操作可以减少争用：例如，如果多个事务同时要增加一个计数器，那么应用增量的顺序（只要计数器不在同一个事务中读取）就无关紧要了，所以并发增量可以全部应用且无需冲突。顾名思义，SSI基于快照隔离——也就是说，事务中的所有读取都是来自数据库的一致性快照（参见“快照隔离和可重复读取”）。与早期的乐观并发控制技术相比这是主要的区别。在快照隔离的基础上，SSI添加了一种算法来检测写入之间的序列化冲突，并确定要中止哪些事务]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>Transaction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经济解释 第二三章]]></title>
    <url>%2Feconomic-explanation2%2F</url>
    <content type="text"><![CDATA[第二章凡指明是基础假设，或是公理，大家都不在这基础上争论经济学的基础假设： 任何人的行为都是个人作出可以被推测的选择而起 每个人都是自私的：局限条件下的个人利益最大化 第三章物品(goods)：有胜于无经济物品(economic goods)：多胜于少，总是缺乏竞争(competition)：一种economic goods的需求有多于一人的需求产权制度(system of property rights)：竞争的游戏规则，也就是约束竞争行为的一种局限条件 经济物品总是缺乏，存在竞争，竞争就得有规则，就是产权制度 竞争准则：所有的规则都是为 某个准则服务 价格是一个决定胜负的准则，而私有产权是为了这个准则服务的 竞争准则决定行为 浪费是指有其他颁发，或用其他资源使用的分配，可使社会的财富或收入增加，但这些“其他”办法缺莫名其妙地不予采用 唯一没有浪费的准则就是市价 经济学与好坏无关：一种准则是好是坏，或对社会福利有何好处，则是伦理价值观的事了，与客观分析无关 经济学的范畴： 在知道有关的局限条件或游戏规则下，我们可以推断所用的竞争准则是什么 有了竞争准则，经济学可以推断人的行为会怎样，资源的使用会怎样，财富或收入的分配会怎样 解释游戏规则是怎样形成的 警惕他人的“理论”： 特殊理论，毫无一般性的解释力 套套逻辑 个人价值观，不够客观，与科学无关 谬论]]></content>
      <categories>
        <category>Economics</category>
      </categories>
      <tags>
        <tag>张五常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初尝 Scrum 标准流程]]></title>
    <url>%2Fmanagement-scrum-first-try%2F</url>
    <content type="text"><![CDATA[经过大概一周的学习和准备，并向一些行业内老前辈求取经验，今天尝试在我们组试行了TW的标准 Scrum流程，其中最重要的一环就是Sprint评审会。遗憾的是这个会大概持续了一天，为什么持续这么久，下面会有详细讨论 前期准备 确立基准点，这里选取了一个包含一个前端组件、一个后端接口以及一个测试的Story，为什么这么选取，是考虑到足够量化能方便求倍数关系 对组员进行背景知识培训，告知会议流程 也是最重要的，为什么我们要用标准Scrum，这里个人有个人的理解，不赘述 会后反馈甲：相对点数不好估，觉得需要全栈的团队，并且经验要资深一点，每个迭代做的事情都是类似的；但实际情况是我们需求涉及面广，并且会有插入需求，会有优先级，sprint周期只有一周共同估点可以帮助一起思考业务的东西，共同熟悉业务 乙：故事还可以再拆小一点，在以前的方法上加上共同估时间帮助一起熟悉业务 丙：今天的评审会每个需求耗时很长，但以后熟悉之后效率会有提升 丁：哪些需求用敏捷，哪些需求不用敏捷 提前区分今天这套流程可以帮助产品理解开发估点两极分化 戊：耗时太长，影响效率，到最后面 已经听不进去了真正到做的时候忘了 己：大部分时间在理解需求之前的方式不会解决问题，只是把问题压到一个人身上了模式没问题时间太长了 庚：产品评审会没利用起来不熟悉需求估不准这套流程需要细心准备需求 辛：闭环流程，每做一次就会更加了解一次，会越来越快估相对复杂度对无法得知上线时间先判断有没有必要做，然后了解清楚需求，怎么做，最后再估点 关键问题总结关于“时间长”时间长，那么时间花到了哪？排除掉对评审会规则的介绍以及新的敏捷估算的方式的学习成本，大家的共识是大部分时间花到了 理解需求 这件事情上。那么为什么之前“时间不长”，关键在于之前是一个人估点，现在是所有人估点，所有人要为自己估出的点做出合理的解释，所有人就有动力去认真的评估这个需求，甚至要具体到写几个页面，写几个接口，而这些的前提就是要理解需求。 花这么多时间让所有人了解所有需求值吗？ 站在做需求这个人角度，你无法保证你理解的需求是真正的需求，你也无法保证你的估点是否准确，而其他人会在理解这个需求的过程中帮助你 站在不做这个需求的其他人角度，你理解了这个需求，在估点的时候，大概率你会知道其他人会怎么实现这个需求，因为技术方案的不同点数会差异很大，他山之石可以攻玉，虽然你没有亲自做这个需求，也可以相当于你做了这个需求；再者你理解了这个需求所代表的产品逻辑和业务，这会不断拓展你作为一个开发的边界 站在团队角度，这可以消除单点故障，不会因为一个人 offline 而造成团队停摆 站在公司角度，懂产品，熟悉公司业务的开发越多，公司就越有底气 组内用相对值估点合适吗?估点的目的是为团队一个sprint的产出做一个度量，并指导下一个 sprint的有效进行，但是要得出对一个sprint的度量，需要满足以下条件： 迭代中没有异常事件，比如插入需求 组内所有人都能参与迭代 做的事情有连续性，这个sprint做的需求和下个sprint做的需求不能差异太大 但我们组目前情况是： 迭代中会有插入需求 敏捷开发和项目制并行，不能保证全勤 需求跨度较大 而采用时间估点会的话，一是更好把控进度，因为那些平时影响我们迭代的事件很容易就能用时间量化，比如周三生病了，周二全公司放假等等；二是更好估点，并且对于复杂的项目，也可以交叉应用相对估点，比方说，一个需求你不知道估多少点，而你知道一个小需求是2点，然后你就可以看这个需求是这个小需求的多少倍，从而得到最终估点 关于估点两极分化这是一件多么令人兴奋的事情啊！对于估点少的人，大概率他漏考虑了一些点对于估点多的人，很可能他考虑到了其他人没考虑的点，亦或是他可能拿不准怎么做，进行了一次“防御性估点”在以上的情况下，把需求掰开了、揉碎了大家一起来评一评，把问题和风险直接提前到评审会上有效率地解决，岂不美哉？ 最后附上“打牌”图]]></content>
      <categories>
        <category>Management</category>
      </categories>
      <tags>
        <tag>scrum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SCRUM敏捷开发]]></title>
    <url>%2Fmanagement-scrum%2F</url>
    <content type="text"><![CDATA[概述Scrum是一个用于开发和维护复杂产品的框架 ，是一个增量的、迭代的开发过程。在这个框架中，整个开发过程由若干个短的迭代周期组成，一个短的迭代周期称为一个Sprint，每个Sprint的建议长度是2到4周(互联网产品研发可以使用1周的Sprint)。在Scrum中，使用产品Backlog来管理产品的需求，产品backlog是一个按照商业价值排序的需求列表，列表条目的体现形式通常为用户故事。Scrum团队总是先开发对客户具有较高价值的需求。在Sprint中，Scrum团队从产品Backlog中挑选最高优先级的需求进行开发。挑选的需求在Sprint计划会议上经过讨论、分析和估算得到相应的任务列表，我们称它为Sprint backlog。在每个迭代结束时，Scrum团队将递交潜在可交付的产品增量。 Scrum起源于软件开发项目，但它适用于任何复杂的或是创新性的项目。 SCRUM框架包括3个角色、3个工件、4个事件、5个价值 SCRUM团队的三个角色产品负责人产品负责人负责最大化产品以及开发团队工作的价值产品负责人是管理产品待办事项列表的唯一责任人。产品待办事项列表的管理包括: 清晰地表达产品代办事项列表条目 对产品代办事项列表中的条目进行排序,最好地实现目标和使命 确保开发团队所执行工作的价值 确保产品代办事项列表对所有人可见、透明、清晰,并且显示 Scrum 团队的下一步工作 确保开发团队对产品代办事项列表中的条目达到一定程度的理解 产品负责人是一个人,而不是一个委员会。产品负责人可能会在产品代办事项列表中 体现一个委员会的需求,但要想改变某条目的优先级必须先说服产品负责人。 为保证产品负责人的工作取得成功,组织中的所有人员都必须尊重他的决定。产品负 责人所作的决定在产品待办事项列表的内容和排序中要清晰可见。任何人都不得要求开发 团队按照另一套需求开展工作,开发团队也不允许听从任何其他人的指令 开发团队开发团队包含了专业人员,负责在每个Sprint的结尾交付潜在可发布的“完成”产品增量 他们是自组织的,没有人(即使是Scrum Master都不可以)告诉开发团队如何把产品 代办事项列表变成潜在可发布的功能。 开发团队是跨职能的,团队作为一个整体拥有创造产品增量所需要的全部技能。 Scrum不认可开发团队成员的头衔,无论承担哪种工作他们都是开发者。此规则无一例外。 开发团队中的每个成员可以有特长和专注领域,但是责任归属于整个开发团队 开发团队不包含如测试或业务分析等负责特定领域的子团队 开发团队的规模开发团队最佳规模是小到足以保持敏捷性,大到足以完成重要工作。少于 3 人的开发 团队没有足够的交互,因而所获得的生产力增长也不会很大。小团队在Sprint中可能会 受到技能限制,从而导致无法交付可发布的产品增量。大于 9 人的团队需要过多的协调沟 通工作。大型团队会产生太多复杂性,不便于经验过程管理。产品负责人和Scrum Master的角色不包含在此数字中,除非他们也参与执行Sprint代表事项列表中的工作。 Scrum MasterScrum Master 负责确保 Scrum 被理解并实施。为了达到这个目的,Scrum Master要确保Scrum团队遵循Scrum的理论、实践和规则。Scrum Master是Scrum团队中的服务式领导。 Scrum Master帮助Scrum团队外的人员了解他们如何与Scrum团队交互是有益的。Scrum Master通过改变这些交互来最大化Scrum团队所创造的价值。 Scrum Master 服务于产品负责人Scrum Master以各种方式服务于产品负责人,包括: 找到有效管理产品代办事项列表的技巧 清晰地和开发团队沟通愿景、目标和产品代表事项列表条目 教导开发团队创建清晰简明的产品代表事项列表条目 在经验主义环境中理解长期的产品规划 理解并实践敏捷 按需推动Scrum活动 Scrum Master 服务于开发团队Scrum Master以各种方式服务于开发团队,包括: 指导开发团队自组织和跨职能 教导并领导开发团队创造高价值的产品 移除开发团队进展过程中的障碍 按需推动Scrum活动 在Scrum还未完全被采纳和理解的组织环境下指导开发团队 SCRUM的三个工件Scrum的工件以不同的方式展现工作和价值,可以用来提供透明性以及检验和适应的机会。Scrum中所定义的工件能最大化关键信息的透明性,来保证Scrum团队成功地交付完成的增量。 PRODUCT BACKLOG – 产品待办事项列表产品待办事项列表是一个排序的列表,包含所有产品需要的东西,也是产品需求变动的唯一来源。产品负责人负责产品待办事项列表的内容、可用性和优先级。产品待办事项列表列出了所有的特性、功能、需求、改进方法和缺陷修复等对未来发布产品进行的改变。产品待办事项列表条目包含描述、次序和估算的特征。产品待办事项列表通常以价值、风险、优先级和必须性排序。它是一个按照优先级由高到低排列的一个序列，每个条目有唯一的顺序。排在顶部的产品待办事项列表条目需要立即进行开发。排序越高,产品待办事项列表条目越紧急,就越需要仔细斟酌,并且对其价值的意见越一致。 SPRINT BACKLOGSprint代办事项列表是一组为当前Sprint选出的产品代办事项列表条目,外加交付 产品增量和实现Sprint目标的计划。Sprint代办事项列表是开发团队对于哪些功能要包 含在下个增量中,以及交付那些功能所需工作的预计。 Sprint代办事项列表定义了开发团队把产品代办事项列表条目转换成“完成”的增量 所需要执行的工作。Sprint代办事项列表使开发团队确定的、达到Sprint目标所需的工作清晰可见。 Sprint代办事项列表是一份足够具体的计划,使得进度上的改变能在每日例会中得到理解。 产品增量（PRODUCT INCREMENT）增量是一个Sprint完成的所有产品待办列表项的总和，以及之前所有Sprint所产生的增量的价值总和。在Sprint的最后，新的增量必须是“完成”的，这意味着它必须可用并且达到了Scrum 团队“完成”的定义的标准。增量是在Sprint结束时支持经验主义的可检视的和已完成的产品组成部分。增量是迈向愿景或目标的一步。无论产品负责人是否决定发布它，增量必须可用。 SCRUM的四个事件Scrum使用固定的事件来产生规律性，以此来减少Scrum之外的其它会议的必要。所有事 件都是有时间盒限定的事件，也就是说每一个事件限制在最长的时间范围内。一旦Sprint开始，它的持续时间是固定的，不能缩短或者延长。而其他事件则可以在该事件的目标达成之后可以立即终止，如此确保时间被适当地使用而不会造成过程中的浪费。 SprintSprint是Scrum的核心，其长度(持续时间)为一个月或更短时间的限时，在这段时间内构建一个“完成的”、可用的和潜在可发布的产品增量。在整个开发过程期间，Sprint的长度通常保持一致。前一个Sprint结束后，新的下一个Sprint紧接着立即开始。Sprint由Sprint计划会议、每日Scrum 站会、开发工作、 Sprint回顾会议构成。 在Sprint期间: 不能做出有害于Sprint目标的改变 不能降低质量 随着对信息掌握的增加，产品负责人与开发团队之间对范围内要做的事可能会要澄清和重新协商。 Sprint 计划会议Sprint中要做的工作在Sprint计划会议中来做计划。这份工作计划是由整个Scrum团队共同协作完成的。 Sprint计划会议回答以下问题: 1. 接下来的 Sprint 交付的增量中要包含什么内容?开发团队预测在这次Sprint中要开发的功能。产品负责人讲解 Sprint的目标以及达成该目 标所需完成的产品待办列表项。整个Scrum团队协同工作来理解Sprint的工作。 Sprint 会议的输入是产品待办列表、最新的产品增量、开发团队在这个Sprint中能力的预测以及开发团队的以往表现。开发团队自己决定选择产品待办列表项的数量。只有开发团队可以评估接下来的Sprint可以完成什么工作。 在开发团队预测完这个Sprint中可交付的产品待办列表项之后，Scrum团队草拟一个Sprint 目标。Sprint目标是在这个Sprint通过实现产品待办列表要达到的目的，同时它也为开发团队提供指引，使得开发团队明确开发增量的目的。 2. 要如何完成交付增量所需的工作?在设定了Sprint目标并选出这个Sprint要完成的产品待办列表项之后，开发团队将决定如何在Sprint中把这些功能构建成“完成”的产品增量。这个Sprint中所选出的产品待办列表项加上交付它们的计划称之为Sprint 待办列表。 开发团队通常从设计整个系统开始，到如何将产品待办列表转换成可工作的产品增量所需 要的工作。工作有不同的大小，或者不同的预估工作量。然而，在Sprint 计划会议中，开 发团队已经挑选出足够量的工作，以此来预估他们在即将到来的Sprint中能够完成。在Sprint计划会议的最后，开发团队规划出在Sprint最初几天内所要做的工作，通常以一天 或更少为一个单位。开发团队自组织地领取Sprint 待办产品列表中的工作，领取工作在Sprint 计划会议和Sprint期间按需进行。 产品负责人能够帮助解释清楚所选定的产品待办列表项，并作出权衡。如果开发团队认为工作过多或过少，他们可以与产品负责人重新协商所选的产品待办列表项。开发团队也可以邀请其他人员参加会议，以获得技术或领域知识方面的建议。 在Sprint 计划会议结束时，开发团队应该能够向产品负责人和 Scrum Master解释他们将如 何以自组织团队的形式完成Sprint目标并开发出预期的产品增量。 每日Scrum站会每日Scrum 站会是一个以 15 分钟为限的事件，它让开发团队同步开发活动，并为接下了 的 24 小时制定计划。这需要检视上次每日站会以来的工作和预测下次每日站会之前所能够完成的工作。 每日Scrum 站会在同一时间同一地点举行，以便降低复杂性。在会议上，每一个开发团队 成员都需要说明: 昨天，我为帮助开发团队达成Sprint目标做了什么? 今天，我为帮助开发团队达成Sprint目标准备做什么? 是否有任何障碍在阻碍我或开发团队达成Sprint目标? 每日Scrum 站会增进交流沟通、减少其他会议、发现开发过程中需要移除的障碍、突显并促进快速地做决策、提高开发团队的认知程度。这是一个进行检视与适应的关键会议。 Sprint 回顾会议Sprint 回顾会议是Scrum团队检视自身并创建下一个Sprint改进计划的机会。 Sprint 回顾会议发生在 Sprint 评审会议结束之后，下个Sprint 计划会议之前。对于长度为 一个月的Sprint来说，会议的限时为 3 小时。对于较短的Sprint来说，会议时间通常会缩短。Scrum Master要确保会议举行，并且每个参会者都明白会议的目的。Scrum Master教导大家遵守时间盒的规则。Scrum Master作为Scrum过程的责任者，作为团队的一员参加该会议。 Sprint回顾会议的目的在于: 检视前一个Sprint中关于人、关系、过程和工具的情况如何 找出并加以排序做得好的和潜在需要改进的主要方面 制定改进Scrum团队工作方式的计划 Scrum Master鼓励Scrum团队在Scrum的过程框架内改进开发过程和实践，使得他们能在 下个Sprint中更高效更愉快。在每个Sprint回顾会议中，Scrum团队通过适当地调整“完成”的定义的方式来计划提高产品质量。 在Sprint回顾会议结束时，Scrum团队应该明确接下来的Sprint中需要实施的改进。在下一个Sprint中实施这些改进是基于Scrum团队对自身的检视而做出的适当调整。虽然改进 可以在任何时间执行，Sprint回顾会议提供了一个专注于检视和适应的正式机会。 SCRUM的五个价值观 承诺 – 愿意对目标做出承诺 专注– 把你的心思和能力都用到你承诺的工作上去 开放– Scrum把项目中的一切开放给每个人看 尊重– 每个人都有他独特的背景和经验 勇气– 有勇气做出承诺，履行承诺，接受别人的尊重 用户故事用户故事是从用户的角度来描述用户渴望得到的功能。一个好的用户故事包括三个要素： 角色：谁要使用这个功能。 活动：需要完成什么样的功能。 商业价值：为什么需要这个功能，这个功能带来什么样的价值。 举例：作为一个“网站管理员”，我想要“统计每天有多少人访问了我的网站”，以便于“我的赞助商了解我的网站会给他们带来什么收益。” 需要注意的是用户故事不能够使用技术语言来描述，要使用用户可以理解的业务语言来描述。 敏捷估算无论是团队研发一款产品或者开发某一个项目，我们都需要回答“我们大概什么时间能够完成？”， 或者到某一个时间点，我们能够做到什么程度， 因此和传统的开发模式一样，我们在工作开始之前需要对我们需要做的事情进行工作量的估算。 相对与传统的工作量估算方式，敏捷估算有如下几个特点： 1. 团队集体估算在Scrum的开发过程中，团队共担责任，集体承诺每个Sprint的工作，因此对于工作量的估算敏捷团队采用集体估算的方式。 2. 估算大小，而不是估算时间周期，使用相对估算，而不是绝对估算一瓶矿泉水，让一个3岁的小妹妹把它喝完所花的时间和一个成年人把它喝完所花的时间肯定不一样，因此同一项工作，不同能力的人完成它花费的时间显然是不一样的。如果我们要估算从家到公司的绝对距离时多少公里，您可能不一定知道，但是如果您时做地铁上班，从家里到公司有多少站，你一定很容易知道，当我们知道有多少站之后，我们就可以大概清楚路上需要花多长时间了。敏捷估算时，我们不会估算绝对时间和周期，我们估算大小，和相对值，也就是倍数。敏捷估算时，我们使用故事点作为计量单位，它是一个倍数，我们会先找一个我们认为最小的一个功能的大小作为参考基准（我司现行的1个故事点的定义是 一个熟练工一个小时的工作量）定义为1个故事点，把其它的故事和它做比较，如果是2倍大小，就是2个故事点，如果是5倍大小，就是5个故事点。 3. 我司对于1个故事点的定义笼统的讲：一个熟练工一个小时的工作量 熟练工得定义：没有技术和业务的障碍 例如有这么一个需求： 用react实现一个前端的展示页面在评估点数的时候就要默认这个人熟悉react 再例如有这么一个需求：H5落地页接入微信支付在评估点数的时候就要默认这个人熟悉我们的支付流程 增长团队场景再现三个角色产品负责人 - 清华开发团队 - 黄攀、家园、肖恒、亮宇Scrum Master - 聪哥 事件流程Sprint增长团队是一周一个Sprint Sprint 计划会议在每周5下午2点举行增长团队全体人员参会产品负责人会根据以往的经验从产品待办事项列表中摘取下个Sprint要完成的Story，并排好优先级对于每个Story，产品负责人会按照用户故事的三个要素来描述Story，开发团队认同后开始敏捷估算，俗称估点开发团队每个人会按照 1个熟练工1个小时的产出 为参照估出点数，根据各自点数协商一个大家都能接受的点数作为最终点数会议期间聪哥会指导并保证会议有序进行 每日Scrum站会下周一到周五每天早上9点会在毛里求斯进行站会，要求开发团队全员参与并且要站着，围城一圈按照顺时针成员依次发言，内容仅为： 昨天干了什么？ 今天要干什么？ 有什么风险需要谁的帮助？ Sprint 评审会议每周五下午2点，在Sprint计划会议之前进行清空这个Sprint 的Story并归档看看有没有Story没有完成，为什么，形成经验并在下次注意反思有什么做得不好的地方，形成经验]]></content>
      <categories>
        <category>Management</category>
      </categories>
      <tags>
        <tag>scrum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sidekiq]]></title>
    <url>%2Fsidekiq%2F</url>
    <content type="text"><![CDATA[job lifecycle Processed - successfully completed, and no further action will be taken. Failed - the number of times all jobs were executed by Sidekiq and raised an error. Since the default retry policy is 25, a single job can lead to Failed increasing by 25. It’s important to note that a job will never end up in Failed, as it’s a purely transitive state. The only possible final states are Processed or Dead. Busy - currently processing. Enqueued - waiting for a turn in the processing queue (listed in chronological order, by queue). Retries - failed, but will be automatically retried sometime in the future (listed in chronological order). Scheduled - configured to be run at some point in the future (may be enqueued when their processing time comes up). Dead - will no longer be retried but is saved so it can be manually retried at some point in the near future. 注意事项证明改变worker后重启Sidekiq ，之前的job用的是新的worker的代码第一步新建一个TestWorker，作用就是输出A1234567class TestWorker include Sidekiq::Worker def perform() puts &apos;=======================================A======================================&apos; endend 第二步进入rails console，约定1分钟后执行1TestWorker.perform_in (Time.now + 60.seconds) 第三步关闭Sidekiq，并修改worker，使其输出B1234567class TestWorker include Sidekiq::Worker def perform() puts &apos;=======================================B======================================&apos; endend 第四步重启Sidekiq，看输出 结论由上图可得，输出的是B由此可以看出，Redis队列里存的只是worker的参数和名字，具体执行worker的时候，再去拿worker的代码 优雅、安全的shut down Sidekiq 在部署流程中尽早的发送TSTP信号，它会告诉Sidekiq停止接收新任务，然后继续执行手头的job直到结束 在部署流程中尽可能迟的发送TERM信号，它会告诉Sidekiq在N秒后exit 在你的部署流程中使用TSTP+TERM会保证SidekiqN秒后一定会exit，如果N秒后这个job还没有执行完毕，Sidekiq会把这个job返还给Redis12345kill -TSTP pid//defaults to 8 secondskill -TERM pid//25 secondskill -TERM -t 25 pid The TSTP signal is sent to a process by its controlling terminal to request it to stop temporarily. It is commonly initiated by the user pressing Control-Z. Unlike SIGSTOP, this process can register a signal handler for or ignore the signal. The TERM signal is sent to a process to request its termination. Unlike the KILL signal, it can be caught and interpreted or ignored by the process. This signal allows the process to perform nice termination releasing resources and saving state if appropriate. It should be noted that SIGINT is nearly identical to SIGTERM. 保持job参数的简单 Sidekiq传递给Redis的数据首先会使用JSON.dump去处理 Redis传递给Sidekiq的数据会使用JSON.load去处理 所以不要传递symbols, named parameters or complex Ruby objects (like Date or Time!)123456//badquote = Quote.find(quote_id)SomeWorker.perform_async(quote)//goodSomeWorker.perform_async(quote_id) 工作中的最佳实践不要用objects，不要用symbol，我们只认准string！！！！ 保持job的幂等性和事务性 幂等性：无副作用的运行多次 事务性：要么都成功，要么都失败 工作中的最佳实践关于推送微信消息的worker，一个worker只推一条消息或者只推一个学生所关联家长的消息 pro版本优点更好的可靠性Sidekiq客户端推送job到Redis问题描述当从Sidekiq客户端推送job到Redis时，如果出现网络问题，job将会丢失 解决方案客户端会维护一个job队列，只有推送成功，job才会出队 具体配置1Sidekiq::Client.reliable_push! unless Rails.env.test? Sidekiq具体执行job时从Redis消费问题描述Sidekiq正在执行一个job的时候突然crash，在Redis中job已经被消费了，但是这个job却没有执行完毕，重启Sidekiq后这个job丢失 解决方案Redis中的队列统一都用的Lists数据结构普通的出队操作是BRPOP，Pro版本用了RPOPLPUSH，区别如下：1234// BRPOP会对queue这个队列执行出队操作// RPOPLPUSH会将queue1出队的job在queue2入队BRPOP queueRPOPLPUSH queue1 queue2 所以本质上Pro新增了一个队列来保存正在执行的job，只有job执行成功后才会真正的出队 具体配置123Sidekiq.configure_server do |config| config.super_fetch!end job从scheduled queue到enqueues queue问题描述当job从scheduled队列到等待执行队列需要跟Sidekiq客户端进行两次交互 Sidekiq客户端请求job从scheduled队列出队 Sidekiq客户端请求job进入等待执行队列 如果两步之间出现问题，很可能丢失这个job 解决方案Pro用Lua重写了上述两个步骤并且保证其原子性 具体配置123Sidekiq.configure_server do |config| config.reliable_scheduler!end Expiring Jobs过期时间后这个job就不再执行了 initializer:1require &apos;sidekiq/pro/expiry&apos; Statically:12345class SomeWorker include Sidekiq::Worker sidekiq_options expires_in: 1.hour ...end Dynamically:12// expires_in must be a relative time, not an absolute timestamp.SomeWorker.set(expires_in: 1.day).perform_async(...) Web UI支持搜索Type any substring that can be found in the job data: the worker class, a snippet of the arguments, etc.配置：123456require &apos;sidekiq-pro&apos;require &apos;sidekiq/pro/web&apos;Your::Application.routes.draw do mount Sidekiq::Web =&gt; &apos;/sidekiq&apos;end job运行时的信息实时发送到Statsd上官方文档 MetricsSidekiq Pro will send the following metrics for a worker named Foo::BarWorker:123456789jobs.Foo.BarWorker.count =&gt; counterjobs.Foo.BarWorker.success =&gt; counterjobs.Foo.BarWorker.failure =&gt; counterjobs.Foo.BarWorker.perform =&gt; gauge (time)jobs.Foo.BarWorker.perform_count =&gt; counterjobs.Foo.BarWorker.perform_sum =&gt; counterjobs.count =&gt; counterjobs.success =&gt; counterjobs.failure =&gt; counter 配置Sidekiq Pro can send runtime metrics to Statsd for distribution. Set up a global metrics handler and add the middleware to track job execution.123456789require &apos;datadog/statsd&apos; # gem &apos;dogstatsd-ruby&apos;Sidekiq::Pro.dogstatsd = -&gt;&#123; Datadog::Statsd.new(&quot;metrics.example.com&quot;, 8125) &#125;Sidekiq.configure_server do |config| config.server_middleware do |chain| require &apos;sidekiq/middleware/server/statsd&apos; chain.add Sidekiq::Middleware::Server::Statsd endend Pro API官方文档1234567jid = MyWorker.perform_asyncqueue = Sidekiq::Queue.newqueue.delete_job(jid)MyWorker.perform_asyncqueue = Sidekiq::Queue.newqueue.delete_by_class(MyWorker) Batches官方文档 You can create a set of jobs to execute in parallel and then execute a callback when all the jobs are finished.]]></content>
      <categories>
        <category>ROR</category>
      </categories>
      <tags>
        <tag>Sidekiq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经济解释 第一章]]></title>
    <url>%2Feconomic-explanation%2F</url>
    <content type="text"><![CDATA[现象必有规律现象有规律，自古皆然。我们知其然，但不一定知起所以然。既知其然，就很想知其所以然，这是人的好奇心。我们要作解释，科学也就由此而起。科学的形成基于三个重要信念： 主观判断要客观认同 现象必有规律 现象的发生一定有其原因 推测和解释是同一回事 事实不能解释事实特殊理论与套套逻辑ad hoc theory：内容过多，只能特殊地解释一个现象，没有一般性的解释力科学的进步，不是因为对的理论代替了错的，而是有较广泛解释力的，替代了较狭窄的。特殊理论内容太多了，而套套逻辑则没有内容。可取的理论，一定在特殊逻辑和套套逻辑之间tautology：在任何情况下都不可能错eg：理性经纪人假设、MV=PQ从不同角度看同一数量给套套理论加上约束或局限条件，增加其内容，将其变成理论科学的进步，往往是从一个极端或另一个极端开始，逐步向中间发展 可能被事实推翻的重要性科学不是求对，也不是求错，科学求的是“可能被事实推翻”。可能被事实推翻而没有推翻，就算是被证实（confirmed）了今天可能被事实推翻而没有被推翻的理论，明天可能晚节不保——这是科学进步的过程理论不应该以对或错来衡量refutable by factstestable or refutable implication验证一个理论含意的方法，是以事实反证fallacy of denying the antecedent 模糊不清与互相矛盾可以解释现象的理论，必然有被现象推翻的可能马克思的剩余价值定义模糊，不能被验证，同样也不可能错，警惕“不错就是绝对”四种情况会使一个理论免于被事实推翻的可能： 模糊不清 互相矛盾 非事实 无限制 非事实与无限制需求量本身是抽象的，无法观察的，但是假设需求定律是对的话，价格上涨，需求就下降，这是可以被观察到的事实。这些事实的含义就是需求定律本身（本身不可以被验证的需求定律所推出来的可以被验证的含意）”不均衡“可以解作因为被推断的现象没有限制，理论因而缺少了可能被事实推翻的含意，而”均衡“则是指因为有限制而达到可以验证的理论抽象的理论，本身不能被事实验证；抽象的理论要有解释力，必须又可以被事实验证的一个或多个含意。可验证的含意，要有被事实推翻的可能 理论的真实性三种非真实我们接受： 理论本身必定有抽象成分，因为事实不能解释事实 为了简化 附加的验证条件（test condition） 以抽象思想或概念为起点的科学理论，”非真实“是需要的，因为事实不能自作解释。”不可能太详尽“与”简化“——这些事容许的。但验证条件不能与真实世界脱节 什么是含意（implication）：逻辑包含（logical implication）表明两个陈述或句子存在的关系。这种关系转成口头语言叫做“逻辑上包含”或者“如果就（if/then）”，这个符号由指向右的双线箭头（=&gt;）表示。如果A和B存在这样的状态那么A=&gt;B意思是A逻辑包含B或者如果A那么B。”包含”这个词用语极强可能性的情况]]></content>
      <categories>
        <category>Economics</category>
      </categories>
      <tags>
        <tag>张五常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB - The Good, The Bad, and The Ugly]]></title>
    <url>%2Fdatabase-MongoDB%2F</url>
    <content type="text"><![CDATA[For those who are new to MongoDB, it’s a NoSQL-Document database. Documents comprise sets of key-value pairs and are the basic unit of data in MongoDB. It is definitely one of the most popular NoSQL databases as of now. It’s widely accepted and fits a wide variety of use cases (though not all of them). In this article of the good, the bad, and the ugly, I would like to give a brief overview based on my experience with MongoDB during the past few years. The GoodSince MongoDB is as popular as it is today, there should be more good than the bad and the ugly. If not, developers wouldn’t accept it. Below are a few good things about MongoDB. Flexible Data ModelIn today’s dynamic use cases and ever-changing applications, having a flexible data model is a boon. A flexible data model means that there is no predefined schema, and the document can hold any set of values based on any key. Expressive Query SyntaxThe query language of MongoDB is very expressive and is easy to understand. Many would say that it’s not like SQL. But why should we stick to a SQL-like query language when we can move forward and be more expressive and simple? Easy to LearnMongoDB is easy to learn and quick to start with. The basic installation, setup, and execution doesn’t take more than a few hours. The more robust setup might be complex, but I will talk about it later. You should be able to use the MongoDB database with ease in your project. PerformanceQuery performance is one of the strong points of MongoDB. It stores most of the workable data in RAM. All data is persisted in the hard disk, but during a query, it does not fetch the data from the hard disk. It gets it from the local RAM and, hence, is able to serve much faster. Here, it is important to have the right indexes and enough RAM to benefit from MongoDB’s performance. Scalable and ReliableMongoDB is highly scalable, using shards. Horizontal scalability is a big plus in most NoSQL databases. MongoDB is no exception. It is also highly reliable due to its replica sets, and the data is replicated in more nodes asynchronously. Async DriversNonblocking IO using async drivers are essential in all modern applications that are built for speed. MongoDB has async driver support for most of the popular languages. DocumentationHaving a good documentation can make developers’ lives a lot easier, especially when the developer is new to the technology. MongoDB has superb documentation. Text SearchIf you are building a website that needs to search all of your data, text search is essential. For example, an eCommerce website with a text-search-enabled database can be a lot more lucrative for the users. Server-Side ScriptIf you need some operations to be performed on the server side and not in your application, you can do that in MongoDB. Put your list of mongo statements in a .js file and execute mongo yourFile.js. Documents = ObjectsThe good thing about having a document database is that your object can directly be stored as a single document in MongoDB. There is no need of an ORM here. The BadWe looked at the various good things about MongoDB. Below are the few bad things. I am sure the critics are more interested in this part. MongoDB can be evil if we use it in for an improper use case. TransactionsNowadays, there are very few applications that actually require transactions. But some applications still need it. MongoDB, unfortunately, does not support transactions. So if you need to update more than one document or collection per user request, don’t use MongoDB. It may lead to corrupted data, as there is no ACID guarantee. Rollbacks have to be handled by your application. No TriggersIn RDBMSs, we have the luxury of triggers, which have saved us in many cases. This luxury is missing in MongoDB. StorageMongoDB needs more storage than other popular databases. The introduction of WiredTiger in MongoDB 3.0 has solved the storage issue, but using WiredTiger may not be ideal for most of the applications. Disk CleanupMongoDB does not automatically clean up the disk space. So if the documents are rewritten or deleted, the disk space is not released. This happens during restart or has to be done manually. The UglySometimes, the ugly can be worse than the bad. It’s important to know the ugly part before using the technology. It does not stop you from using the product, but it can make your life very tough. Hierarchy of SelfIf you have a data model where an object can have recursive children (i.e., same object type is a child of an object and it keeps going for ‘n’ levels), the MongoDB document can become very ugly. Indexing, searching, and sorting these recursive embedded documents can be very hard. JoinsJoining two documents is also not simple in MongoDB. Though MongoDB 3.2 supports left outer joins (lookup), it is not yet mature. If your applications require pulling data from multiple collections in a single query, it might not be possible. Hence you have to make multiple queries, which might make your code look a bit messy. IndexingThough speed is advertised as a big plus of MongoDB, it is achievable only if you have the right indexes. If you end up having poorly implemented indexes or composite indexes in an incorrect order, MongoDB can be one of the slowest databases. If you have a lot of filter by and sort by fields, you may end up having a lot of indexes on a collection, which, of course, is not good. Duplicate DataYou may end up having a lot of duplicate data, as MongoDB does not support well-defined relationships. Updating this duplicate data can be hard and, also due to lack of ACID compliance, we might end up having corrupted data. ConclusionOverall, MongoDB is a good database, provided it suits your use case. If it does not, it can get very ugly. Try using it in the wrong place and you will get burned. Analyze it well and consult an expert. You will definitely enjoy using it when it’s right. As for the bad and the ugly parts, you can work around a few of them using the design patterns which I have explained in my article MongoDB Design Patterns. MongoDB Best PracticesA few MongoDB best practices are listed below: Hardware Ensure your working set fits in RAM. Use compression. Run a single MongoDB per server. Use SSDs for write-heavy applications. Data Model Store all data for a record in a single document. Avoid large documents. Avoid unnecessarily long field names. Eliminate unnecessary indexes. Remove indexes that are prefixes of other indexes. Application Update only modified fields. Avoid negation in queries. Run explain() for every complex query. Use covered queries when possible. Use bulk inserts when needed. Setup and Configuration Have at least one secondary and one arbiter. Set write concern to 2 when the data is critical. Havea daily dump of data for backup.]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP 1.X]]></title>
    <url>%2Fnetwork-HTTP1-x%2F</url>
    <content type="text"><![CDATA[持久连接HTTP 1.1 的一个主要改进就是引入了持久 HTTP 连接持久连接可以避免第二次 TCP 连接时的三次握手、消除另一次 TCP 慢启动的往返，节约 整整一次网络延迟比如TCP 连接要发送 N 次 HTTP 请求 没有持久连接，每次请求都会导致两次往返延迟 有持久连接，只有第一次请求会导致两次往返延迟，后续请求只会导致一次往返延迟 在启用持久连接的情况下，N 次请求节省的总延迟时间就是(N-1)×RTT HTTP 管道持久 HTTP 可以让我们重用已有的连接来完成多次应用请求，但多次请求必须严格 满足先进先出(FIFO)的队列顺序:发送请求，等待响应完成，再发送客户端队列 中的下一个请求。 HTTP 管道是一个很小但对上述工作流却非常重要的一次优化。 管道可以让我们把 FIFO 队列从客户端(请求队列)迁移到服务器(响应队列) 要理解这样做的好处，需要看下图 服务器处理完第一次请求后， 会发生了一次完整的往返:先是响应回传，接着是第二次请求。在此期间服务器空 闲。如果服务器能在处理完第一次请求后，立即开始处理第二次请求呢?甚至，如 果服务器可以并行或在多线程上或者使用多个工作进程，同时处理两个请求呢 局限性HTTP 1.x只能严格串行地返回响应。特别是，HTTP 1.x不允许一个连接上的多个 响应数据交错到达(多路复用)，因而一个响应必须完全返回后，下一个响应才会开 始传输。为说明这一点，我们可以看看服务器并行处理请求的情况图演示了如下几个方面: HTML 和 CSS 请求同时到达，但先处理的是 HTML 请求 服务器并行处理两个请求，其中处理HTML用时40ms，处理CSS用时20ms CSS 请求先处理完成，但被缓冲起来以等候发送 HTML 响应 发送完HTML响应后，再发送服务器缓冲中的CSS响应 即使客户端同时发送了两个请求，而且 CSS 资源先准备就绪，服务器也会先发送 HTML 响应，然后再交付 CSS。这种情况通常被称作队首阻塞，并经常导致次优化 交付:不能充分利用网络连接，造成服务器缓冲开销，最终导致无法预测的客户端 延迟。假如第一个请求无限期挂起，或者要花很长时间才能处理完，怎么办呢?在 HTTP 1.1 中，所有后续的请求都将被阻塞，等待它完成 问题 一个慢响应就会阻塞所有后续请求 并行处理请求时，服务器必须缓冲管道中的响应，从而占用服务器资源，如果有个响应非常大，则很容易形成服务器的受攻击面 响应失败可能终止TCP连接，从页强迫客户端重新发送对所有后续资源的请求，导致重复处理 由于可能存在中间代理，因此检测管道兼容性，确保可靠性很重要 如果中间代理不支持管道，那它可能会中断连接，也可能会把所有请求串联起来 使用多个TCP连接浏览器开发商允许我们并行打开多个 TCP会话大多数现代浏览器，包括桌面和移动浏览器，都支持每个 主机打开 6 个连接 域名分区HTTP 1.x协议的一项空白强迫浏览器开发商引入并维护着连接池，每个主机最多6 个 TCP 流。好的一方面是对这些连接的管理工作都由浏览器来处理。作为应用开发 者，你根本不必修改自己的应用。不好的一方面呢，就是 6 个并行的连接对你的应 用来说可能仍然不够用 根据HTTP Archive的统计，目前平均每个页面都包含90多个独立的资源，如果这 些资源都来自同一个主机，那么仍然会导致明显的排队等待 实际上， 何必把自己只限制在一个主机上呢?我们不必只通过一个主机(例如 www.example. com)提供所有资源，而是可以手工将所有资源分散到多个子域名:{shard1, shardn}.example.com。由于主机名称不一样了，就可以突破浏览器的连接限制，实 现更高的并行能力。域名分区使用得越多，并行能力就越强! 当然，天下没有免费的午餐，域名分区也不例外:每个新主机名都要求有一次额外 的 DNS 查询，每多一个套接字都会多消耗两端的一些资源，而更糟糕的是，站点作 者必须手工分离这些资源，并分别把它们托管到多个主机上 实践中，把多个域名(如 shard1.example.com、shard2.example.com)解析 到同一个IP地址是很常见的做法。所有分区都通过CNAME DNS记录指 向同一个服务器，而浏览器连接限制针对的是主机名，不是 IP 地址。另 外，每个分区也可以指向一个 CDN 或其他可以访问到的服务器 缺点 更多的套接字会占用客户端、服务器以及代理的资源，包括内存缓冲区和CPU 时钟周期 并行TCP流之间竞争共享的带宽 由于处理多个套接字，实现复杂性更高 即使并行TCP流，应用的并行能力也受限制]]></content>
      <categories>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HTTP 2.0]]></title>
    <url>%2Fnetwork-http2-0%2F</url>
    <content type="text"><![CDATA[HTTP 2.0 相比于 HTTP 1.X，可以说是大幅度提高了 web 的性能。 在 HTTP 1.X 中，为了性能考虑，我们会引入雪碧图、将小图内联、使用多个域名等等的方式。这一切都是因为浏览器限制了同一个域名下的请求数量，当页面中需要请求很多资源的时候，队头阻塞（Head of line blocking）会导致在达到最大请求数量时，剩余的资源需要等待其他资源请求完成后才能发起请求在 HTTP 1.X 中，因为队头阻塞的原因，你会发现请求是这样的在 HTTP 2.0 中，因为引入了多路复用，你会发现请求是这样的 队首阻塞就是需要排队，队首的事情没有处理完的时候，后面的人都要等着 http1.0的队首阻塞对于同一个tcp连接，所有的http1.0请求放入队列中，只有前一个请求的响应收到了，然后才能发送下一个请求。 可见，http1.0的队首组塞发生在客户端 http1.1的队首阻塞对于同一个tcp连接，http1.1允许一次发送多个http1.1请求，也就是说，不必等前一个响应收到，就可以发送下一个请求，这样就解决了http1.0的客户端的队首阻塞。但是，http1.1规定，服务器端的响应的发送要根据请求被接收的顺序排队，也就是说，先接收到的请求的响应也要先发送。这样造成的问题是，如果最先收到的请求的处理时间长的话，响应生成也慢，就会阻塞已经生成了的响应的发送。也会造成队首阻塞。 可见，http1.1的队首阻塞发生在服务器端 http2是怎样解决队首阻塞的http2无论在客户端还是在服务器端都不需要排队，在同一个tcp连接上，有多个stream，由各个stream发送和接收http请求，各个steam相互独立，互不阻塞。 只要tcp没有人在用那么就可以发送已经生成的requst或者reponse的数据，在两端都不用等，从而彻底解决了http协议层面的队首阻塞问题 二进制传输HTTP 2.0 中所有加强性能的核心点在于此。在之前的 HTTP 版本中，我们是通过文本的方式传输数据。在 HTTP 2.0 中引入了新的编码机制，所有传输的数据都会被分割，并采用二进制格式编码 多路复用在 HTTP 2.0 中，有两个非常重要的概念，分别是帧（frame）和流（stream） 帧代表着最小的数据单位，每个帧会标识出该帧属于哪个流，流也就是多个帧组成的数据流。 多路复用，就是在一个 TCP 连接中可以存在多条流。换句话说，也就是可以发送多个请求，对端可以通过帧中的标识知道属于哪个请求。通过这个技术，可以避免 HTTP 旧版本中的队头阻塞问题，极大的提高传输性能 Notes 所有通信在一个TCP连接上完成 流是一个虚拟信道，每个流都有一个唯一的整数标识符 消息指逻辑上的HTTP消息，比如请求和响应，由一个或者多个帧组成 帧是最小的通信单位，承载着特定类型的数据，如HTTP header、负荷等 客户端发起的流偶数ID，srever端奇数 HTTP2把HTTP通信的基本单位缩小为一个个帧，这些帧组成 流 中所传输的 消息。很多流可以并发的在同一个TCP连接上交换消息HTTP2虽然只有一条TCP连接，但是在逻辑上分成了很多stream。HTTP2把要传输的信息分割成一个个二进制帧，首部信息会被封装到HEADER frame，相应的request body就放到DATA frame,一个帧你可以看成路上的一辆车,只要给这些车编号，让1号车都走1号门出，2号车都走2号门出，就把不同的http请求或者响应区分开来了。但是，这里要求同一个请求或者响应的帧必须是有有序的，要保证FIFO的，但是不同的请求或者响应帧可以互相穿插。这就是HTTP2的多路复用，是不是充分利用了网络带宽，是不是提高了并发度 单一长连接在HTTP/2中，客户端向某个域名的服务器请求页面的过程中，只会创建一条TCP连接，即使这页面可能包含上百个资源。 单一的连接应该是HTTP2的主要优势，单一的连接能减少TCP握手带来的时延 。HTTP2中用一条单一的长连接，避免了创建多个TCP连接带来的网络开销，提高了吞吐量 Header 压缩在 HTTP 1.X 中，我们使用文本的形式传输 header，在 header 携带 cookie 的情况下，可能每次都需要重复传输几百到几千的字节。 在 HTTP 2.0 中，使用了 HPACK 压缩格式对传输的 header 进行编码，减少了 header 的大小。并在两端维护了索引表，用于记录出现过的 header ，后面在传输过程中就可以传输已经记录过的 header 的键名，对端收到数据后就可以通过键名找到对应的值 服务端 PushHTTP 2.0 新增的一个强大的新功能，就是服务器可以对一个客户端请求发送多个响应。换句话说，除了对最初请求的响应外，服务器还可以额外向客户端推送资源，而无需客户端明确地请求为什么需要这样一个机制呢？通常的 Web 应用都由几十个资源组成，客户端需要分析服务器提供的文档才能逐个找到它们那为什么不让服务器提前就把这些资源推送给客户端，从而减少额外的时间延迟呢？服务器已经知道客户端下一步要请求什么资源了，这时候服务器推送即可派上用场。事实上，如果你在网页里嵌入过 CSS、JavaScript，或者通过数据 URI 嵌入过其他资源，那你就已经亲身体验过服务器推送了]]></content>
      <categories>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[央行的调节工具]]></title>
    <url>%2Feconomics-MLF%2F</url>
    <content type="text"><![CDATA[央妈最常用法宝公开市场操作，是指央妈通过在公开市场上拿钱买卖有价证券，来调节货币供应量和利率。这法宝其实是个百宝箱，央妈不断鼓捣出了很多小玩意儿，使用起来比较灵便，效果比较温和。最初主要是回购，后来补充了很多工具，有些还挺国际范儿。2013年初，央妈创设了常备借贷便利（SLF），2014年1月引入了短期流动性调节工具（SLO），2014年4月份创设了抵押补充贷款（PSL），2014年9月份创设了中期借贷便利（MLF），2015年10月，又扩大信贷资产质押再贷款扩围。百宝箱的工具越来越丰富多样。 异同下面我们打开百宝箱，来看看这些新奇的工具。我们可以先从三个方面整理比较下： 工具时间长短不同下图直观地显示了各类工具的贷款期限长度（图中圆点表示主要操作期限，三角形是辅助操作期限），从短到长分别为SLO(1-6天)，正/逆回购（7、14、28天）、SLF（一般1-3个月，1、7日较少），MLF（3个月到1年），PSL（3年到5年）。 工具作用不同不同长短的工具用来引导相应不同期限的利率，对应的，SLO引导超短期利率；逆回购引导短期利率，必要时用SLF调整；MLF引导中期利率。但有时央妈也会连续通过期限短的工具解决下稍长期的利率问题。此外，央妈也会通过调剂不同工具的使用量，来调整市场上资金的平均利率高低。 使用范围不同从窄到宽依次是PSL（政策性银行），MLF（政策性银行、商业银行），SLF与SLO（大中型金融机构）。 工具细节回购包括正回购和逆回购，这是央行最常用的工具。逆回购是指央行出钱，买交易商的证券，这样市场上钱就多了，同时约定几天后（7日为主，常态化操作，14、28日为辅），央行再按照原价加一定利息（按一定利率即逆回购利率计算）把券卖给交易商。正回购是指央行卖证券给交易商，交易商给央行钱，这样市场上的钱就少了，同时约定几天后央行再按照最初价加上利息买回来。这样，央行通过控制正/逆回购的节奏、规模、利率，来调节市场上钱的供应量和短期利率水平。（正回购、逆回购的作用经常会搞混，大家只要记住，“水逆水逆”，放水的逆回购，逆回购就是给市场放水，钱就多了） 短期流动性调节工具（Short-term Liquidity Operations，SLO）（为了好记，姑且称为小“酸辣圈”吧）这是用来调节比7天更短的货币供应和利率，也是采用回购的方式，时间从1天到6天都有。例如，2013年12月底，临近年末资金紧张，央行连续5天进行了SLO操作向市场投放钱，将资金利率缓和下来。 常备借贷便利（Standing LendingFacility，SLF），市场俗称“酸辣粉”大型金融机构缺钱了，可以向央行一对一申请抵押贷款，期限一般是1到3个月。一般在市场缺钱的时候，才用SLF来补充流动性，例如2013年下半年；市场不缺钱的是有一般不用，例如2016年第三季度仅操作了22.5亿元。央行可以通过SFL进行短期利率引导，将其作为“利率走廊”上限（下限是超额存款准备金率），就是说，如果市场利率大于某个值，有可能触发央妈强制使用，阻止金融机构之间的拆借，转向央行贷款。这个工具不常用，平常备着，所以叫“常备”。 中期借贷便利（Medium-term Lending Facility，MLF），俗称“麻辣粉”这工具要求借钱的银行投放于三农和小微贷款，期限一般为3个月、6个月和1年。MLF的量很大，央行每月进行常态化操作，2016年前三季度累计开展操作 3.2万亿元，期末余额为 1.9万亿元，成为基础货币供给的重要渠道，并以此引导中期利率。（央妈喜欢麻辣粉多于酸辣粉） 抵押补充贷款（Pledged Supplementary Lending，PSL），就称为“颇酸辣”吧央妈为了支持特定政策性银行完成国家重点项目（例如国开行的棚改项目），直接提供一部分低成本抵押资金，期限是3-5年，这就是抵押补充贷款。 随着利率市场化的推进，为了提高货币调控效果，央妈的百宝箱还在不断丰富着]]></content>
      <categories>
        <category>Economics</category>
      </categories>
      <tags>
        <tag>MLF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP原理]]></title>
    <url>%2Fnetwork-tcp%2F</url>
    <content type="text"><![CDATA[头部对于 TCP 头部来说，以下几个字段是很重要的 Sequence number，这个序号保证了 TCP 传输的报文都是有序的，对端可以通过序号顺序的拼接报文 Acknowledgement Number，这个序号表示数据接收端期望接收的下一个字节的编号是多少，同时也表示上一个序号的数据已经收到 Window Size，窗口大小，表示还能接收多少字节的数据，用于流量控制 标识符 ACK=1：该字段为一表示确认号字段有效。此外，TCP 还规定在连接建立后传送的所有报文段都必须把 ACK 置为一 SYN=1：当SYN=1，ACK=0时，表示当前报文段是一个连接请求报文。当SYN=1，ACK=1时，表示当前报文段是一个同意建立连接的应答报文 FIN=1：该字段为一表示此报文段是一个释放连接的请求报文 建立连接三次握手在 TCP 协议中，主动发起请求的一端为客户端，被动连接的一端称为服务端。不管是客户端还是服务端，TCP 连接建立完后都能发送和接收数据，所以 TCP 也是一个全双工的协议 起初，两端都为 CLOSED 状态。在通信开始前，双方都会创建 TCB(传输控制块Transmission Control Block)。 服务器创建完 TCB 后遍进入 LISTEN 状态，此时开始等待客户端发送数据。 第一次握手客户端选择一个随机序列号x，报文中SYN为1。请求发送后，客户端便进入 SYN-SENT 状态 第二次握手服务端收到连接请求报文段后，如果同意连接，则会发送一个应答，该应答中SYN 和 ACK 都置为1，并且给x+1，同时生成一个自己的随机数y，发送完成后便进入 SYN-RECEIVED 状态 第三次握手当客户端收到连接同意的应答后，还要向服务端发送一个确认报文。这个报文中ACK为1，并且给x 和 y 都 加 1客户端发完这个报文段后便进入ESTABLISHED 状态，服务端收到这个应答后也进入 ESTABLISHED 状态，此时连接建立成功。 为什么还需要第三次应答因为这是为了防止失效的连接请求报文段被服务端接收，从而产生错误。 可以想象如下场景。客户端发送了一个连接请求 A，但是因为网络原因造成了超时，这时 TCP 会启动超时重传的机制再次发送一个连接请求 B。此时请求顺利到达服务端，服务端应答完就建立了请求。如果连接请求 A 在两端关闭后终于抵达了服务端，那么这时服务端会认为客户端又需要建立 TCP 连接，从而应答了该请求并进入 ESTABLISHED 状态。此时客户端其实是 CLOSED 状态，那么就会导致服务端一直等待，造成资源的浪费。 PS：在建立连接中，任意一端掉线，TCP 都会重发 SYN 包，一般会重试五次，在建立连接中可能会遇到 SYN FLOOD 攻击。遇到这种情况你可以选择调低重试次数或者干脆在不能处理的情况下拒绝请求。 拥塞处理拥塞处理和流量控制不同，后者是作用于接收方，保证接收方来得及接受数据。而前者是作用于网络，防止过多的数据拥塞网络，避免出现网络负载过大的情况。 拥塞处理包括了四个算法，分别为：慢开始，拥塞避免，快速重传，快速恢复。cwnd（拥塞窗口）ssthresh（慢开始门限） 慢开始算法慢开始算法，顾名思义，就是在传输开始时将发送窗口慢慢指数级扩大，从而避免一开始就传输大量数据导致网络拥塞 慢开始算法步骤具体如下 连接初始设置拥塞窗口（Congestion Window） 为 1 MSS（一个分段的最大数据量） 每过一个 RTT 就将窗口大小乘二 指数级增长肯定不能没有限制的，所以有一个阈值限制，当窗口大小大于阈值时就会启动拥塞避免算法。 拥塞避免算法拥塞避免算法相比简单点，每过一个 RTT 窗口大小只加一，这样能够避免指数级增长导致网络拥塞，慢慢将大小调整到最佳值。在传输过程中可能定时器超时的情况，这时候 TCP 会认为网络拥塞了，会马上进行以下步骤： 将阈值设为当前拥塞窗口的一半 将拥塞窗口设为 1 MSS 启动慢开始算法 快重传 快重传算法要求首先接收方收到一个失序的报文段后就立刻发出重复确认，而不要等待自己发送数据时才进行捎带确认。接收方成功的接受了发送方发送来的M1、M2并且分别给发送了ACK，现在接收方没有收到M3，而接收到了M4，显然接收方不能确认M4，因为M4是失序的报文段。如果根据可靠性传输原理接收方什么都不做，但是按照快速重传算法，在收到M4、M5等报文段的时候，不断重复的向发送方发送M2的ACK,如果接收方一连收到三个重复的ACK,那么发送方不必等待重传计时器到期，由于发送方尽早重传未被确认的报文段。 快恢复 当发送发连续接收到三个确认时，就执行乘法减小算法，把慢启动开始门限（ssthresh）减半，但是接下来并不执行慢开始算法 此时不执行慢启动算法，而是把cwnd设置为ssthresh的一半， 然后执行拥塞避免算法，使拥塞窗口缓慢增大 Hintcwnd（拥塞窗口）是发送方维持的一个值rwnd（接收窗口）是接收方维持的一个值实际发送速率是min(cwnd, rwnd)接收窗口会随每次 ACK 一起发送，而拥塞窗口则由发送端根据拥塞控制和预防算法动态调整 总结 TCP 三次握手增加了整整一次往返时间 TCP 慢启动将被应用到每个新连接 TCP 流量及拥塞控制会影响所有连接的吞吐量 TCP 的吞吐量由当前拥塞窗口大小控制 优化大多数情况下，TCP 的瓶颈都是延迟，而非带宽 把服务器内核升级到最新版本(Linux:3.2+) 确保 cwnd 大小为 10 禁用空闲后的慢启动 确保启动窗口缩放 减少传输冗余数据 压缩要传输的数据 把服务器放到离用户近的地方以减少往返时间 尽最大可能重用已经建立的 TCP 连接]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浏览器中的Event loop]]></title>
    <url>%2Fbrowser-event-loop%2F</url>
    <content type="text"><![CDATA[众所周知 JS 是门非阻塞单线程语言，因为在最初 JS 就是为了和浏览器交互而诞生的。如果 JS 是门多线程的语言话，我们在多个线程中处理 DOM 就可能会发生问题（一个线程中新加节点，另一个线程中删除节点），当然可以引入读写锁解决这个问题 执行过程JS 在执行的过程中会产生执行环境，这些执行环境会被顺序的加入到执行栈中。如果遇到异步的代码，会被挂起并加入到 Task（有多种 task） 队列中。一旦执行栈为空，Event Loop 就会从 Task 队列中拿出需要执行的代码并放入执行栈中执行，所以本质上来说 JS 中的异步还是同步行为1234567console.log('script start');setTimeout(function() &#123; console.log('setTimeout');&#125;, 0);console.log('script end');以上代码虽然 setTimeout 延时为 0，其实还是异步。这是因为 HTML5 标准规定这个函数第二个参数不得小于 4 毫秒，不足会自动增加。所以 setTimeout还是会在 script end 之后打印。 微任务（microtask） 和 宏任务（macrotask）不同的任务源会被分配到不同的 Task 队列中，任务源可以分为 微任务（microtask） 和 宏任务（macrotask）。在 ES6 规范中，microtask 称为 jobs，macrotask 称为 task1234567891011121314151617console.log('script start');setTimeout(function() &#123; console.log('setTimeout');&#125;, 0);new Promise((resolve) =&gt; &#123; console.log('Promise') resolve()&#125;).then(function() &#123; console.log('promise1');&#125;).then(function() &#123; console.log('promise2');&#125;);console.log('script end');// script start =&gt; Promise =&gt; script end =&gt; promise1 =&gt; promise2 =&gt; setTimeout以上代码虽然 setTimeout 写在 Promise 之前，但是因为 Promise 属于微任务而 setTimeout 属于宏任务，所以会有以上的打印。 微任务包括：process.nextTick ，promise ，Object.observe ，MutationObserver 宏任务包括：script ， setTimeout ，setInterval ，setImmediate ，I/O ，UI rendering 很多人有个误区，认为微任务快于宏任务，其实是错误的。因为宏任务中包括了 script ，浏览器会先执行一个宏任务，接下来有异步代码的话就先执行微任务 总结正确的一次 Event loop 顺序是这样的： 执行同步代码，这属于宏任务 执行栈为空，查询是否有微任务需要执行 执行所有微任务 必要的话渲染 UI 然后开始下一轮 Event loop，执行宏任务中的异步代码 通过上述的 Event loop 顺序可知，如果宏任务中的异步代码有大量的计算并且需要操作 DOM 的话，为了更快的 界面响应，我们可以把操作 DOM 放入微任务中]]></content>
      <categories>
        <category>Browser</category>
      </categories>
      <tags>
        <tag>Event loop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浏览器中的Cookie]]></title>
    <url>%2Fbrowser-cookie%2F</url>
    <content type="text"><![CDATA[cookie 路径cookie 一般都是由于用户访问页面而被创建的，可是并不是只有在创建 cookie 的页面才可以访问这个 cookie默认情况下，只有与创建 cookie 的页面在同一个目录或子目录下的网页才可以访问，这个是因为安全方面的考虑所以不是所有页面都可以随意访问其他页面创建的 cookie example在 “http://www.cnblogs.com/Darren_code/&quot; 这个页面创建一个cookie在”http://www.cnblogs.com/Darren_code/archive/2011/11/07/Cookie.html&quot;这个页面默认就能取到cookie信息可在默认情况下， “http://www.cnblogs.com&quot;或者 “http://www.cnblogs.com/xxxx/&quot; 就不可以访问这个 cookie那么如何让这个 cookie 能被其他目录或者父级的目录访问类，通过设置 cookie 的路径就可以实现12document.cookie = "name=value;path=path"document.cookie = "name=value;expires=date;path=path"最常用的例子就是让 cookie 在根目录下,这样不管是哪个子页面创建的 cookie，所有的页面都可以访问到了:1document.cookie = "name=Darren;path=/" cookie 域路径能解决在同一个域下访问 cookie 的问题，咱们接着说 cookie 实现同域之间访问的问题例如 “www.qq.com&quot; 与 “sports.qq.com” 公用一个关联的域名”qq.com”，我们如果想让 “sports.qq.com” 下的cookie被 “www.qq.com&quot; 访问，我们就需要用到 cookie 的domain属性，并且需要把path属性设置为 “/“1document.cookie = "username=Darren;path=/;domain=qq.com" 发请求带上 cookie以下两个条件缺一不可 Request 需设置 withCredentials 为 true1234567891011var invocation = new XMLHttpRequest();var url = 'http://bar.other/resources/credentialed-content/'; function callOtherDomain()&#123; if(invocation) &#123; invocation.open('GET', url, true); invocation.withCredentials = true; invocation.onreadystatechange = handler; invocation.send(); &#125;&#125; Response 需设置 Access-Control-Allow-Credentials 为 true123456789101112131415HTTP/1.1 200 OKDate: Mon, 01 Dec 2008 01:34:52 GMTServer: Apache/2.0.61 (Unix) PHP/4.4.7 mod_ssl/2.0.61 OpenSSL/0.9.7e mod_fastcgi/2.4.2 DAV/2 SVN/1.4.2X-Powered-By: PHP/5.2.6Access-Control-Allow-Origin: http://foo.exampleAccess-Control-Allow-Credentials: trueCache-Control: no-cachePragma: no-cacheSet-Cookie: pageAccess=3; expires=Wed, 31-Dec-2008 01:34:53 GMTVary: Accept-Encoding, OriginContent-Encoding: gzipContent-Length: 106Keep-Alive: timeout=2, max=100Connection: Keep-AliveContent-Type: text/plain 其他注意 如果设置Access-Control-Allow-Origin为*，请求将不会带上cookie 请设置好http-only，这样浏览器端就不能通过 JS 访问 Cookie，减少 XSS 攻击 属性 作用 value 如果用于保存用户登录态，应该将该值加密，不能使用明文的用户标识 http-only 不能通过 JS 访问 Cookie，减少 XSS 攻击 secure 只能在协议为 HTTPS 的请求中携带 same-site 规定浏览器不能在跨域请求中携带 Cookie，减少 CSRF 攻击]]></content>
      <categories>
        <category>Browser</category>
      </categories>
      <tags>
        <tag>cookie</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浏览器跨域]]></title>
    <url>%2Fbrowser-CORS%2F</url>
    <content type="text"><![CDATA[因为浏览器出于安全考虑，有同源策略。也就是说，如果协议、域名或者端口有一个不同就是跨域，Ajax 请求会失败。我们可以通过以下几种常用方法解决跨域的问题 JSONPJSONP 的原理很简单，就是利用 &lt;script&gt; 标签没有跨域限制的漏洞通过 &lt;script&gt;标签指向一个需要访问的地址并提供一个回调函数来接收数据当需要通讯时123456&lt;script src="http://domain/api?param1=a&amp;param2=b&amp;callback=jsonp"&gt;&lt;/script&gt;&lt;script&gt; function jsonp(data) &#123; console.log(data) &#125;&lt;/script&gt;JSONP 使用简单且兼容性不错，但是只限于 get 请求。在开发中可能会遇到多个 JSONP 请求的回调函数名是相同的，这时候就需要自己封装一个 JSONP，以下是简单实现1234567891011121314151617function jsonp(url, jsonpCallback, success) &#123; let script = document.createElement("script"); script.src = url; script.async = true; script.type = "text/javascript"; window[jsonpCallback] = function(data) &#123; success &amp; success(data); &#125;; document.body.appendChild(script);&#125;jsonp( "http://xxx", "callback", function(value) &#123; console.log(value); &#125;); HTTP访问控制（CORS）MDNCORS需要浏览器和后端同时支持浏览器会自动进行 CORS 通信，实现CORS通信的关键是后端。只要后端实现了 CORS，就实现了跨域。服务端设置 Access-Control-Allow-Origin 就可以开启 CORS。 该属性表示哪些域名可以访问资源，如果设置通配符则表示所有网站都可以访问资源。 预检请求(Preflighted requests CORS)client通过options请求 知道server接受哪些method、header等，来判断真正要发出的请求是否是可接受的 触发条件当请求满足下述任一条件时，即应首先发送预检请求: 使用了下面任一HTTP方法： PUT DELETE CONNECT OPTIONS TRACE PATCH 人为设置了对 CORS 安全的首部字段集合之外的其他首部字段。该集合为 Accept Accept-Language Content-Language Content-Type (but note the additional requirements below) DPR Downlink Save-Data Viewport-Width Width Content-Type 的值不属于下列之一: application/x-www-form-urlencoded multipart/form-data text/plain example Server side code1234567891011app.all('*', function (req, res, next) &#123; // handle all request contain Preflight request and set CORS header res.header('Access-Control-Allow-Origin', req.headers.origin); res.header('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS'); res.header('Access-Control-Allow-Headers', 'Content-Type, Authorization, Origin, X-Requested-With, Accept'); res.header('Access-Control-Allow-Credentials', true); if (req.method === "OPTIONS") &#123;. // handle Preflight request and send res with 200 return res.sendStatus(200); &#125; else &#123; next() // pass control to the next handler &#125;&#125;); CORS规则Request with credential1Origin: http://foo.example Response123Access-Control-Allow-Origin: http://foo.exampleAccess-Control-Allow-Method: POST, GET, OPTIONSAccess-Control-Allow-Credentials: true]]></content>
      <categories>
        <category>Browser</category>
      </categories>
      <tags>
        <tag>CORS</tag>
        <tag>JSONP</tag>
        <tag>OPTIONS</tag>
        <tag>Origin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浏览器事件机制]]></title>
    <url>%2FJS-event%2F</url>
    <content type="text"><![CDATA[事件触发三阶段 document往事件触发处传播，遇到注册的捕获事件会触发 传播到事件触发处时触发注册的事件 从事件触发处往 document 传播，遇到注册的冒泡事件会触发 事件触发一般来说会按照上面的顺序进行，但是也有特例，如果给一个目标节点同时注册冒泡和捕获事件，事件触发会按照注册的顺序执行1234567// 以下会先打印冒泡然后是捕获node.addEventListener('click',(event) =&gt;&#123; console.log('冒泡')&#125;,false);node.addEventListener('click',(event) =&gt;&#123; console.log('捕获 ')&#125;,true) 注册事件通常我们使用 addEventListener 注册事件，该函数的第三个参数可以是布尔值，也可以是对象。对于布尔值 useCapture 参数来说，该参数默认值为 false 。useCapture 决定了注册的事件是捕获事件还是冒泡事件。对于对象参数来说，可以使用以下几个属性: capture，布尔值，和 useCapture 作用一样 once，布尔值，值为 true 表示该回调只会调用一次，调用后会移除监听 passive，布尔值，表示永远不会调用 preventDefault 阻止冒泡 event.stopPropagation(): 事件处理过程中，阻止了事件冒泡，但不会阻击默认行为 return false: 事件处理过程中，阻止了事件冒泡，也阻止了默认行为 event.preventDefault() 阻止了默认行为，但不阻止冒泡 事件代理如果一个节点中的子节点是动态生成的，那么子节点需要注册事件的话应该注册在父节点上12345678910111213&lt;ul id="ul"&gt; &lt;li&gt;1&lt;/li&gt; &lt;li&gt;2&lt;/li&gt; &lt;li&gt;3&lt;/li&gt; &lt;li&gt;4&lt;/li&gt; &lt;li&gt;5&lt;/li&gt;&lt;/ul&gt;&lt;script&gt; let ul = document.querySelector('##ul') ul.addEventListener('click', (event) =&gt; &#123; console.log(event.target); &#125;)&lt;/script&gt;事件代理的方式相对于直接给目标注册事件来说，有以下优点: 节省内存 不需要给子节点注销事件]]></content>
      <categories>
        <category>Browser</category>
      </categories>
      <tags>
        <tag>event</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES6 模块与 CommonJS 模块的差异]]></title>
    <url>%2FJS-module%2F</url>
    <content type="text"><![CDATA[CommonJS模块输出的是一个值的拷贝，ES6 模块输出的是值的引用 CommonJS模块是运行时加载，ES6 模块是编译时输出接口 ES6在有 Babel 的情况下，我们可以直接使用 ES6 的模块化12345678export var firstName = ‘Michael’ === var firstName = 'Michael’; export &#123;firstName&#125;;export function v1() &#123; &#125;; === function v1() &#123; &#125;; export &#123;v1&#125;;import &#123;firstName, lastName, year&#125; from './profile’;import * as circle from './circle’;export default function () &#123; console.log('foo'); &#125;import customName from './export-default’; CommonJSCommonJs 是 Node 独有的规范，浏览器中使用就需要用到Browserify解析了。 每个模块内部，module变量代表当前模块。 这个变量是一个对象，它的exports属性（即module.exports）是对外的接口。加载某个模块，其实是加载该模块的module.exports`属性。1234567891011var x = 5;var addX = function (value) &#123; return value + x;&#125;;module.exports.x = x;module.exports.addX = addX;var example = require('./example.js');console.log(example.x); // 5console.log(example.addX(1)); // 6]]></content>
      <categories>
        <category>JS</category>
      </categories>
      <tags>
        <tag>ES6</tag>
        <tag>CommonJS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS closure]]></title>
    <url>%2FJS-closure%2F</url>
    <content type="text"><![CDATA[定义在作用域链上的对象访问只能向上，内部作用域能够访问外部作用域的变量，但是外部却无法向内部访问所以在JS中，实现外部作用域 访问 内部作用域中变量的方法叫做闭包，这得益于高阶函数的特性：函数可以作为参数或者返回值12345678function A() &#123; let a = 1 function B() &#123; console.log(a) &#125; return B&#125;A()() // 1 经典面试题12345for ( var i=1; i&lt;=5; i++) &#123; setTimeout( function timer() &#123; console.log( i ); &#125;, i*1000 );&#125; 首先因为 setTimeout 是个异步函数，所有会先把循环全部执行完毕，这时候 i 就是 6 了，所以会输出一堆 6使用闭包解决1234567for (var i = 1; i &lt;= 5; i++) &#123; (function(j) &#123; setTimeout(function timer() &#123; console.log(j); &#125;, j * 1000); &#125;)(i);&#125;]]></content>
      <categories>
        <category>JS</category>
      </categories>
      <tags>
        <tag>closure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS this]]></title>
    <url>%2FJS-this%2F</url>
    <content type="text"><![CDATA[JS（ES5）里面有三种函数调用形式 func(p1, p2) obj.child.method(p1, p2) func.call(context, p1, p2) 一般，初学者都知道前两种形式，而且认为前两种形式「优于」第三种形式但是，第三种调用形式，才是正常调用形式1func.call(context, p1, p2) 其他两种都是语法糖，可以等价地变为 call 形式12345func(p1, p2) 等价于func.call(undefined, p1, p2)obj.child.method(p1, p2) 等价于obj.child.method.call(obj.child, p1, p2) 至此我们的函数调用只有一种形式：1func.call(context, p1, p2) this是什么this，就是上面代码中的 contextthis 是你 call 一个函数时传的 context，由于你从来不用 call 形式的函数调用，所以你一直不知道 普通函数中的this: this总是代表它的直接调用者, 例如 obj.func ,那么func中的this就是obj 在默认情况(非严格模式下,未使用 ‘use strict’),没找到直接调用者,则this指的是 window 在严格模式下,没有直接调用者的函数中的this是 undefined 使用call,apply,bind(ES5新增)绑定的,this指的是 绑定的对象 箭头函数中的this默认指向在定义它时,它所处的对象,而不是执行时的对象, 定义它的时候,可能环境是window12345678function a() &#123; return () =&gt; &#123; return () =&gt; &#123; console.log(this) &#125; &#125;&#125;console.log(a()()()) //window箭头函数其实是没有 this 的，这个函数中的 this 只取决于他外面的第一个不是箭头函数的函数的 this。在这个例子中，因为调用 a 符合前面代码中的第一个情况，所以 this 是 window。并且 this 一旦绑定了上下文，就不会被任何代码改变]]></content>
      <categories>
        <category>JS</category>
      </categories>
      <tags>
        <tag>this</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS 原型]]></title>
    <url>%2Fjs-prototype%2F</url>
    <content type="text"><![CDATA[prototype这是一个显式原型属性，只有函数才拥有该属性prototype 一般可读可写 prototype 如何产生的当我们声明一个函数时，这个属性就被自动创建了1function Foo() &#123;&#125; 并且这个属性的值是一个对象（也就是原型），只有一个属性 constructorconstructor 对应着构造函数，也就是 Foo1Foo.prototype.constructor === Foo //true constructorconstructor是一个公有且不可枚举的属性。一旦我们改变了函数的 prototype ，那么新对象就没有这个属性了（当然可以通过原型链取到 constructor）那么你肯定也有一个疑问，这个属性到底有什么用呢？其实这个属性可以说是一个历史遗留问题，在大部分情况下是没用的，在我的理解里，我认为他有两个作用： 让实例对象知道是什么函数构造了它(obj.__proto__.constructor) 如果想给某些类库中的构造函数增加一些自定义的方法，就可以通过 xx.constructor.method 来扩展 __proto__这是每个对象都有的隐式原型属性，指向了创建该对象的构造函数的原型。其实这个属性指向了 [[prototype]]，但是 [[prototype]] 是内部属性，我们并不能访问到，所以使用 __proto__ 来访问。一般只读因为在 JS 中是没有类的概念的，为了实现类似继承的方式，通过 __proto__将对象和原型联系起来组成原型链，得以让对象可以访问到不属于自己的属性。 实例对象的 __proto__ 如何产生的当我们使用 new 操作符时，生成的实例对象拥有了 __proto__属性。 new 的过程 新生成了一个对象 链接到原型 绑定 this 返回新对象 在调用 new 的过程中会发生以上四件事情，我们也可以试着来自己实现一个 new123456789101112function create() &#123; // 创建一个空的对象 let obj = new Object() // 获得构造函数 let Con = [].shift.call(arguments) // 链接到原型 obj.__proto__ = Con.prototype // 绑定 this，执行构造函数 let result = Con.apply(obj, arguments) // 确保 new 出来的是个对象 return typeof result === 'object' ? result : obj&#125; 判断 一个 对象a是不是类b的实例a.__proto__ === b.prototype Function.__proto__ === Function.prototype 对于对象来说，xx.__proto__.contrcutor 是该对象的构造函数，但是在图中我们可以发现 Function.__proto__ === Function.prototype，难道这代表着 Function 自己产生了自己?答案肯定是否认的，要说明这个问题我们先从 Object 说起。从图中我们可以发现，所有对象都可以通过原型链最终找到 Object.prototype ，虽然 Object.prototype 也是一个对象，但是这个对象却不是 Object 创造的，而是引擎自己创建了 Object.prototype 。所以可以这样说，所有实例都是对象，但是对象不一定都是实例接下来我们来看 Function.prototype 这个特殊的对象，如果你在浏览器将这个对象打印出来，会发现这个对象其实是一个函数。这个函数也是引擎自己创建的。首先引擎创建了 Object.prototype ，然后创建了 Function.prototype ，并且通过__proto__ 将两者联系了起来现在可以来解释 Function.__proto__ === Function.prototype 这个问题了。因为先有的 Function.prototype 以后才有的 function Function() ，所以也就不存在鸡生蛋蛋生鸡的悖论问题了。对于为什么 Function.__proto__ 会等于 Function.prototype ，个人的理解是：其他所有的构造函数都可以通过原型链找到 Function.prototype ，并且 function Function() 本质也是一个函数，为了不产生混乱就将 function Function() 的 __proto__联系到了 Function.prototype 上。 总结 Object 是所有对象的爸爸，所有对象都可以通过__proto__ 找到它 Function 是所有函数的爸爸，所有函数都可以通过 __proto__ 找到它 Function.prototype 和 Object.prototype 是两个特殊的对象，他们由引擎来创建 除了以上两个特殊对象，其他对象都是通过构造器 new 出来的 函数的 prototype 是一个对象，也就是原型 对象的 __proto__ 指向构造函数的原型， __proto__ 将对象和构造函数的原型连接起来组成了原型链]]></content>
      <categories>
        <category>JS</category>
      </categories>
      <tags>
        <tag>prototype</tag>
        <tag>__proto__</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前端项目理想架构]]></title>
    <url>%2Fdesign-frontend-dream%2F</url>
    <content type="text"><![CDATA[易于开发 开发工具是否完善 生态圈是否繁荣 社区是否活跃 易于扩展 增加新功能是否容易 新功能是否会显著增加系统复杂度 易于维护 代码是否容易理解 文档是否健全 易于测试 功能的分层是否清晰 副作用少 尽量使用纯函数 易于构建 使用通用的技术和架构 构建工具的选择]]></content>
      <categories>
        <category>设计与架构</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[BFC（Block Formatting Context）]]></title>
    <url>%2FBFC%2F</url>
    <content type="text"><![CDATA[什么是BFC写CSS样式时，对一个元素设置css,我们首先要知道这个元素是块级元素还是行内元素，而BFC就是用来格式化块级盒子的。 Formatting Context：指页面中一个渲染区域，并且拥有一套渲染规则，它决定了其子元素如何定位，以及与其他元素的相互关系和作用 BFC定义：块级格式化上下文，它是指一个独立的块级渲染区域，只有Block-level Box参与，该区域拥有一套渲染规则来约束块级盒子的布局，且与区域外部无关。 BFC的生成满足下列CSS声明之一的元素便会生成BFC： 根元素或其它包含它的元素 float的值不为none overflow的值不为visible position的值不为static display的值为inline-block、table-cell、table-caption flex boxes BFC的布局规则 内部的元素会在垂直方向一个接一个地排列，可以理解为是BFC中的一个常规流 Box垂直方向的距离由margin决定。属于同一个BFC的两个相邻Box的margin会发生重叠 每个元素的左外边距与包含块的左边界相接触(从左往右，否则相反)，即使存在浮动也是如此，这说明BFC中的子元素不会超出它的包含块 BFC的区域不会与float元素区域重叠 计算BFC的高度时，浮动子元素也参与计算 BFC就是页面上的一个隔离的独立容器，容器里面的子元素不会影响到外面的元素，反之亦然 BFC的应用解决margin重叠问题玩css的朋友都知道margin collapse，也就是相邻的垂直元素同时设置了margin后，实际margin值会塌陷到其中较大的那个值。其根本原理就是它们处于同一个BFC，符合“属于同一个BFC的两个相邻元素的margin会发生重叠”的规则1234567891011121314&lt;style&gt; p &#123; color: #f55; background: #fcc; width: 200px; line-height: 100px; text-align:center; margin: 100px; &#125;&lt;/style&gt;&lt;body&gt; &lt;p&gt;Haha&lt;/p&gt; &lt;p&gt;Hehe&lt;/p&gt;&lt;/body&gt; 两个p之间的距离为100px，发送了margin重叠 Box垂直方向的距离由margin决定。属于同一个BFC的两个相邻Box的margin会发生重叠 我们可以在p外面包裹一层容器，并触发该容器生成一个BFC。那么两个P便不属于同一个BFC，就不会发生margin重叠了。12345678910111213141516171819&lt;style&gt; .wrap &#123; overflow: hidden; &#125; p &#123; color: #f55; background: #fcc; width: 200px; line-height: 100px; text-align:center; margin: 100px; &#125;&lt;/style&gt;&lt;body&gt; &lt;p&gt;Haha&lt;/p&gt; &lt;div class="wrap"&gt; &lt;p&gt;Hehe&lt;/p&gt; &lt;/div&gt;&lt;/body&gt; 自适应双栏布局12345678910111213141516171819202122&lt;style&gt; body &#123; width: 300px; position: relative; &#125; .aside &#123; width: 100px; height: 150px; float: left; background: #f66; &#125; .main &#123; height: 200px; background: #fcc; &#125;&lt;/style&gt;&lt;body&gt; &lt;div class="aside"&gt;&lt;/div&gt; &lt;div class="main"&gt;&lt;/div&gt;&lt;/body&gt; 每个元素的margin box的左边， 与包含块border box的左边相接触(对于从左往右的格式化，否则相反)。即使存在浮动也是如此 因此，虽然存在浮动的元素aslide，但main的左边依然会与包含块的左边相接触。根据BFC布局规则第四条: BFC的区域不会与float box重叠 我们可以通过通过触发main生成BFC， 来实现自适应两栏布局123.main &#123; overflow: hidden;&#125;当触发main生成BFC后，这个新的BFC不会与浮动的aside重叠。因此会根据包含块的宽度，和aside的宽度，自动变窄。效果如下 清除内部浮动12345678910111213141516171819&lt;style&gt; .par &#123; border: 5px solid #fcc; width: 300px; &#125; .child &#123; border: 5px solid #f66; width:100px; height: 100px; float: left; &#125;&lt;/style&gt;&lt;body&gt; &lt;div class="par"&gt; &lt;div class="child"&gt;&lt;/div&gt; &lt;div class="child"&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt; 根据BFC布局规则第六条： 计算BFC的高度时，浮动元素也参与计算 为达到清除内部浮动，我们可以触发par生成BFC，那么par在计算高度时，par内部的浮动元素child也会参与计算123.par &#123; overflow: hidden;&#125; 总结其实以上的几个例子都体现了BFC布局规则第五条： BFC就是页面上的一个隔离的独立容器，容器里面的子元素不会影响到外面的元素。反之也如此 因为BFC内部的元素和外部的元素绝对不会互相影响，因此， 当BFC外部存在浮动时，它不应该影响BFC内部Box的布局，BFC会通过变窄，而不与浮动有重叠。同样的，当BFC内部有浮动时，为了不影响外部元素的布局，BFC计算高度时会包括浮动的高度。避免margin重叠也是这样的一个道理]]></content>
      <categories>
        <category>CSS</category>
      </categories>
      <tags>
        <tag>BFC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Huffman-Tree]]></title>
    <url>%2FHuffman-Tree%2F</url>
    <content type="text"><![CDATA[概览给定n个权值作为n个叶子结点，构造一棵二叉树，若该树的带权路径长度达到最小，称这样的二叉树为最优二叉树，也称为哈夫曼树(Huffman Tree)。 特点哈夫曼树是带权路径长度最短的树，权值较大的结点离根较近。 构造方法假设有n个权值，则构造出的哈夫曼树有n个叶子结点，n个权值分别设为 w1、w2、…、wn 将w1、w2、…，wn看成是有n 棵树的森林(每棵树仅有一个结点) 在森林中选出两个根结点的权值最小的树合并，作为一棵新树的左、右子树，且新树的根结点权值为其左、右子树根结点权值之和 从森林中删除选取的两棵树，并将新树加入森林 重复(2)、(3)步，直到森林中只剩一棵树为止，该树即为所求得的哈夫曼树 注意为了使得到的哈夫曼树的结构尽量唯一，通常规定生成的哈夫曼树中每个结点的左子树根结点的权小于等于右子树根结点的权 应用在数据通信中，让使用频率高的用短码，使用频率低的用长码，以优化整个报文编码]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Huffman Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hash indexes and B-Trees]]></title>
    <url>%2FB-Trees%2F</url>
    <content type="text"><![CDATA[Hash IndexesBitcaskDeleting records If you want to delete a key and its associated value, you have to append a special deletion record to the data file (sometimes called a tombstone). When log segments are merged, the tombstone tells the merging process to discard any previous values for the deleted key. Crash recovery you can restore each segment’s hash map by reading the entire segment file from beginning to end and noting the offset of the most recent value for every key as you go along. However, that might take a long time if the segment files are large, which would make server restarts painful. Bitcask speeds up recovery by storing a snapshot of each segment’s hash map on disk, which can be loaded into memory more quickly. Concurrency control As writes are appended to the log in a strictly sequential order, a common implementation choice is to have only one writer thread. Data file segments are append-only and otherwise immutable, so they can be read concurrently by multiple threads. append-only design turns out to be good for several reasons: Appending and segment merging are sequential write operations, which are generally much faster than random writes Concurrency and crash recovery are much simpler if segment files are append- only or immutable. For example, you don’t have to worry about the case where a crash happened while a value was being overwritten, leaving you with a file con‐ taining part of the old and part of the new value spliced together. 合并操作能使有用的数据文件更加紧凑 limitations The hash table must fit in memory, so if you have a very large number of keys, you’re out of luck. In principle, you could maintain a hash map on disk, but unfortunately it is difficult to make an on-disk hash map perform well. It requires a lot of random access I/O Range queries are not efficient. SSTables and LSM-TreesSSTable(Sorted String Table) sequence of key-value pairs is sorted by key each key only appears once within each merged segment fileSSTable 优势 Merging segments用了归并排序，保证新合并的segment依然有序，注意当多个segment出现相同的key时，取most recent segment In order to find a particular key in the file, you no longer need to keep an index of all the keys in memory. 取而代之的是一个稀疏的索引You still need an in-memory index to tell you the offsets for some of the keys, but it can be sparse 支持range query disk writes are sequential the LSM-tree can support remarkably high write throughputConstructing and maintaining SSTables When a write comes in, add it to an in-memory balanced tree data structure (for example, a red-black tree). This in-memory tree is sometimes called a memtable. When the memtable gets bigger than some threshold, write it out to disk as an SSTable file In order to serve a read request, first try to find the key in the memtable, then in the most recent on-disk segment, then in the next-older segment, etc. From time to time, run a merging and compaction process in the background to combine segment files and to discard overwritten or deleted values This scheme works very well. It only suffers from one problem: if the database crashes, the most recent writes (which are in the memtable but not yet written out to disk) are lost. In order to avoid that problem, we can keep a separate log on disk to which every write is immediately appended, just like in the previous section. That log is not in sorted order, but that doesn’t matter, because its only purpose is to restore the memtable after a crash. Every time the memtable is written out to an SSTable, the corresponding log can be discarded. Performance optimizations 当查找一个不存在的key时，首先去memtable找，然后从近到远依次找segment，所以为了避免这种情况发生，storage engines often use additional Bloom filters B-TreesConstructing log-structured indexes 将数据拆分成一个个大小可变的segment，并且能够满足顺序写，与之相比的是B-Trees将数据拆分成大小固定的blocks or pages, traditionally 4 KB in size (sometimes bigger), 一次只能读或者写一个page 一个page就是树的一个节点，每个节点指向多个儿子，节点的儿子数量被称为branching factor 当加入一个key的时候，如果符合条件的page空间被占满，这时这个page会被拆分成2个page，然后再更新他们的parent指针Making B-trees reliable write-ahead log (WAL, also known as a redo log)This is an append-only file to which every B-tree modification must be written before it can be applied to the pages of the tree itself, When the data‐base comes back up after a crash, this log is used to restore the B-tree back to a consistent state concurrency control is required if multiple threads are going to access the B-tree at the same time, 此时需要用锁B-tree optimizations Instead of overwriting pages and maintaining a WAL for crash recovery, some databases (like LMDB) use a copy-on-write scheme. 改动过的page会存在另外一个地方，并且它的父亲page会指向这个新儿子 不用存整个key的名称，只存能标识它的‘缩写’，这样可以节约空间，增加一个page的branching factor，从而缩小depth 存在兄弟指针，保证顺序遍历key的时候不用回到父节点 当大范围查询的时候，直接顺序遍历叶子节点Comparing B-Trees and LSM-TreesAs a rule of thumb,LSM-trees are typically faster for writes,whereas B-trees are thought to be faster for reads one write to the database resulting in multiplewrites to the disk over the course of the database’s lifetime—is known as write amplification. LSM-trees 能维持更高的写吞吐量，一部分是因为更小的write amplification，另一部分是因为顺序写真实他妈快啊]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>B-tree</tag>
        <tag>Hash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UDP原理]]></title>
    <url>%2Fnetwork-UDP%2F</url>
    <content type="text"><![CDATA[面向报文UDP 是一个面向报文（报文可以理解为一段段的数据）的协议。意思就是 UDP 只是报文的搬运工，不会对报文进行任何拆分和拼接操作 具体来说 在发送端，应用层将数据传递给传输层的 UDP 协议，UDP 只会给数据增加一个 UDP 头标识下是 UDP 协议，然后就传递给网络层了 在接收端，网络层将数据传递给传输层，UDP 只去除 IP 报文头就传递给应用层，不会任何拼接操作 不可靠性 UDP 是无连接的，也就是说通信不需要建立和断开连接 UDP 也是不可靠的。协议收到什么数据就传递什么数据，并且也不会备份数据，对方能不能收到是不关心的 UDP 没有拥塞控制，一直会以恒定的速度发送数据。即使网络条件不好，也不会对发送速率进行调整。这样实现的弊端就是在网络条件不好的情况下可能会导致丢包，但是优点也很明显，在某些实时性要求高的场景（比如电话会议）就需要使用 UDP 而不是 TCP 高效因为 UDP 没有 TCP 那么复杂，需要保证数据不丢失且有序到达。所以 UDP 的头部开销小，只有八字节，相比 TCP 的至少二十字节要少得多，在传输数据报文时是很高效的。头部包含了以下几个数据 两个十六位的端口号，分别为源端口（可选字段）和目标端口 整个数据报文的长度 整个数据报文的检验和（IPv4 可选 字段），该字段用于发现头部信息和数据中的错误 传输方式UDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，也就是说 UDP 提供了单播，多播，广播的功能。 与TCP区别 UDP无连接 UDP尽最大努力交付，不保证可靠交付 UDP没有拥塞控制 UDP首部开销小 UDP支持一对一，一对多，多对一和多对多的交互通信 不保证消息交付不确认，不重传，无超时 不保证交付顺序不设置包序号，不重排，不会发生队首阻塞 不跟踪连接状态不必建立连接或重启状态机 不需要拥塞控制不内置客户端或网络反馈机制]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>UDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浏览器端缓存]]></title>
    <url>%2Fperformance-brower-cache%2F</url>
    <content type="text"><![CDATA[在请求一个静态文件的时候（图片，css，js）等，这些文件的特点是文件不经常变化，将这些不经常变化的文件存储起来，对客户端来说是一个优化用户浏览体验的方法。那么这个就是客户端缓存的意义了 通常浏览器缓存策略分为两种：强缓存和协商缓存 强缓存实现强缓存可以通过两种响应头实现：Expires 和 Cache-Control 。强缓存表示在缓存期间不需要请求，直接从缓存拿，state code 为 2001Expires: Wed, 22 Oct 2018 08:41:00 GMT Expires 是 HTTP / 1.0 的产物，表示资源会在 Wed, 22 Oct 2018 08:41:00 GMT 后过期，需要再次请求。并且 Expires 受限于本地时间，如果修改了本地时间，可能会造成缓存失效。1Cache-control: max-age=30 Cache-Control 出现于 HTTP / 1.1，优先级高于 Expires 。该属性表示资源会在 30 秒后过期，需要再次请求 协商缓存如果缓存过期了，我们就可以使用协商缓存来解决问题。协商缓存需要请求，如果缓存有效会返回 304。协商缓存需要客户端和服务端共同实现，和强缓存一样，也有两种实现方式。 Last-Modified 和 If-Modified-Since客户端第一次访问资源的时候，服务端返回资源内容的同时返回了Last-Modifed:Wed, 07 Aug 2013 15:32:18 GMT 服务端在告诉客户端：你获取的这个文件我最后的修改时间是Wed, 07 Aug 2013 15:32:18 GMT 。浏览器在获取这个文件存到缓存中的时候，给缓存中的文件同时记录上这个最后修改时间。第二次访问的时候, 那么服务端访问资源的时候会带上If-Modify-since:Wed, 07 Aug 2013 15:32:18 GMT 客户端询问服务端：喂，我需要的这个资源其实我这边已经有缓存了，我的缓存文件的最后修改时间是这个，如果你那边的资源在这个时间以后没有修改的话，你就告诉我一下就好了，不需要返回实际的资源内容。反之，要是你有修改的话，你就把文件内容返回给我吧 服务端回应说：哦。行为是看下资源是否在这个时间后没有修改过，如果没有修改返回个304告诉客户端，我没有修改过。如果有变化了，我就返回200，并且带上资源内容 需要注意的是如果在本地打开缓存文件，就会造成 Last-Modified 被修改，所以在 HTTP / 1.1 出现了 ETag HeadingETag 和 If-None-Match第一次客户端访问资源的时候，服务端返回资源内容的同时返回了ETag：1234，告诉客户端：这个文件的标签是1234，我如果修改了我这边的资源的话，这个标签就会不一样了。第二次客户端访问资源的时候，由于缓存中已经有了Etag为1234的资源，客户端要去服务端查询的是这个资源有木有过期呢？所以带上了If-None-Match: 1234。告诉服务端：如果你那边的资源还是1234标签的资源，你就返回304告诉我，不需要返回资源内容了。如果不是的话，你再返回资源内容给我就行了。服务端就比较下Etag来看是返回304还是200。所以，ETag 类似于文件指纹，If-None-Match 会将当前 ETag 发送给服务器，询问该资源 ETag 是否变动，有变动的话就将新的资源发送回来。并且 ETag 优先级比 Last-Modified 高 选择合适的缓存策略对于大部分的场景都可以使用强缓存配合协商缓存解决，但是在一些特殊的地方可能需要选择特殊的缓存策略 对于某些不需要缓存的资源，可以使用 Cache-control: no-store ，表示该资源不需要缓存 对于频繁变动的资源，可以使用 Cache-Control: no-cache 并配合 ETag 使用，表示该资源已被缓存，但是每次都会发送请求询问资源是否更新 对于代码文件来说，通常使用 Cache-Control: max-age=31536000 并配合策略缓存使用，然后对文件进行指纹处理，一旦文件名变动就会立刻下载新的文件 浏览器刷新浏览器第一次访问，获取资源内容和cache-control: max-age:600，Last_Modify: Wed, 10 Aug 2013 15:32:18 GMT于是浏览器把资源文件放到缓存中，并且决定下次使用的时候直接去缓存中取了 浏览器中写地址，回车浏览器发现缓存中有这个文件了，好了，就不发送任何请求了，直接去缓存中获取展现。（最快） 按下F5刷新F5就是告诉浏览器，别偷懒，好歹去服务器看看这个文件是否有过期了。于是浏览器就胆胆襟襟的发送一个请求带上If-Modify-since：Wed, 10 Aug 2013 15:32:18 GMT然后服务器发现：诶，这个文件我在这个时间后还没修改过，不需要给你任何信息了，返回304就行了。于是浏览器获取到304后就去缓存中欢欢喜喜获取资源了 Ctrl+F5这个可是要命了，告诉浏览器，你先把你缓存中的这个文件给我删了，然后再去服务器请求个完整的资源文件下来。于是客户端就完成了强行更新的操作… Hint对于静态文件，例如：CSS、图片，服务器会自动完成 Last Modified 和 If Modified Since 的比较，完成缓存或者更新]]></content>
      <categories>
        <category>Frontend Performance</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[将CSS文件放在顶部，将Scripts放在底部]]></title>
    <url>%2Fperformance-css-script%2F</url>
    <content type="text"><![CDATA[将CSS文件放在顶部简介在web页面设计中，一般在HTML中不直接写样式，而是通过link标签引用一个CSS文件在下载页面过程中，document最早开始下载，然后浏览器解析document，下载相关的css、js、图片、字体、视频等资源css文件放置在head中和放在body底部，对css的下载时间不会有影响，但是对页面的呈现有着非常大的影响，与用户的体验密切相关 CSS文件放置在顶部的原理CSS文件放在顶部一方面是因为放置顺序决定了下载的优先级，更关键的是浏览器的渲染机制最理想的情况，我们希望浏览器逐渐的渲染下载好的CSS，将页面逐渐的展现给用户。但是浏览器为了避免样式变化时重新渲染绘制页面元素，会阻塞内容逐步呈现，浏览器等待所有样式加载完成之后才一次性渲染呈现页面如此，CSS文件如果放置底部，浏览器阻止内容逐步呈现，浏览器在等待最后一个CSS文件下载完成的过程中，就出现了“白屏”（新打开连接时为白屏，尔后先出现文字，图片，样式最后出现）这点非常严重，因为在网速非常慢的情况下，css下载时间比较长，这样就给用户带来“白屏”的时间自然也就很长了，用户体验非常差 将Scripts放在底部原因由于下载script会阻塞并行下载，也就是下载script就下载不了其他的资源所以通常将script放到底部 defer 属性 和 async属性 async是异步执行，表示下载完js马上异步执行js defer表示延迟执行，需要等页面资源下载完成后执行 通常还可以再加上一个defer属性执行 JS 代码过长会卡住渲染，对于需要很多时间计算的代码可以考虑使用 WebworkerWebworker 可以让我们另开一个线程执行脚本而不影响渲染]]></content>
      <categories>
        <category>Frontend Performance</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[React多种方法绑定this]]></title>
    <url>%2FReact-bind-this%2F</url>
    <content type="text"><![CDATA[React.createClass 自动绑定React 中创建组件的方式已经很多，比较古老的诸如 React.createClass 应该很多人并不陌生。当然，从 React 0.13 开始，可以使用 ES6 Class 代替 React.createClass 了，这应该是今后推荐的方法。 但是需要知道，React.createClass 创建的组件，可以自动绑定 this。也就是说，this 这个关键字会自动绑定在组件实例上面123// This magically works with React.createClass// because `this` is bound for you.onChange = &#123;this.handleChange&#125;当然很遗憾，对于组件的创建，官方已经推荐使用 class 声明组件或使用 functional 无状态组件： Later, classes were added to the language as part of ES2015, so we added the ability to create React components using JavaScript classes. Along with functional components, JavaScript classes are now the preferred way to create components in React. For your existing createClass components, we recommend that you migrate them to JavaScript classes. 我认为，这其实是 React 框架本身的自我完善和对未来的迎合，是框架和语言发展的大势所趋 渲染时绑定通过前文，我们知道最传统的组件创建方式不会有 this 绑定的困扰。接下来，我们假定所有的组件都采取 ES6 classes 方式声明。这种情况下，this 无法自动绑定。一个常见的解决方案便是1onChange = &#123;this.handleChange.bind(this)&#125;这种方法简明扼要，但是有一个潜在的性能问题：当组件每次重新渲染时，都会有一个新的函数创建。OMG! 这听上去貌似是一个很大的问题，但是其实在真正的开发场景中，由此引发的性能问题往往不值一提（除非是大型组件消费类应用或游戏） 箭头函数绑定这种方法其实和第二种类似，拜 ES6 箭头函数所赐，我们可以隐式绑定 this：1onChange = &#123;e =&gt; this.handleChange(e)&#125;当然，也与第二种方法一样，它同样存在潜在的性能问题。下面将要介绍的两种方法，可以有效规避不必要的性能消耗，请继续阅读。 Constructor 内绑定constructor 方法是类的默认方法，通过new命令生成对象实例时，自动调用该方法。 所以我们可以1234constructor(props) &#123; super(props); this.handleChange = this.handleChange.bind(this);&#125;这种方式往往被推荐为“最佳实践”，也是笔者最为常用的方法。 但是就个人习惯而言，我认为与前两种方法相比，constructor 内绑定在可读性和可维护性上也许有些欠缺。 同时，我们知道在 constructor 声明的方法不会存在实例的原型上，而属于实例本身的方法。每个实例都有同样一个 handleChange，这本身也是一种重复和浪费。 Class 属性中使用 = 和箭头函数这个方法依赖于 ES next 的新特性1234handleChange = () =&gt; &#123; // call this function from render // and this.whatever in here works fine.&#125;; 小总结本文在对比 React 绑定 this 的五种方法的同时，也由远及近了解了 javascript 语言的发展：从 ES5 的 bind， 到 ES6 的箭头函数，再到 ES next 对 class 的改进。 React 作为蓬勃发展的框架也同样在与时具进，不断完善，结合语言特性的发展不断调整着自身。 最后，我们通过这张图片来完整回顾 牛逼做法下面代码是我前同事杨峥的骚操作，构造了一个BaseComponent的类，里面的bindMethods方法会将这个类里面所有以 _ 开头的方法给自动绑定this，其他组件直接继承这个BaseComponent就可以自动绑定了12345678910111213141516171819202122232425262728293031323334353637383940import React from 'react';export default class BaseComponent extends React.Component &#123; constructor(props) &#123; super(props); this.state = this.getInitState(); this.bindMethods(); &#125; getInitState() &#123; return &#123;&#125;; &#125; bindMethods() &#123; // only method startwith only one _ will be mounted let props = []; let obj = this; do &#123; let l = Object.getOwnPropertyNames(obj) .concat(Object.getOwnPropertySymbols(obj).map(s =&gt; s.toString())) .sort() .filter((p, i, arr) =&gt; p[0] === '_' &amp;&amp; p[1] !== '_' &amp;&amp; typeof obj[p] === 'function' &amp;&amp; p !== 'constructor' &amp;&amp; props.indexOf(p) === -1 ); props = props.concat(l) &#125; while ( (obj = Object.getPrototypeOf(obj)) &amp;&amp; //walk-up the prototype chain Object.getPrototypeOf(obj) //not the the Object prototype methods (hasOwnProperty, etc...) ) for (let prop of props) &#123; this[prop] = this[prop].bind(this); &#125; &#125;&#125;每每想起骚哥，那种对代码的追求和那股较真的劲儿，让我深刻的认识到一个泥水匠和一个艺术家的区别PS：他们初次review我代码被打回去了10+次 Passing Arguments to Event HandlersInside a loop it is common to want to pass an extra parameter to an event handler. For example, if id is the row ID, either of the following would work12&lt;button onClick=&#123;(e) =&gt; this.deleteRow(id, e)&#125;&gt;Delete Row&lt;/button&gt;&lt;button onClick=&#123;this.deleteRow.bind(this, id)&#125;&gt;Delete Row&lt;/button&gt;The above two lines are equivalent, and use arrow functions and Function.prototype.bind respectivelyIn both cases, the e argument representing the React event will be passed as a second argument after the ID. With an arrow function, we have to pass it explicitly, but with bind any further arguments are automatically forwarded.]]></content>
      <categories>
        <category>React</category>
      </categories>
      <tags>
        <tag>this</tag>
        <tag>bind</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图片方案的选择]]></title>
    <url>%2Fperformance-image-type%2F</url>
    <content type="text"><![CDATA[基础知识有损vs无损图片文件格式有可能会对图片的文件大小进行不同程度的压缩，图片的压缩分为有损压缩和无损压缩两种 有损压缩。指在压缩文件大小的过程中，损失了一部分图片的信息，也即降低了图片的质量，并且这种损失是不可逆的，我们不可能从有一个有损压缩过的图片中恢复出全来的图片。常见的有损压缩手段，是按照一定的算法将临近的像素点进行合并 无损压缩。指在压缩文件大小的过程中，图片的质量没有任何损耗。我们任何时候都可以从无损压缩过的图片中恢复出原来的信息 索引色vs直接色计算机在表示颜色的时候，有两种形式，一种称作索引颜色,一种称作直接颜色 索引色。用一个数字来代表（索引）一种颜色，在存储图片的时候，存储一个数字的组合，同时存储数字到图片颜色的映射。这种方式只能存储有限种颜色，通常是256种颜色，对应到计算机系统中，使用一个字节的数字来索引一种颜色 直接色。使用四个数字来代表一种颜色，这四个数字分别代表这个颜色中红色、绿色、蓝色以及透明度。现在流行的显示设备可以在这四个维度分别支持256种变化，所以直接色可以表示2的32次方种颜色。当然并非所有的直接色都支持这么多种，为压缩空间使用，有可能只有表达红、绿、蓝的三个数字，每个数字也可能不支持256种变化之多 点阵图vs矢量图 点阵图，也叫做位图，像素图。构成点阵图的最小单位是象素，位图就是由象素阵列的排列来实现其显示效果的，每个象素有自己的颜色信息，在对位图图像进行编辑操作的时候，可操作的对象是每个象素，我们可以改变图像的色相、饱和度、明度，从而改变图像的显示效果。点阵图缩放会失真，用最近非常流行的沙画来比喻最恰当不过，当你从远处看的时候，画面细腻多彩，但是当你靠的非常近的时候，你就能看到组成画面的每粒沙子以及每个沙粒的颜色 矢量图，也叫做向量图。矢量图并不纪录画面上每一点的信息，而是纪录了元素形状及颜色的算法，当你打开一付矢量图的时候，软件对图形象对应的函数进行运算，将运算结果[图形的形状和颜色]显示给你看。无论显示画面是大还是小，画面上的对象对应的算法是不变的，所以，即使对画面进行倍数相当大的缩放，其显示效果仍然相同(不失真) GIF全称Graphics Interchange Format，采用LZW压缩算法进行编码。是无损的、采用索引色的、点阵图GIF是无损的，采用GIF格式保存图片不会降低图片质量。但得益于数据的压缩，GIF格式的图片，其文件大小要远小于BMP格式的图片。文件小，是GIF格式的优点，同时，GIF格式还具有支持动画以及透明的优点。但，GIF格式仅支持8bit的索引色，即在整个图片中，只能存在256种不同的颜色GIF格式适用于对色彩要求不高同时需要文件体积较小的场景，比如企业Logo、线框类的图等。因其体积小的特点，现在GIF被广泛的应用在各类网站中 JPEG or JPGJPEG是有损的、采用直接色的、点阵图，不支持透明度JPEG图片格式的设计目标，是在不影响人类可分辨的图片质量的前提下，尽可能的压缩文件大小。这意味着JPEG去掉了一部分图片的原始信息，也即是进行了有损压缩。JPEG的图片的优点，是采用了直接色，得益于更丰富的色彩，JPEG非常适合用来存储照片，用来表达更生动的图像效果，比如颜色渐变。与GIF相比，JPEG不适合用来存储企业Logo、线框类的图。因为有损压缩会导致图片模糊，而直接色的选用，又会导致图片文件较GIF更大 PNG-8PNG全称Portable Network Graphics，PNG-8是PNG的索引色版本。PNG-8是无损的、使用索引色的、点阵图。PNG是一种比较新的图片格式，PNG-8是非常好的GIF格式替代者，在可能的情况下，应该尽可能的使用PNG-8而不是GIF，因为在相同的图片效果下，PNG-8具有更小的文件体积。除此之外，PNG-8还支持透明度的调节，而GIF并不支持。 现在，除非需要动画的支持，否则我们没有理由使用GIF而不是PNG-8。当然了，PNG-8本身也是支持动画的，只是浏览器支持得不好，不像GIF那样受到广泛的支持。可以看到PNG-8具有更好的透明度支持 PNG-24PNG-24是PNG的直接色版本。PNG-24是无损的、使用直接色的、点阵图。无损的、使用直接色的点阵图，听起来非常像BMP，是的，从显示效果上来看，PNG-24跟BMP没有不同。PNG-24的优点在于，它压缩了图片的数据，使得同样效果的图片，PNG-24格式的文件大小要比BMP小得多。当然，PNG24的图片还是要比JPEG、GIF、PNG-8大得多。虽然PNG-24的一个很大的目标，是替换JPEG的使用。但一般而言，PNG-24的文件大小是JPEG的五倍之多，而显示效果则通常只能获得一点点提升。所以，只有在你不在乎图片的文件体积，而想要最好的显示效果时，才应该使用PNG-24格式。另外，PNG-24跟PNG-8一样，是支持图片透明度的 SVG全称Scalable Vector Graphics，是无损的、矢量图。SVG跟上面这些图片格式最大的不同，是SVG是矢量图。这意味着SVG图片由直线和曲线以及绘制它们的方法组成。当你放大一个SVG图片的时候，你看到的还是线和曲线，而不会出现像素点。这意味着SVG图片在放大时，不会失真，所以它非常适合用来绘制企业Logo、Icon等。SVG是很多种矢量图中的一种，它的特点是使用XML来描述图片。借助于前几年XML技术的流行，SVG也流行了很多。使用XML的优点是，任何时候你都可以把它当做一个文本文件来对待，也就是说，你可以非常方便的修改SVG图片，你所需要的只需要一个文本编辑器 WebPWebP是谷歌开发的一种新图片格式，WebP是同时支持有损和无损压缩的、使用直接色的、点阵图。从名字就可以看出来它是为Web而生的，什么叫为Web而生呢？就是说相同质量的图片，WebP具有更小的文件体积。现在网站上充满了大量的图片，如果能够降低每一个图片的文件大小，那么将大大减少浏览器和服务器之间的数据传输量，进而降低访问延迟，提升访问体验。在无损压缩的情况下，相同质量的WebP图片，文件大小要比PNG小26%；在有损压缩的情况下，具有相同图片精度的WebP图片，文件大小要比JPEG小25%~34%；WebP图片格式支持图片透明度，一个无损压缩的WebP图片，如果要支持透明度只需要22%的格外文件大小。想象Web上的图片之多，百分之几十的提升，是非常非常大的优化。只可惜，目前只有Chrome浏览器和Opera浏览器支持WebP格式，所以WebP的应用并不广泛。为了使用更先进的技术，比如WebP图片格式，来压缩互联网上传输的数据流量，谷歌甚至提供了Chrome Data Compression Proxy，设置了Chrome Data Compression Proxy作为Web代理之后，你访问的所有网站中的图片，在经过Proxy的时候，都会被转换成WebP格式，以降低图片文件的大小 IconfontUnicode 码表是一个很大的表格，每个表格都对应一个 Unicode 字符，每个字符都有一个 Unicode 码值对应，如 “李” 对应 “\u674e”, “靖” 对应 “\u9756”。因为码表很大，有部分表格并没有对应的字符，但是它有自己的码值。iconfont 的制作，首先将绘制的图形（可以是一张图片、也可以是一个 svg 描述）通过工具或者程序生成文字icon，然后将文字icon对应到码表之中，为了不干预码表中已有的字符，我们通常会把文字icon对应到没有字符的表格中，最后导出我们额外对应的表格信息，生成iconfont 总结 GIF： 无损、索引色、点阵图，支持动画、透明 JPG or JPEG：有损、直接色、点阵图，不支持透明 PNG-8：无损、索引色、点阵图，支持透明 PNG-24：无损的、直接色、点阵图，支持透明 SVG：无损、矢量图 WEBP：支持有损和无损压缩的、使用直接色的、点阵图，支持透明，但浏览器兼容性不好 IconFont：被转为了字体，字体css可操纵 相同效果的图片花费空间从大到小的顺序： 大图片：PNG-24 &gt; JPG &gt; WEBP 小图标：PNG-8 &gt; SVG &gt; IconFont 动图：GIF 应用一般情况下 单色图标用iconfont 动图用gif 带透明的图用png 颜色色数较小的图片用svg，如图标 如按钮 颜色色数较小的图片、尺寸较小图片用png8，如图标 如按钮 如缩略图 需要尽量锐利的图片用png，如按钮 如形象广告 如有文字的图片 尺寸较大的图片用jpg，如横幅广告 如正文插图 只针对chrome 单色图标用iconfont 动图用gif 带透明的图用png 颜色色数较小的图标用svg，如扁平风格彩色图标 颜色色数较小的图片、尺寸较小图片用png8，如图标 如按钮 如缩略图 需要尽量锐利的图片用png，如按钮如 如形象广告 有文字的图片 尺寸较大的图片用webp，如横幅广告 如正文插图]]></content>
      <categories>
        <category>Frontend Performance</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Immutability - redux最佳实践]]></title>
    <url>%2FReact-immutable%2F</url>
    <content type="text"><![CDATA[JavaScript 中的对象一般是可变的（Mutable），因为使用了引用赋值，新的对象简单的引用了原始对象，改变新的对象将影响到原始对象如 foo={a: 1}; bar=foo; bar.a=2 你会发现此时 foo.a 也被改成了 2虽然这样做可以节约内存，但当应用复杂后，这就造成了非常大的隐患，Mutable 带来的优点变得得不偿失为了解决这个问题，一般的做法是使用 shallowCopy（浅拷贝）或 deepCopy（深拷贝）来避免被修改，但这样做造成了 CPU 和内存的浪费 优点 优化性能：改变前的state和改变后的state不是同一个对象，直接可以根据state对象不同来判断state已经变化 易于调试和跟踪：改变前的sate和改变后的state都存在，方便追溯 易于推测：state的变化一定是action引起的，每个action对应一个新的state 操作不可变数据的方法原生写法 immutable.jsImmutable Data 就是一旦创建，就不能再被更改的数据。对 Immutable 对象的任何修改或添加删除操作都会返回一个新的 Immutable 对象。Immutable 实现的原理是 Persistent Data Structure（持久化数据结构），也就是使用旧数据创建新数据时，要保证旧数据同时可用且不变。同时为了避免 deepCopy 把所有节点都复制一遍带来的性能损耗，Immutable 使用了 Structural Sharing（结构共享），即如果对象树中一个节点发生变化，只修改这个节点和受它影响的父节点，其它节点则进行共享 Immutable 降低了 Mutable 带来的复杂度12345function touchAndLog(touchFn) &#123; let data = &#123; key: 'value' &#125;; touchFn(data); console.log(data.key); // 猜猜会打印什么？&#125; 在不查看 touchFn 的代码的情况下，因为不确定它对 data 做了什么，你是不可能知道会打印什么（这不是废话吗）但如果 data 是 Immutable 的呢，你可以很肯定的知道打印的是 value 节省内存Immutable.js 使用了 Structure Sharing 会尽量复用内存，甚至以前使用的对象也可以再次被复用没有被引用的对象会被垃圾回收123456789import &#123; Map&#125; from 'immutable';let a = Map(&#123; select: 'users', filter: Map(&#123; name: 'Cam' &#125;)&#125;)let b = a.set('select', 'people');a === b; // falsea.get('filter') === b.get('filter'); // true上面 a 和 b 共享了没有变化的 filter 节点 Undo/Redo，Copy/Paste，甚至时间旅行这些功能做起来小菜一碟因为每次数据都是不一样的，只要把这些数据放到一个数组里储存起来，想回退到哪里就拿出对应数据即可，很容易开发出撤销重做这种功能 并发安全传统的并发非常难做，因为要处理各种数据不一致问题，因此『聪明人』发明了各种锁来解决。但使用了 Immutable 之后，数据天生是不可变的，并发锁就不需要了。 然而现在并没什么卵用，因为 JavaScript 还是单线程运行的啊。但未来可能会加入，提前解决未来的问题不也挺好吗? 拥抱函数式编程Immutable 本身就是函数式编程中的概念，纯函数式编程比面向对象更适用于前端开发。因为只要输入一致，输出必然一致，这样开发的组件更易于调试和组装 immutability-helper immer]]></content>
      <categories>
        <category>React</category>
      </categories>
      <tags>
        <tag>immutable-js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[React 注意事项]]></title>
    <url>%2FReact-hints%2F</url>
    <content type="text"><![CDATA[用Functional Component 代替 Stateful ComponentStateful Component12345class Avatar extends React.Component &#123; render() &#123; return &lt;img src=&#123;this.props.url&#125; /&gt;; &#125;&#125;Functional Component1234// simplified version of our real componentconst Avatar = (props) =&gt; &#123; return &lt;img src=&#123;props.url&#125; /&gt;;&#125;Functional Component 实际上就是一个JS的function，它返回一些元素但是，你可能会认为Functional Component可能会避开像mounting / unmounting这些生命周期事件，那你就傻逼了下面是React作者的twitter This pattern is designed to encourage the creation of these simple components that should comprise large portions of your apps. In the future, we’ll also be able to make performance optimizations specific to these components by avoiding unnecessary checks and memory allocations. 用调用函数的方式加载Component原文链接123456789101112131415161718192021 ReactDOM.render( &lt;div&gt;- &lt;Avatar url=&#123;avatarUrl&#125; /&gt;+ &#123;Avatar(&#123; url: avatarUrl &#125;)&#125; &lt;div&gt;&#123;commentBody&#125;&lt;/div&gt; &lt;/div&gt;, mountNode ); // Compiled JavaScript ReactDOM.render(React.createElement( 'div', null,- React.createElement(Avatar, &#123; url: avatarUrl &#125;),+ Avatar(&#123; url: avatarUrl &#125;), React.createElement( 'div', null, commentBody ) ), mountNode);像上面代码展示的那样用+的方式（直接调用函数）代替 JSX tags 的方式 来加载Avatar这个Functional Component，我们会省略掉一个React.createElement的调用 和 它里面的所有的React生命周期事件，我们再也不需要等待React Team 去优化Functional Component了。实验证明，使用直接加载Functional Component的方式，会加速45% 阻止冒泡React自己实现了一套时间代理机制叫做SyntheticEvent，一个基于浏览器原生事件的跨浏览器实现。它拥有和浏览器原生事件一样的接口，包括stopPropagation()和preventDefault()，除了那些所有浏览器功能一样的事件而例如jQuery的事件代理是可以在React内部并行的所以当你要阻止一个事件冒泡时，很可能你需要像下面这样写：1234stopPropagation: function(e)&#123; e.stopPropagation(); //阻止React冒泡 e.nativeEvent.stopImmediatePropagation();//阻止原生js冒泡&#125;, 不要忘了React KeyReact will determine whether it is the same component or not based on keyNew component on the same level as old one? Would you kindly change a key? 不要在render里call ActionThe render() function should be pure, meaning that it does not modify component state.If you in render function call Action an return some data to setState,This will cause render a page multiple times or endless-loop. passing the component initial state a prop an anti-pattern这其实是一种props和state混用的情况，不建议在 initial state 根据传入的 props 来赋值当然最好是保证组件stateless React setState setState不会马上改变state，是异步操作 setState总会触发render除非shouldComponentUpdate返回false]]></content>
      <categories>
        <category>React</category>
      </categories>
      <tags>
        <tag>hints</tag>
        <tag>Functional Component</tag>
        <tag>SyntheticEvent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[React-Context]]></title>
    <url>%2Freact-context%2F</url>
    <content type="text"><![CDATA[在一个典型的 React 应用中，数据是通过 props 属性由上向下（由父及子）的进行传递的，但这对于某些类型的属性而言是极其繁琐的（例如：地区偏好，UI主题），这是应用程序中许多组件都所需要的。 Context 提供了一种在组件之间共享此类值的方式，而不必通过组件树的每个层级显式地传递 props 何时使用 ContextContext 设计目的是为共享那些被认为对于一个组件树而言是“全局”的数据，例如当前认证的用户、主题或首选语言。例如，在下面的代码中，我们通过一个“theme”属性手动调整一个按钮组件的样式：1234567891011121314151617181920function ThemedButton(props) &#123; return &lt;Button theme=&#123;props.theme&#125; /&gt;;&#125;// 中间组件function Toolbar(props) &#123; // Toolbar 组件必须添加一个额外的 theme 属性 // 然后传递它给 ThemedButton 组件 return ( &lt;div&gt; &lt;ThemedButton theme=&#123;props.theme&#125; /&gt; &lt;/div&gt; );&#125;class App extends React.Component &#123; render() &#123; return &lt;Toolbar theme="dark" /&gt;; &#125;&#125;使用 context, 我可以避免通过中间元素传递 props：123456789101112131415161718192021222324252627282930// 创建一个 theme Context, 默认 theme 的值为 lightconst ThemeContext = React.createContext('light');function ThemedButton(props) &#123; // ThemedButton 组件从 context 接收 theme return ( &lt;ThemeContext.Consumer&gt; &#123;theme =&gt; &lt;Button &#123;...props&#125; theme=&#123;theme&#125; /&gt;&#125; &lt;/ThemeContext.Consumer&gt; );&#125;// 中间组件function Toolbar(props) &#123; return ( &lt;div&gt; &lt;ThemedButton /&gt; &lt;/div&gt; );&#125;class App extends React.Component &#123; render() &#123; return ( &lt;ThemeContext.Provider value="dark"&gt; &lt;Toolbar /&gt; &lt;/ThemeContext.Provider&gt; ); &#125;&#125; 改变Context内state的状态，所有的consumer随之更新实现点击切换语言来更新语言 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import React from "react";const enStrings = &#123; submit: "Submit", cancel: "Cancel"&#125;;const cnStrings = &#123; submit: "提交", cancel: "取消"&#125;;const LocaleContext = React.createContext(enStrings);class LocaleProvider extends React.Component &#123; state = &#123; locale: cnStrings &#125;; toggleLocale = () =&gt; &#123; const locale = this.state.locale === enStrings ? cnStrings : enStrings; this.setState(&#123; locale &#125;); &#125;; render() &#123; return ( &lt;LocaleContext.Provider value=&#123;this.state.locale&#125;&gt; &lt;button onClick=&#123;this.toggleLocale&#125;&gt; 切换语言 &lt;/button&gt; &#123;this.props.children&#125; &lt;/LocaleContext.Provider&gt; ); &#125;&#125;class LocaledButtons extends React.Component &#123; render() &#123; return ( &lt;LocaleContext.Consumer&gt; &#123;locale =&gt; ( &lt;div&gt; &lt;button&gt;&#123;locale.cancel&#125;&lt;/button&gt; &amp;nbsp;&lt;button&gt;&#123;locale.submit&#125;&lt;/button&gt; &lt;/div&gt; )&#125; &lt;/LocaleContext.Consumer&gt; ); &#125;&#125;export default () =&gt; ( &lt;div&gt; &lt;LocaleProvider&gt; &lt;div&gt; &lt;br /&gt; &lt;LocaledButtons /&gt; &lt;/div&gt; &lt;/LocaleProvider&gt; &lt;/div&gt;);]]></content>
      <categories>
        <category>React</category>
      </categories>
      <tags>
        <tag>Context</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[React Diff算法]]></title>
    <url>%2FReact-Diff%2F</url>
    <content type="text"><![CDATA[前言React 中最值得称道的部分莫过于 Virtual DOM 与 diff 的完美结合，特别是其高效的 diff 算法，让用户可以无需顾忌性能问题而”任性自由”的刷新页面，让开发者也可以无需关心 Virtual DOM 背后的运作原理，因为 React diff 会帮助我们计算出 Virtual DOM 中真正变化的部分，并只针对该部分进行实际 DOM 操作，而非重新渲染整个页面，从而保证了每次操作更新后页面的高效渲染，因此 Virtual DOM 与 diff 是保证 React 性能口碑的幕后推手 假设树中元素个数为n，最先进的算法 的时间复杂度为O(n3)React基于三点假设，实现了一个启发的O(n)算法 类型相同的节点总是生成同样的树，而类型不同的节点也总是生成不同的树 类型相同的兄弟节点可以被唯一标识 Web UI 中 DOM 节点跨层级的移动操作特别少，可以忽略不计 Diff算法tree diff两棵树只会对同一层次的节点进行比较 component diff 如果是同一类型的组件，按照原策略继续比较 virtual DOM tree 如果不是，则将该组件判断为 dirty component，从而替换整个组件下的所有子节点 对于同一类型的组件，有可能其 Virtual DOM 没有任何变化，如果能够确切的知道这点那可以节省大量的 diff 运算时间，因此 React 允许用户通过 shouldComponentUpdate() 来判断该组件是否需要进行 diff element diff当节点处于同一层级时，React diff 提供了三种节点操作，分别为：INSERT_MARKUP（插入）、MOVE_EXISTING（移动）和 REMOVE_NODE（删除）可以根据key值进行优化 总结 React 通过制定大胆的 diff 策略，将 O(n3) 复杂度的问题转换成 O(n) 复杂度的问题 React 通过分层求异的策略，对 tree diff 进行算法优化 React 通过相同类生成相似树形结构，不同类生成不同树形结构的策略，对 component diff 进行算法优化 React 通过设置唯一 key的策略，对 element diff 进行算法优化 建议，在开发组件时，保持稳定的 DOM 结构会有助于性能的提升 建议，在开发过程中，尽量减少类似将最后一个节点移动到列表首部的操作，当节点数量过大或更新操作过于频繁时，在一定程度上会影响 React 的渲染性能]]></content>
      <categories>
        <category>React</category>
      </categories>
      <tags>
        <tag>diff</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flux]]></title>
    <url>%2Freact-flux%2F</url>
    <content type="text"><![CDATA[Flux是Facebook用于构建客户端Web应用程序的一个系统架构。它通过利用单向数据流来补充React的可组合视图组件。它更像是一种模式，而不是一个正式的框架 Predicatable code predicatable data (Flux) predicatable UI (React) 过程 用户访问 View View 发出用户的 Action Dispatcher 收到 Action，要求 Store 进行相应的更新 Store 更新后，发出一个change事件 View 收到change事件后，更新页面 Control/View 一个component 的loading state 在本component内处理 view只负责render数据，不对数据做任何处理 view只负责invoke Action 123456789101112131415161718_onNoticesStoreChange () &#123; if (this.state.isDeleting) &#123; Alert.onSuccess('删除公告成功'); &#125; else if (this.state.isEditing) &#123; Alert.onSuccess('更新公告成功'); &#125; else if (this.state.isAdding) &#123; Alert.onSuccess('发布公告成功'); &#125; this.setState(&#123; isLoading: false, notices: NoticesStore.data, modalType: '', isDeleting: false, isEditing: false, isAdding: false &#125;); this.refs.modal.close();&#125;, Action 不要在action里操作dom 只有view invoke Action 每个Action都是一个对象，包含一个actionType属性和一些其他属性（用来传递数据） Action中利用Dispatcher的把具体的动作（actionType）派发到Store 12345678910111213141516retrieveChannel () &#123; this.callApi('DataApi.getChannel', DataApi.getChannel.bind(this), (data) =&gt; &#123; AppDispatcher.dispatch(&#123; type: ActionTypes.RETRIEVE_CHANNEL_SUCCESS, data: data['data'] &#125;); &#125;, (error) =&gt; &#123; AppDispatcher.dispatch(&#123; type: ActionTypes.RETRIEVE_CHANNEL_ERROR, data: Error.generateFromHttpError(error) &#125;); &#125; );&#125; Dispatcher Dispatcher 的作用是将 Action 派发到 Store，即触发注册的回调方法callbacks 可以把它看作一个路由器，负责在 View 和 Store 之间，建立 Action 的正确传递路线 Dispatcher 只能有一个，而且是全局的 1234567891011121314151617import &#123;Dispatcher&#125; from 'flux';class AppDispatcher extends Dispatcher &#123; dispatch (payload) &#123; if (this.isDispatching()) &#123; window.setTimeout(() =&gt; &#123; super.dispatch(payload); &#125;); &#125; else &#123; super.dispatch(payload); &#125; &#125;&#125;module.exports = new AppDispatcher(); Store Store 保存整个应用的state状态。它的角色有点像 MVC 架构之中的Model store 只负责响应对应的action并存储或者更新数据，但不对数据做任何再加工，数据加工交给API层实现 store 的change事件是全局的，最好不同的数据用不同的store来存储（由同一个事件触发而变化的数据 称为同种数据）， 分别addChangeListener到相应的callback function， 以应对不同的变化且相互不干涉 12345678910111213141516171819202122232425262728293031323334353637383940414243import Immutable from 'immutable';import BaseStore from './BaseStore.js';import &#123;ActionTypes&#125; from '../Constants.js';import Category from '../models/Category.js';import AppDispatcher from '../dispatcher/AppDispatcher.js';class CategoriesStore extends BaseStore &#123; constructor () &#123; super(); &#125; reset () &#123; this._data = Immutable.List([]); &#125; update (data) &#123; var categories = data ? data.map(category =&gt; &#123; return new Category(category); &#125;) : []; this._data = Immutable.List(categories); &#125; registerDispatcher () &#123; super.registerDispatcher(); this._dispatcherToken = AppDispatcher.register(payload =&gt; &#123; switch(payload.type) &#123; case ActionTypes.RETRIEVE_CATEGORIES_SUCCESS: this.update(payload.data); this.emitChange(); break; case ActionTypes.RETRIEVE_CATEGORIES_ERROR: this.updateError(payload.data); this.emitError(); break; default: // do nothing break; &#125; &#125;); &#125;&#125; 优势 应用的状态必须独立出来放到 store 里面统一管理，通过侦听 action 来执行具体的状态操作。 视图组件变得很薄，只包含了渲染逻辑和触发 action 这两个职责 要理解一个 store 可能发生的状态变化，只需要看它所注册的 actions 回调就可以 单向数据流动：任何状态的变化都必须通过 action 触发，而 action 又必须通过 dispatcher 走，所以整个应用的每一次状态变化都会从同一个地方流过。Flux 的意义就在于强制让所有的状态变化都必须留下一笔记录，这样就可以利用这个来做各种 debug 工具、历史回滚等等]]></content>
      <categories>
        <category>React</category>
      </categories>
      <tags>
        <tag>Flux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[React setState]]></title>
    <url>%2Freact-setState%2F</url>
    <content type="text"><![CDATA[setState 在 React 中是经常使用的一个 API，但是它存在一些问题，可能会导致犯错，核心原因就是因为这个 API 是异步的。 首先 setState 的调用并不会马上引起 state 的改变，并且如果你一次调用了多个 setState ，那么结果可能并不如你期待的一样12345678handle() &#123; // 初始化 `count` 为 0 console.log(this.state.count) // -&gt; 0 this.setState(&#123; count: this.state.count + 1 &#125;) this.setState(&#123; count: this.state.count + 1 &#125;) this.setState(&#123; count: this.state.count + 1 &#125;) console.log(this.state.count) // -&gt; 0&#125;第一，两次的打印都为 0，因为 setState 是个异步 API，只有同步代码运行完毕才会执行。setState 异步的原因我认为在于，setState 可能会导致 DOM 的重绘，如果调用一次就马上去进行重绘，那么调用多次就会造成不必要的性能损失。设计成异步的话，就可以将多次调用放入一个队列中，在恰当的时候统一进行更新过程第二，虽然调用了三次 setState ，但是 count 的值还是为 1。因为多次调用会合并为一次，只有当更新结束后 state 才会改变，三次调用等同于如下代码123456Object.assign( &#123;&#125;, &#123; count: this.state.count + 1 &#125;, &#123; count: this.state.count + 1 &#125;, &#123; count: this.state.count + 1 &#125;,)当然你也可以通过以下方式来实现调用三次 setState 使得 count 为 312345handle() &#123; this.setState((prevState) =&gt; (&#123; count: prevState.count + 1 &#125;)) this.setState((prevState) =&gt; (&#123; count: prevState.count + 1 &#125;)) this.setState((prevState) =&gt; (&#123; count: prevState.count + 1 &#125;))&#125;如果你想在每次调用 setState 后获得正确的 state ，可以通过如下代码实现12345handle() &#123; this.setState((prevState) =&gt; (&#123; count: prevState.count + 1 &#125;), () =&gt; &#123; console.log(this.state) &#125;)&#125;]]></content>
      <categories>
        <category>React</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[React lifecycle v16.3 changes]]></title>
    <url>%2FReact-lifecycle%2F</url>
    <content type="text"><![CDATA[v16.3之前的lifecycle传统组件生命周期会导致一些不安全的编码实践，他们是： componentWillMount componentWillReceiveProps componentWillUpdate 这些生命周期方法经常被误解和滥用此外，我们预计他们的潜在滥用可能在异步渲染方面有更大的问题。因此，我们将在即将发布的版本中为这些生命周期添加一个“UNSAFE_”前缀 逐渐迁移的计划 16.3：为不安全生命周期引入别名UNSAFE_componentWillMount， UNSAFE_componentWillReceiveProps和UNSAFE_componentWillUpdate。 （旧的生命周期名称和新的别名都可以在此版本中使用。） 未来的16.x版本: 为componentWillMount，componentWillReceiveProps和componentWillUpdate启用弃用警告。 （旧的生命周期名称和新的别名都可以在此版本中使用，但旧名称会记录DEV模式警告。） 17.0: 删除componentWillMount，componentWillReceiveProps和componentWillUpdate。 （从现在开始，只有新的“UNSAFE_”生命周期名称将起作用。） v16.3lifecycle改动 We are adding the following lifecycle aliases: UNSAFE_componentWillMount, UNSAFE_componentWillReceiveProps, and UNSAFE_componentWillUpdate. (Both the old lifecycle names and the new aliases will be supported.) We are introducing two new lifecycles, static getDerivedStateFromProps and getSnapshotBeforeUpdate getDerivedStateFromProps12345class Example extends React.Component &#123; static getDerivedStateFromProps(nextProps, prevState) &#123; // ... &#125;&#125; 新的静态getDerivedStateFromProps生命周期在组件实例化以及接收新props后调用。它可以返回一个对象来更新state，或者返回null来表示新的props不需要任何state更新与componentDidUpdate一起，这个新的生命周期应该覆盖传统componentWillReceiveProps的所有用例 getSnapshotBeforeUpdate12345class Example extends React.Component &#123; getSnapshotBeforeUpdate(prevProps, prevState) &#123; // ... &#125;&#125; 新的getSnapshotBeforeUpdate生命周期在更新之前被调用（例如，在DOM被更新之前）。此生命周期的返回值将作为第三个参数传递给componentDidUpdate。 （这个生命周期不是经常需要的，但可以用于在恢复期间手动保存滚动位置的情况。）与componentDidUpdate一起，这个新的生命周期将覆盖旧版componentWillUpdate的所有用例 Examples初始化状态（Initializing state）这个例子展示了一个调用componentWillMount中带有setState的组件：1234567891011// Beforeclass ExampleComponent extends React.Component &#123; state = &#123;&#125;; componentWillMount() &#123; this.setState(&#123; currentColor: this.props.defaultColor, palette: 'rgb', &#125;); &#125;&#125;这种类型的组件最简单的重构是将状态初始化移动到构造函数或属性初始值设定项，如下所示：1234567// Afterclass ExampleComponent extends React.Component &#123; state = &#123; currentColor: this.props.defaultColor, palette: 'rgb', &#125;;&#125; 获取外部数据(Fetching external data)在componentDidMount中获取外部数据1234567891011121314151617181920212223242526272829// Afterclass ExampleComponent extends React.Component &#123; state = &#123; externalData: null, &#125;; componentDidMount() &#123; this._asyncRequest = asyncLoadData().then( externalData =&gt; &#123; this._asyncRequest = null; this.setState(&#123;externalData&#125;); &#125; ); &#125; componentWillUnmount() &#123; if (this._asyncRequest) &#123; this._asyncRequest.cancel(); &#125; &#125; render() &#123; if (this.state.externalData === null) &#123; // Render loading state ... &#125; else &#123; // Render real UI ... &#125; &#125;&#125; 添加事件监听(Adding event listeners )添加事件监听的推荐方式是使用componentDidMount生命周期12345678910111213141516171819202122232425262728293031323334353637// Afterclass ExampleComponent extends React.Component &#123; state = &#123; subscribedValue: this.props.dataSource.value, &#125;; componentDidMount() &#123; // Event listeners are only safe to add after mount, // So they won't leak if mount is interrupted or errors. this.props.dataSource.subscribe( this.handleSubscriptionChange ); // External values could change between render and mount, // In some cases it may be important to handle this case. if ( this.state.subscribedValue !== this.props.dataSource.value ) &#123; this.setState(&#123; subscribedValue: this.props.dataSource.value, &#125;); &#125; &#125; componentWillUnmount() &#123; this.props.dataSource.unsubscribe( this.handleSubscriptionChange ); &#125; handleSubscriptionChange = dataSource =&gt; &#123; this.setState(&#123; subscribedValue: dataSource.value, &#125;); &#125;;&#125; 基于props更新state以下是使用旧版componentWillReceiveProps生命周期基于新的道具值更新状态的组件示例123456789101112131415// Beforeclass ExampleComponent extends React.Component &#123; state = &#123; isScrollingDown: false, &#125;; componentWillReceiveProps(nextProps) &#123; if (this.props.currentRow !== nextProps.currentRow) &#123; this.setState(&#123; isScrollingDown: nextProps.currentRow &gt; this.props.currentRow, &#125;); &#125; &#125;&#125;尽管上面的代码本身并没有问题，但componentWillReceiveProps生命周期通常会被错误地用于解决问题。因此，该方法将被弃用。从版本16.3开始，更新state以响应props更改的推荐方法是使用新的静态getDerivedStateFromProps生命周期。 （生命周期在组件创建时以及每次收到新道具时调用）：12345678910111213141516171819202122// Afterclass ExampleComponent extends React.Component &#123; // Initialize state in constructor, // Or with a property initializer. state = &#123; isScrollingDown: false, lastRow: null, &#125;; static getDerivedStateFromProps(nextProps, prevState) &#123; if (nextProps.currentRow !== prevState.lastRow) &#123; return &#123; isScrollingDown: nextProps.currentRow &gt; prevState.lastRow, lastRow: nextProps.currentRow, &#125;; &#125; // Return null to indicate no change to state. return null; &#125;&#125; 调用外部回调函数(Invoking external callbacks)下面是一个在内部状态发生变化时调用外部函数的组件示例：1234567891011// Beforeclass ExampleComponent extends React.Component &#123; componentWillUpdate(nextProps, nextState) &#123; if ( this.state.someStatefulValue !== nextState.someStatefulValue ) &#123; nextProps.onChange(nextState.someStatefulValue); &#125; &#125;&#125;在异步模式下使用componentWillUpdate都是不安全的，因为外部回调可能会多次调用只更新一次。相反，应该使用componentDidUpdate生命周期，因为它保证每次更新只调用一次1234567891011// Afterclass ExampleComponent extends React.Component &#123; componentDidUpdate(prevProps, prevState) &#123; if ( this.state.someStatefulValue !== prevState.someStatefulValue ) &#123; this.props.onChange(this.state.someStatefulValue); &#125; &#125;&#125;props改变的副作用(Side effects on props change)与上述 事例类似，有时组件在props更改时会产生副作用12345678// Beforeclass ExampleComponent extends React.Component &#123; componentWillReceiveProps(nextProps) &#123; if (this.props.isVisible !== nextProps.isVisible) &#123; logVisibleChange(nextProps.isVisible); &#125; &#125;&#125;与componentWillUpdate一样，componentWillReceiveProps可能会多次调用但是只更新一次。出于这个原因，避免在此方法中导致的副作用非常重要。相反，应该使用componentDidUpdate，因为它保证每次更新只调用一次：12345678// Afterclass ExampleComponent extends React.Component &#123; componentDidUpdate(prevProps, prevState) &#123; if (this.props.isVisible !== prevProps.isVisible) &#123; logVisibleChange(this.props.isVisible); &#125; &#125;&#125; props改变时获取外部数据(Fetching external data when props change)以下是根据propsvalues提取外部数据的示例12345678910111213141516171819202122232425262728293031323334353637383940// Beforeclass ExampleComponent extends React.Component &#123; state = &#123; externalData: null, &#125;; componentDidMount() &#123; this._loadAsyncData(this.props.id); &#125; componentWillReceiveProps(nextProps) &#123; if (nextProps.id !== this.props.id) &#123; this.setState(&#123;externalData: null&#125;); this._loadAsyncData(nextProps.id); &#125; &#125; componentWillUnmount() &#123; if (this._asyncRequest) &#123; this._asyncRequest.cancel(); &#125; &#125; render() &#123; if (this.state.externalData === null) &#123; // Render loading state ... &#125; else &#123; // Render real UI ... &#125; &#125; _loadAsyncData(id) &#123; this._asyncRequest = asyncLoadData(id).then( externalData =&gt; &#123; this._asyncRequest = null; this.setState(&#123;externalData&#125;); &#125; ); &#125;&#125;此组件的推荐升级路径是将数据更新移动到componentDidUpdate中。在渲染新道具之前，您还可以使用新的getDerivedStateFromProps生命周期清除陈旧的数据：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// Afterclass ExampleComponent extends React.Component &#123; state = &#123; externalData: null, &#125;; static getDerivedStateFromProps(nextProps, prevState) &#123; // Store prevId in state so we can compare when props change. // Clear out previously-loaded data (so we don't render stale stuff). if (nextProps.id !== prevState.prevId) &#123; return &#123; externalData: null, prevId: nextProps.id, &#125;; &#125; // No state update necessary return null; &#125; componentDidMount() &#123; this._loadAsyncData(this.props.id); &#125; componentDidUpdate(prevProps, prevState) &#123; if (this.state.externalData === null) &#123; this._loadAsyncData(this.props.id); &#125; &#125; componentWillUnmount() &#123; if (this._asyncRequest) &#123; this._asyncRequest.cancel(); &#125; &#125; render() &#123; if (this.state.externalData === null) &#123; // Render loading state ... &#125; else &#123; // Render real UI ... &#125; &#125; _loadAsyncData(id) &#123; this._asyncRequest = asyncLoadData(id).then( externalData =&gt; &#123; this._asyncRequest = null; this.setState(&#123;externalData&#125;); &#125; ); &#125;&#125; 在更新之前读取DOM属性(Reading DOM properties before an update)下面是一个组件的例子，它在更新之前从DOM中读取属性，以便在列表中保持滚动位置123456789101112131415161718192021222324252627282930313233343536class ScrollingList extends React.Component &#123; listRef = null; previousScrollOffset = null; componentWillUpdate(nextProps, nextState) &#123; // Are we adding new items to the list? // Capture the scroll position so we can adjust scroll later. if (this.props.list.length &lt; nextProps.list.length) &#123; this.previousScrollOffset = this.listRef.scrollHeight - this.listRef.scrollTop; &#125; &#125; componentDidUpdate(prevProps, prevState) &#123; // If previousScrollOffset is set, we've just added new items. // Adjust scroll so these new items don't push the old ones out of view. if (this.previousScrollOffset !== null) &#123; this.listRef.scrollTop = this.listRef.scrollHeight - this.previousScrollOffset; this.previousScrollOffset = null; &#125; &#125; render() &#123; return ( `&lt;div&gt;` &#123;/* ...contents... */&#125; `&lt;/div&gt;` ); &#125; setListRef = ref =&gt; &#123; this.listRef = ref; &#125;;&#125;在上面的例子中，componentWillUpdate被用来读取DOM属性。但是，对于异步渲染，“render”阶段生命周期（如componentWillUpdate和render）与“commit”阶段生命周期（如componentDidUpdate）之间可能存在延迟。如果用户在这段时间内做了类似调整窗口大小的操作，则从componentWillUpdate中读取的scrollHeight值将失效。 解决此问题的方法是使用新的“commit”阶段生命周期getSnapshotBeforeUpdate。在数据发生变化之前立即调用该方法（例如，在更新DOM之前）。它可以将React的值作为参数传递给componentDidUpdate，在数据发生变化后立即调用它。123456789101112131415161718192021222324252627282930313233343536class ScrollingList extends React.Component &#123; listRef = null; getSnapshotBeforeUpdate(prevProps, prevState) &#123; // Are we adding new items to the list? // Capture the scroll position so we can adjust scroll later. if (prevProps.list.length &lt; this.props.list.length) &#123; return ( this.listRef.scrollHeight - this.listRef.scrollTop ); &#125; return null; &#125; componentDidUpdate(prevProps, prevState, snapshot) &#123; // If we have a snapshot value, we've just added new items. // Adjust scroll so these new items don't push the old ones out of view. // (snapshot here is the value returned from getSnapshotBeforeUpdate) if (snapshot !== null) &#123; this.listRef.scrollTop = this.listRef.scrollHeight - snapshot; &#125; &#125; render() &#123; return ( `&lt;div&gt;` &#123;/* ...contents... */&#125; `&lt;/div&gt;` ); &#125; setListRef = ref =&gt; &#123; this.listRef = ref; &#125;;&#125;]]></content>
      <categories>
        <category>React</category>
      </categories>
      <tags>
        <tag>lifecycle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人民币的汇率控制（贬值）]]></title>
    <url>%2FRMB-exchange-rate%2F</url>
    <content type="text"><![CDATA[操作方法 经常性干预汇市以维持价位，央行印人民币持续购买美元来降低人民币对美元汇率。于是市场中的人民币增加了，同时可出售央行发行的人民币票据从市场回收人民币起到冲销的作用。持有的美元又通过购买美国国债流到美国和其他投资者手中 资本管制，个人和机构资本流入流出需要获得有关部门审批和授权 通过印人民币来收回贸易顺差中的外币 通过印人民币来收回外国直接投资中国的外币 利 增加出口 增加外汇储备 促进就业 弊 增发本币，通胀 进口商品贵，影响生活水平 出口产业靠高汇率的优势不思进取，产业竞争力变化不大 国内资源被贱卖 出口退税，全国人民的税收来补贴出口]]></content>
      <categories>
        <category>Economics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[React-Router]]></title>
    <url>%2FReact-Router%2F</url>
    <content type="text"><![CDATA[为什么需要react-router 单页应用需要进行页面切换 通过 URL 可以定位到页面 更有语义的组织资源 三种路由实现方式 URL 路径 hash 路由 内存路由 核心API 原理 react-router依赖基础 - historyhistory是一个独立的第三方js库，可以用来兼容在不同浏览器、不同环境下对历史记录的管理，拥有统一的API。具体来说里面的history分为三类: 老浏览器的history: 主要通过hash来实现，对应createHashHistory 高版本浏览器: 通过html5里面的history，对应createBrowserHistory node环境下后端渲染: 主要存储在memeory里面，对应createMemoryHistory 实现方式 createBrowserHistory: 利用HTML5里面的history createHashHistory: 通过hash来存储在不同状态下的history信息 createMemoryHistory: 在内存中进行历史记录的存储 执行URL前进 createBrowserHistory: pushState、replaceState createHashHistory: location.hash=*** location.replace() createMemoryHistory: 在内存中进行历史记录的存储 检测URL回退 createBrowserHistory: popstate createHashHistory: hashchange createMemoryHistory: 因为是在内存中操作，跟浏览器没有关系，不涉及UI层面的事情，所以可以直接进行历史信息的回退 同步每个history对象包含下面的属性： history.length - The number of entries in the history stack history.location - The current location (see below) history.action - The current navigation action (see below) 同时history对象还有个listen方法123456history.listen((location, action) =&gt; &#123; console.log( `The current URL is $&#123;location.pathname&#125;$&#123;location.search&#125;$&#123;location.hash&#125;` ) console.log(`The last navigation action was $&#123;action&#125;`)&#125;)这样一来，就可以根据location的变化来执行对应的action，也就是加载某个路径对应的Component综上，实现URL与UI界面的同步也就转变成location与components之间的同步 具体实现组件层面描述实现过程在react-router中最主要的component是Router RouterContext Link history库起到了中间桥梁的作用 API层面描述实现过程 browserHistory 和 hashHistory hashHistory URL：http://myurl.com/#page/another_page/another_page BrowserHistory URL：http://myurl.com/page/another_page/another_page hashHistory可以兼容低版本浏览器用browserHistory ，在你登录前访问某个url时，server端能拿到完整的url，所以登录后可以帮你重定向到这个url如果用hashHistory server端拿不到#后面的东西所以就不能正确重定向 Browsers do not send the #hash part of URL in any of HTTP requests.Therefore server-side (i.e. NodeJS) would not know what #hash was in the URL when user requested a page.A good example is user trying to load a page that requires a login (via oAuth etc.). Before user is taken to a separate website for authentication, you app’s server-side would tell the authentication vendor what URL redirect user to after a successful login (usually it is to the original URL requested as most websites do this). If you were to use hashHistory - server-side would know only the bits before # symbol and would redirect user to the main page of your app and not a sub-page that user wanted to load.]]></content>
      <categories>
        <category>React</category>
      </categories>
      <tags>
        <tag>React-Router</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redux]]></title>
    <url>%2FRedux%2F</url>
    <content type="text"><![CDATA[一个JS状态管理框架 不需要redux 用户的使用方式非常简单 用户之间没有协作 不需要与服务器大量交互，也没有使用 WebSocket 视图层（View）只从单一来源获取数据 需要redux 某个组件的状态，需要共享 某个状态需要在任何地方都可以拿到 一个组件需要改变全局状态 一个组件需要改变另一个组件的状态 特性Single Source of Truth 可预测性 纯函数更新 StoreReducer 函数最重要的特征是，它是一个纯函数。也就是说，只要是同样的输入，必定得到同样的输出。纯函数是函数式编程的概念，必须遵守以下一些约束 不得改写参数 不能调用系统 I/O 的API 不能调用Date.now()或者Math.random()等不纯的方法，因为每次会得到不一样的结果 由于 Reducer 是纯函数，就可以保证同样的State，必定得到同样的 View。但也正因为这一点，Reducer 函数里面不能改变 State，必须返回一个全新的对象 StoreStore 就是保存数据的地方，你可以把它看成一个容器。整个应用只能有一个 StoreRedux 提供createStore这个函数，用来生成 Store12import &#123; createStore &#125; from &apos;redux&apos;;const store = createStore(Reducer); store.getState()Store对象包含所有数据。如果想得到某个时点的数据，就要对 Store 生成快照。这种时点的数据集合，就叫做 State。当前时刻的 State，可以通过store.getState()拿到 store.dispatch()store.dispatch()是 View 发出 Action 的唯一方法1234567import &#123; createStore &#125; from &apos;redux&apos;;const store = createStore(fn);store.dispatch(&#123; type: &apos;ADD_TODO&apos;, payload: &apos;Learn Redux&apos;&#125;); 上面代码中，store.dispatch接受一个 Action 对象作为参数，将它发送出去结合 Action Creator，这段代码可以改写如下:1store.dispatch(addTodo(&apos;Learn Redux&apos;)); store.subscribe()Store 允许使用store.subscribe方法设置监听函数，一旦 State发生变化，就自动执行这个函数显然，只要把 View 的更新函数（对于 React 项目，就是组件的render方法或setState方法）放入listen，就会实现 View 的自动渲染 actionState 的变化，会导致 View 的变化。但是，用户接触不到 State，只能接触到 View。所以，State 的变化必须是 View 导致的。Action 就是 View 发出的通知，表示 State 应该要发生变化了Action 是一个对象。其中的type属性是必须的，表示 Action 的名称。其他属性可以自由设置。1234const action = &#123; type: &apos;ADD_TODO&apos;, payload: &apos;Learn Redux&apos;&#125;; 上面代码中，Action 的名称是ADD_TODO，它携带的信息是字符串Learn Redux。可以这样理解，Action 描述当前发生的事情。改变 State 的唯一办法，就是使用 Action。它会运送数据到 Store reducerStore 收到 Action 以后，必须给出一个新的 State，这样 View 才会发生变化。这种 State 的计算过程就叫做 ReducerReducer 是一个函数，它接受 Action 和当前 State 作为参数，返回一个新的 State。1234const reducer = function (state, action) &#123; // ... return new_state;&#125;; bindActionCreators就是将action和dispatch action 结合起来返回一个函数，这个函数实现了两个功能 生成action dispatch这个action 总结首先，用户发出 Action1store.dispatch(action) 然后，Store 自动调用 Reducer，并且传入两个参数：当前 State 和收到的 Action。 Reducer 会返回新的 State1let nextState = todoApp(previousState, action) State 一旦有变化，Store 就会调用监听函数12// 设置监听函数store.subscribe(listener); listener可以通过store.getState()得到当前状态。如果使用的是 React，这时可以触发重新渲染 View1234function listerner() &#123; let newState = store.getState(); component.setState(newState); &#125; example123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import React from "react";import &#123; createStore, combineReducers, bindActionCreators&#125; from "redux";function run() &#123; // Store initial state const initialState = &#123; count: 0 &#125;; // reducer const counter = (state = initialState, action) =&gt; &#123; switch (action.type) &#123; case "PLUS_ONE": return &#123; count: state.count + 1 &#125;; case "MINUS_ONE": return &#123; count: state.count - 1 &#125;; case "CUSTOM_COUNT": return &#123; count: state.count + action.payload.count &#125;; default: break; &#125; return state; &#125;; const todos = (state = &#123;&#125;) =&gt; state; // Create store const store = createStore( combineReducers(&#123; todos, counter &#125;) ); // Action creator function plusOne() &#123; // action return &#123; type: "PLUS_ONE" &#125;; &#125; function minusOne() &#123; return &#123; type: "MINUS_ONE" &#125;; &#125; function customCount(count) &#123; return &#123; type: "CUSTOM_COUNT", payload: &#123; count &#125; &#125;; &#125; plusOne = bindActionCreators(plusOne, store.dispatch); store.subscribe(() =&gt; console.log(store.getState())); // store.dispatch(plusOne()); plusOne(); store.dispatch(minusOne()); store.dispatch(customCount(5));&#125;export default () =&gt; ( &lt;div&gt; &lt;button onClick=&#123;run&#125;&gt;Run&lt;/button&gt; &lt;p&gt;* 请打开控制台查看运行结果&lt;/p&gt; &lt;/div&gt;);]]></content>
      <categories>
        <category>React</category>
      </categories>
      <tags>
        <tag>Redux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行存储和（HBase）列存储的区别]]></title>
    <url>%2FRow-based-storage%2F</url>
    <content type="text"><![CDATA[原理 Row-based storage stores atable in a sequence of rows Column-based storage storesa table in a sequence of columns 优缺点行式存储下一张表的数据都是放在一起的，但列式存储下都被分开保存了。所以它们就有了如下这些优缺点： 列存储为什么快 减少了磁盘IO 随机读变成了连续读]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Certificate Authority]]></title>
    <url>%2Fsecurity-CA%2F</url>
    <content type="text"><![CDATA[证书授权中心（certificate authority，CA）是管理和签发安全凭证和加密信息安全密钥的网络机构 证书的生成过程 CA将 申请者的信息、申请者的public key、要使用签名算法、签名hash算法等 用里面声明的签名hash算法生成一个digest CA用自己的private key 对这个digest加密生成signature 将这个signature 附在1中所有信息的后面 组合成 申请者的 digital certificate，然后返回给申请者 一个digital certificate如下图（知乎的）上图可以看出 知乎这个证书 里面的public key是明文的，加密的只是 digest所以digital certificate的作用是将持有者的公钥和持有者的身份绑定起来，CA的签名是保证了这个绑定关系。 证书应用在https中，client拿到server端的digital certificate后，用digital certificate中的hash算法对内容进行hash，得到一个digest，另一方面 用CA的公钥去 解密 signature，得到另一个digest，看前后生成的digest是否一样进而判断是否secure。 Notes 数字证书的作用是将持有者的公钥和持有者的身份绑定起来，CA的签名是保证了这个绑定关系 证书(digital certificate)中“网站公钥”是明文出现的，CA并不对网站公钥进行加密 CA对上面这一堆数据的一个摘要(digest)，用自己的私钥加密，产生的密文就是数字签名(signature)，附在上面这一堆明文的后面（对摘要进行加密） 由于CA的公钥每个人都持有，所以每个人都可以验证，证书中的这一堆信息是不是真实的（使用CA的公钥解密得到消息摘要，与自己计算出的消息摘要比对） 证书中，某人声明他的身份，并且附上了自己的公钥——这个身份与公钥的绑定，是由CA来验证的——证书持有者的公钥的真实性是由CA来保证的 CA链子CA的真实性是由上一级CA来保证的一级一级往上推，最终的问题是：根CA的真实性是谁来保证的？根CA可以给下一级CA签发证书，保证下一级CA的真实性，但根CA自己的真实性——可以说是由他自己保证的比如这个是12306的根证书：根证书的颁发者就是证书的使用者浏览器里一般预置有绝大多数根CA的证书，所以遇到其他网站的证书，可以通过证书路径，一级级往下验证，建立信任链。比如知乎的证书但遇到某些奇葩的网站，比如12306，由于该网站的根CA并没有预存在浏览器中，那么浏览器就无法建立信任链——所以登入12306时候会提示证书错误 安全 浏览器预存的根CA的证书是不是真实的？ 这个所谓的根CA是不是靠谱的 某些恶意的浏览器可能会预置一些虚假的根CA的证书——但这属于法律层面的问题。对于使用者而言，有两条建议 不使用野路子的浏览器 不轻易安装未知的根证书 这两点满足之后，至少可以保证：持有虚假证书的网站，是不会通过浏览器的信任检查的]]></content>
      <categories>
        <category>Security</category>
      </categories>
      <tags>
        <tag>CA</tag>
        <tag>Digital Certificate</tag>
        <tag>Signature</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTPS]]></title>
    <url>%2Fsecurity-HTTPS%2F</url>
    <content type="text"><![CDATA[HTTPS 还是通过了 HTTP 来传输信息，但是信息通过 TLS（Transparent Layer Security） 协议进行了加密 TLSTLS 协议位于传输层之上，应用层之下。首次进行 TLS 协议传输需要两个 RTT ，接下来可以通过 Session Resumption 减少到一个 RTT。 在 TLS 中使用了两种加密技术，分别为：对称加密和非对称加密。 对称加密对称加密就是两边拥有相同的秘钥，两边都知道如何将密文加密解密 非对称加密有公钥私钥之分，公钥所有人都可以知道，可以将数据用公钥加密，但是将数据解密必须使用私钥解密，私钥只有分发公钥的一方才知道 TLS 握手过程 客户端 (浏览器) 发起 HTTP 请求，请求连接服务端，发送支持的加密通信协议 (和版本)，并且生成一个随机数，后续用于生成”对话密钥”。 服务端确认加密通信协议 (和版本)，同时也生成一个随机数，后续用于生成”对话密匙”，并且将 CA 颁发的数字证书，一起发送给客户端。 客户端收到数字证书后，检测内置的”受信任的根证书颁发机构”，查看解开数字证书的公匙是否在。 如果解开数字证书的公匙存在，则使用它解开数字证书，得到正确的服务器公匙，同时再次生成一个随机数，用于服务器公匙加密，并发送给服务器。 此时本地和服务器同时将三个随机数，根据约定的加密方法进行加密，各自生成本次会话的所使用的同一把 “会话密匙” 。 到这里，认证阶段已经完毕，数据传输从 非对称加密 换成了 对称加密 (因为考虑到性能)，接下来所有的数据传输都是使用HTTP协议进行传输，只不过使用了 “会话密匙” 来加密内容 TLS加密了什么除了domain，都加密了，包括 path query body header 等 与HTTP比较 安全性 HTTP HTTPS 窃听风险 传递的信息是明文的，可能会被有心人拦截下来窃听 信息加密传播 篡改风险 传递的信息可能会被篡改 信息校验，一旦被篡改立刻就会被发现 伪装风险 没有验证通信另外一头对方的身份，可能遭遇伪装 身份校验]]></content>
      <categories>
        <category>Security</category>
      </categories>
      <tags>
        <tag>TLS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSRF (Cross-site request forgery)]]></title>
    <url>%2Fsecurity-CSRF%2F</url>
    <content type="text"><![CDATA[XSS 利用的是用户对指定网站的信任，CSRF 利用的是网站对用户网页浏览器的信任 简单点说，CSRF 就是利用用户的登录态发起恶意请求 如何攻击假设网站中有一个通过 Get 请求提交用户评论的接口，那么攻击者就可以在钓鱼网站中加入一个图片，图片的地址就是评论接口1&lt;img src=&quot;http://www.domain.com/xxx?comment=&apos;attack&apos;&quot;/&gt; 如果接口是 Post 提交的，就相对麻烦点，需要用表单来提交接口123&lt;form action=&quot;http://www.domain.com/xxx&quot; id=&quot;CSRF&quot; method=&quot;post&quot;&gt; &lt;input name=&quot;comment&quot; value=&quot;attack&quot; type=&quot;hidden&quot;&gt;&lt;/form&gt; 如何防御验证 HTTP Referer 字段根据 HTTP 协议，在 HTTP 头中有一个字段叫 Referer，它记录了该 HTTP 请求的来源地址。在通常情况下，访问一个安全受限页面的请求来自于同一个网站，比如需要访问 http://bank.example/withdraw?account=bob&amp;amount=1000000&amp;for=Mallory，用户必须先登陆 bank.example，然后通过点击页面上的按钮来触发转账事件。因此，要防御 CSRF 攻击，网站只需要对于每一个转账请求验证其 Referer 值，如果是以 bank.example 开头的域名，则说明该请求是来自银行网站自己的请求，是合法的。如果 Referer 是其他网站的话，则有可能是黑客的 CSRF 攻击，拒绝该请求。这种方法的显而易见的好处就是简单易行，网站的普通开发人员不需要操心 CSRF 的漏洞，只需要在最后给所有安全敏感的请求统一增加一个拦截器来检查 Referer 的值就可以。然而，这种方法并非万无一失。Referer 的值是由浏览器提供的，虽然 HTTP 协议上有明确的要求，但是每个浏览器对于 Referer 的具体实现可能有差别，并不能保证浏览器自身没有安全漏洞。使用验证 Referer 值的方法，就是把安全性都依赖于第三方（即浏览器）来保障，从理论上来讲，这样并不安全。事实上，对于某些浏览器，比如IE6 或 FF2，目前已经有一些方法可以篡改 Referer 值。如果 网站支持IE6 浏览器，黑客完全可以把用户浏览器的 Referer 值设为以 bank.example 域名开头的地址，这样就可以通过验证，从而进行 CSRF 攻击。即便是使用最新的浏览器，黑客无法篡改 Referer 值，这种方法仍然有问题。因为 Referer 值会记录下用户的访问来源，有些用户认为这样会侵犯到他们自己的隐私权，特别是有些组织担心 Referer 值会把组织内网中的某些信息泄露到外网中。因此，用户自己可以设置浏览器使其在发送请求时不再提供 Referer。当他们正常访问银行网站时，网站会因为请求没有 Referer 值而认为是 CSRF 攻击，拒绝合法用户的访问。 在请求地址中添加 token 并验证CSRF 攻击之所以能够成功，是因为黑客可以完全伪造用户的请求，该请求中所有的用户验证信息都是存在于 cookie 中，因此黑客可以在不知道这些验证信息的情况下直接利用用户自己的 cookie 来通过安全验证。要抵御 CSRF，关键在于在请求中放入黑客所不能伪造的信息，并且该信息不存在于 cookie 之中。可以在 HTTP 请求中以参数的形式加入一个随机产生的 token，并在服务器端建立一个拦截器来验证这个 token，如果请求中没有 token 或者 token 内容不正确，则认为可能是 CSRF 攻击而拒绝该请求。这种方法要比检查 Referer 要安全一些，token 可以在用户登陆后产生并放于 session 之中，然后在每次请求时把 token 从 session 中拿出，与请求中的 token 进行比对，但这种方法的难点在于如何把 token 以参数的形式加入请求。对于 GET 请求，token 将附在请求地址之后，这样 URL 就变成 http://url?csrftoken=tokenvalue。 而对于 POST 请求来说，要在 form 的最后加上 ，这样就把 token 以参数的形式加入请求了。但是，在一个网站中，可以接受请求的地方非常多，要对于每一个请求都加上 token 是很麻烦的，并且很容易漏掉，通常使用的方法就是在每次页面加载时，使用 javascript 遍历整个 dom 树，对于 dom 中所有的 a 和 form 标签后加入 token。这样可以解决大部分的请求，但是对于在页面加载之后动态生成的 html 代码，这种方法就没有作用，还需要程序员在编码时手动添加 token。该方法还有一个缺点是难以保证 token 本身的安全。特别是在一些论坛之类支持用户自己发表内容的网站，黑客可以在上面发布自己个人网站的地址。由于系统也会在这个地址后面加上 token，黑客可以在自己的网站上得到这个 token，并马上就可以发动 CSRF 攻击。为了避免这一点，系统可以在添加 token 的时候增加一个判断，如果这个链接是链到自己本站的，就在后面添加 token，如果是通向外网则不加。不过，即使这个 csrftoken 不以参数的形式附加在请求之中，黑客的网站也同样可以通过 Referer 来得到这个 token 值以发动 CSRF 攻击。这也是一些用户喜欢手动关闭浏览器 Referer 功能的原因。 在 HTTP 头中自定义属性并验证这种方法也是使用 token 并进行验证，和上一种方法不同的是，这里并不是把 token 以参数的形式置于 HTTP 请求之中，而是把它放到 HTTP 头中自定义的属性里。通过 XMLHttpRequest 这个类，可以一次性给所有该类请求加上 csrftoken 这个 HTTP 头属性，并把 token 值放入其中。这样解决了上种方法在请求中加入 token 的不便，同时，通过 XMLHttpRequest 请求的地址不会被记录到浏览器的地址栏，也不用担心 token 会透过 Referer 泄露到其他网站中去。然而这种方法的局限性非常大。XMLHttpRequest 请求通常用于 Ajax 方法中对于页面局部的异步刷新，并非所有的请求都适合用这个类来发起，而且通过该类请求得到的页面不能被浏览器所记录下，从而进行前进，后退，刷新，收藏等操作，给用户带来不便。另外，对于没有进行 CSRF 防护的遗留系统来说，要采用这种方法来进行防护，要把所有请求都改为 XMLHttpRequest 请求，这样几乎是要重写整个网站，这代价无疑是不能接受的。]]></content>
      <categories>
        <category>Security</category>
      </categories>
      <tags>
        <tag>CSRF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSRF Protection in Rails ( ActionController::RequestForgeryProtection )]]></title>
    <url>%2Fsecurity-ROR-CSRF%2F</url>
    <content type="text"><![CDATA[The BasicsThere are two components to CSRF. First, a unique token is embedded in your site’s HTML. That same token is also stored in the session cookie. When a user makes a POST request, the CSRF token from the HTML gets sent with that request. Rails compares the token from the page with the token from the session cookie to ensure they match. Generation encrypted token and embedded into form Token generation(raw token) &amp; Store into session123456#actionpack/lib/action_controller/metal/request_forgery_protection.rbdef real_csrf_token(session) # :doc: session[:_csrf_token] ||= SecureRandom.base64(AUTHENTICITY_TOKEN_LENGTH) Base64.strict_decode64(session[:_csrf_token])end Token encryption123456789101112def masked_authenticity_token(session, form_options: &#123;&#125;) # :doc: # ... raw_token = if per_form_csrf_tokens &amp;&amp; action &amp;&amp; method # ... else real_csrf_token(session) end one_time_pad = SecureRandom.random_bytes(AUTHENTICITY_TOKEN_LENGTH) encrypted_csrf_token = xor_byte_strings(one_time_pad, raw_token) masked_token = one_time_pad + encrypted_csrf_token Base64.strict_encode64(masked_token)end Embeded into form12345678def csrf_meta_tags if protect_against_forgery? [ tag("meta", name: "csrf-param", content: request_forgery_protection_token), tag("meta", name: "csrf-token", content: form_authenticity_token) ].join("\n").html_safe endend then you’ll see: In the post params, get authenticity_token &amp; decryption token and verifyGet masked_token1masked_token = Base64.strict_decode64(encoded_masked_token) Get unmasked_token12345def unmask_token(masked_token) # :doc: one_time_pad = masked_token[0...AUTHENTICITY_TOKEN_LENGTH] encrypted_csrf_token = masked_token[AUTHENTICITY_TOKEN_LENGTH..-1] xor_byte_strings(one_time_pad, encrypted_csrf_token)end Verify123def compare_with_real_token(token, session) # :doc: ActiveSupport::SecurityUtils.secure_compare(token, real_csrf_token(session))end]]></content>
      <categories>
        <category>ROR</category>
      </categories>
      <tags>
        <tag>CSRF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么scripts, styles, and images不遵循Same-Origin Policy]]></title>
    <url>%2Ftags-SOP%2F</url>
    <content type="text"><![CDATA[GET 请求没有副作用并且页面不能直接读取结果 Sending a GET request (even with cookies) is not harmful; GET requests are not supposed to have any side-effects.None of these tags allow the calling page to (directly) read responses, so they don’t leak information.Therefore, there is (almost) nothing wrong with allow it. 由于历史原因需要兼容之前的网站 The reason is because of legacy. It was built that way many years ago and if it changes now, too many sites will fail. Plus the security implications are well known, since it has been around for so long. 如果都遵循SOP构建网站过于艰难 In short: it would be much harder to build the Web if everything was subject to SOP]]></content>
      <categories>
        <category>JS</category>
      </categories>
      <tags>
        <tag>SOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LCA 多次询问 解法总结]]></title>
    <url>%2FLCA%2F</url>
    <content type="text"><![CDATA[题目N个节点，M次询问，求两点间的最近公共祖先 并查集+DFS（也叫Tarjan）—— 离线O(M+N)每个节点(比如为x)运行完之后就将x的父指针指向它的父亲(这时父亲节点的父指针依然指向自己)，然后再去运行x的兄弟节点，这时兄弟节点下的某个节点(比如y)如果在查询中，且查询如果恰好是(y, x的子孙)，则x所在并查集树中的根节点一定是x的父节点，而这个父节点也是y的祖先，因此可知(y, x的子孙)的祖先一定包含x的父节点，由上面过程知道不能可包含比x的父节点更低的祖先节点，因此x的祖先节点必然是(y, x的子孙)的最近公共祖先)；123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include &lt;cstdio&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;using namespace std;#define N 10100vector &lt;int&gt; a[N];int vis[N],fa[N],In[N];int l,r;void ini()&#123; for(int i = 0; i &lt; N; i++)a[i].clear();//存树 for(int i = 0; i &lt; N; i++)vis[i]=0;//存是否被处理过 for(int i = 0; i &lt; N; i++)fa[i]=i; for(int i = 0; i &lt; N; i++)In[i]=0;//存每个节点的入度&#125;int get_fa(int x)&#123; if(x==fa[x])return x; return fa[x]=get_fa(fa[x]);&#125;void LCA(int u)&#123; int len=a[u].size(); for(int i = 0; i &lt; len; i++) &#123; int v=a[u][i]; LCA(v); fa[v]=u; &#125; vis[u]=1; if((l==u&amp;&amp;vis[r])) //此时l,r不在以u为根的子树上，且r那颗子树已经处理完毕，fa[r所在子树的根]=l,r共同的祖先（已经赋过值），所以get_fa(r); printf("%d\n",get_fa(r)); if(r==u&amp;&amp;vis[l]) printf("%d\n",get_fa(l));//同理 //printf("haha\n");&#125;int main()&#123; int t;scanf("%d",&amp;t); while(t--) &#123; int n;scanf("%d",&amp;n); ini(); for(int i = 1; i &lt; n; i++) &#123; int x,y; scanf("%d%d",&amp;x,&amp;y); a[x].push_back(y); In[y]++; &#125; scanf("%d%d",&amp;l,&amp;r); // printf("haha\n"); for(int i = 1; i &lt;= n; i++) if(!In[i])LCA(i); &#125; return 0;&#125; 裸RMQ——–在线（N(log N)预处理，每次 log N 查询） O（N(log N)+M(log N)）设P[i][j]表示结点i往上移动2^j步所到达的结点，P[i][j]可以通过以下递推公式计算：利用P数组可以快速的将结点i向上移动n步，方法是将n表示为2进制数。比如n=6，二进制为110，那么利用P数组先向上移动4步(2^2)，然后再继续移动2步(2^1)，即P[ P[i][2] ][1]。 预处理计算P数组代码如下12345678910111213141516171819202122232425map&lt;TreeNode*, int&gt; nodeToId;map&lt;int, TreeNode*&gt; idToNode;const int MAXLOGN=20; //树中最大结点数为1&lt;&lt;20int P[1 &lt;&lt; MAXLOGN][MAXLOGN];//allNodes存放树中所有的结点void preProcessTree(vector&lt;TreeNode *&gt; allNodes) &#123; int n = allNodes.size(); // 初始化P中所有元素为-1 for (int i = 0; i &lt; n; i++) for (int j = 0; 1 &lt;&lt; j &lt; n; j++) P[i][j] = -1; for (int i = 0; i &lt; n; i++) &#123; nodeToId[allNodes[i]] = i; idToNode[i] = allNodes[i]; &#125; // P[i][0]=parent(i) for (int i = 0; i &lt; n; i++) P[i][0] = allNodes[i]-&gt;parent ? nodeToId[allNodes[i]-&gt;parent] : -1; // 计算P[i][j] for (int j = 1; 1 &lt;&lt; j &lt; n; j++) for (int i = 0; i &lt; n; i++) if (P[i][j] != -1) P[i][j] = P[P[i][j - 1]][j - 1];&#125;另外我们还需要预处理计算出每个结点的深度L[]，预处理之后，查询node1和node2的LCA算法如下12345678910111213141516171819TreeNode* getLCA(TreeNode *node1, TreeNode *node2, int L[]) &#123; int id1 = nodeToId[node1], id2 = nodeToId[node2]; //如果node2的深度比node1深，那么交换node1和node2 if (L[id1] &lt; L[id2]) swap(id1, id2); //计算[log(L[id1])] int log; for (log = 1; 1 &lt;&lt; log &lt;= L[id1]; log++); log--; //将node1向上移动L[id1]-L[id2]步，使得node1和node2在同一深度上 for (int i = log; i &gt;= 0; i--) if (L[id1] - (1 &lt;&lt; i) &gt;= L[id2]) id1 = P[id1][i]; if (id1 == id2) return idToNode[id1]; //使用P数组计算LCA(idToNode[id1], idToNode[id2]) for (i = log; i &gt;= 0; i--) if (P[id1][i] != -1 &amp;&amp; P[id1][i] != P[id2][i]) id1 = P[id1][i], id2 = P[id2][i]; return idToNode[id1];&#125; 时间复杂度分析：假设树包含n个结点，由于P数组有nlogn个值需要计算，因此预处理的时间复杂度为O(nlogn)。查询两个结点的LCA时，函数getLCA中两个循环最多执行2logn次，因此查询的时间复杂度为O(logn) LCA转化为RMQ——–在线（N(log N)预处理，每次 O(1) 查询） O（N(log N)+M）对于有根树T的两个结点u、v，最近公共祖先LCA(T,u,v)表示一个结点x，满足x是u、v的祖先且x的深度尽可能大。另一种理解方式是把T理解为一个无向无环图，而LCA(T,u,v)即u到v的最短路上深度最小的点。这里给出一个LCA的例子：例一对于T=&lt;V,E&gt;V={1,2,3,4,5}E={(1,2),(1,3),(3,4),(3,5)}则有：LCA(T,5,2)=1LCA(T,3,4)=3LCA(T,4,5)=3 RMQ问题与LCA问题的关系紧密，可以相互转换，相应的求解算法也有异曲同工之妙。下面给出LCA问题向RMQ问题的转化方法。 对树进行深度优先遍历，每当“进入”或回溯到某个结点时，将这个结点的编号存入数组E最后一位。同时记录结点i在数组中第一次出现的位置(事实上就是进入结点i时记录的位置)，记做R[i]。如果结点E[i]的深度记做D[i]，易见，这时求LCA(T,u,v)，就等价于求E[RMQ(D,R[u],R [v])]，(R[u]&lt;R[v])，其中RMQ(D,R[u],R [v])就是在D数组中求下标从R[u]到R[v]的最小值的下标。例如一，求解步骤如下： 数列E[i]为：1,2,1,3,4,3,5,3,1 R[i]为：1,2,4,5,7 D[i]为：0,1,0,1,2,1,2,1,0 于是有： LCA(T,5,2) = E[RMQ(D,R[2],R[5])] = E[RMQ(D,2,7)] = E[3] = 1 LCA(T,3,4) = E[RMQ(D,R[3],R[4])] = E[RMQ(D,4,5)] = E[4] = 3 LCA(T,4,5) = E[RMQ(D,R[4],R[5])] = E[RMQ(D,5,7)] = E[6] = 3 易知，转化后得到的数列长度为树的结点数的两倍减一， 所以转化后的RMQ问题与LCA问题的规模同次 再举一个例子帮助理解: (1) / \ (2) (7) / \ \(3) (4) (8) / \(5) (6)一个nlogn 预处理，O(1)查询的算法. Step 1: 按先序遍历整棵树，记下两个信息:结点访问顺序和结点深度. 如上图: 结点访问顺序是: 1 2 3 2 4 5 4 6 4 2 1 7 8 7 1 //共2n-1个值 结点对应深度是: 0 1 2 1 2 3 2 3 2 1 0 1 2 1 0 Step 2: 如果查询结点3与结点6的公共祖先,则考虑在访问顺序中 3第一次出现，到6第一次出现的子序列: 3 2 4 5 4 6. 这显然是由结点3到结点6的一条路径. 在这条路径中，深度最小的就是最近公共祖先(LCA). 即 结点2是3和6的LCA. Step 3: 于是问题转化为, 给定一个数组R,及两个数字i,j,如何找出 数组R中从i位置到j位置的最小值.. 如上例,就是R[]={0,1,2,1,2,3,2,3,2,1,0,1,2,1,0}. i=2;j=7; 接下来就是经典的RMQ问题.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;cmath&gt;#include &lt;algorithm&gt;using namespace std;#define maxn 100010int cnt,id;int head[maxn];bool vis[maxn];int dep[maxn*2+1], E[maxn*2+1], R[maxn];//R表示节点第一次出现的位置；dep表示时间戳为i时的深度；E表示时间戳为i时的节点int f[maxn*2+1][20],d[50]; //f[] is RMQ, d[i] is 2^istruct Edge&#123; int to,next,weight;&#125;edges[maxn]; //邻接表void init()&#123; cnt=id=0; memset(vis,false,sizeof(vis)); memset(head,-1,sizeof(head));&#125; void insert(int a, int b, int weight)&#123; edges[cnt].to=b; edges[cnt].next=head[a]; edges[cnt].weight=weight; head[a]=cnt++;&#125;void DFS(int u, int d)&#123; vis[u]=1; R[u]=id;E[id]=u;dep[id++]=d; for(int i = head[u]; i != -1; i=edges[i].next) &#123; int v=edges[i].to; if(!vis[v]) &#123; DFS(v,d+1); E[id]=u;dep[id++]=d; &#125; &#125;&#125;void InitRMQ(const int &amp;id,int n)&#123; d[0]=1; for(int i = 1; i &lt; n; i++)d[i]=2*d[i-1]; for(int i = 0; i &lt; id; i++)f[i][0]=i; int k=int(log(double(n))/log(2.0))+1; for(int j = 1; j &lt; k; j++) for(int i = 0; i &lt; id; i++) &#123; if(i+d[j-1]-1&lt;id) f[i][j]=dep[f[i][j-1]]&gt;dep[f[i+d[j-1]][j-1]]?f[i+d[j-1]][j-1]:f[i][j-1]; else break; &#125;&#125;int Query(int x, int y)&#123; int k; k=int(log(double(y-x+1))/log(2.0)); return dep[f[x][k]]&gt;dep[f[y-d[k]+1][k]]?f[y-d[k]+1][k]:f[x][k];&#125;void Answer()&#123; int Q;scanf("%d",&amp;Q); for(int i = 0; i &lt; Q; i++) &#123; int x,y; scanf("%d%d",&amp;x,&amp;y); //查询x,y的LCA x=R[x];y=R[y]; if(x&gt;y)swap(x,y); printf("%d\n",E[Query(x,y)]); &#125;&#125;int main()&#123; int t;scanf("%d",&amp;t); while(t--) &#123; init(); int n,m;scanf("%d%d",&amp;n,&amp;m); for(int i = 0; i &lt; m; i++) &#123; int a,b,c;scanf("%d%d%d",&amp;a,&amp;b,&amp;c); insert(a,b,c); &#125; DFS(1,0); InitRMQ(id,n); Answer(); &#125; return 0;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>LCA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Suffix Tree 后缀数组]]></title>
    <url>%2Fsuffix-tree%2F</url>
    <content type="text"><![CDATA[前言这是13年3月28日的文章，那时的我在机房熬了一天一夜翻来覆去看罗穗骞的代码，最后才把这个后缀数组给整明白了，故有此文现在的我回想那过去的岁月，难免唏嘘不已，今时今日借整理此文，以缅怀那过去的岁月吧！原文链接 声明我的模板是根据 罗穗骞 和 网上一模板 相结合改编而来，层次更加分明，数组名称的选择是根据用途来定义的，总的来说应该更好理解一些先声明一些概念： k-后缀数组：我这里用的是 sa[ ] ( k ) k-名次数组：我这里用的是 rank[ ] ( k ) 思路知道rank[ ] (1) 先求出sa[ ] (1)，然后根据 sa[ ] (1) 和 rank[ ] (1) 调用sorting函数求出 sa[ ] (2)，再求出rank[ ] (2) 等等 总的过程是rank[ ] (1)—-&gt;sa[ ] (1) —&gt;rank[ ] (2) —&gt; sa[ ] (2) —&gt; rank[ ] (k) —-&gt;sa[ ] (k) Hint 我的数组小标全是从1开始的，程序运行前一定要初始化函数ini() 其它该有的东西都在代码里，建议一边看代码下面的图，一边理解！ 模板代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;cmath&gt;using namespace std;#define N 3000//处理完成之后下标都是从1开始char s[N];//存原始数据int sum[N], rank[2*N], trank[2*N];//sum用来基数排序；trank === temp-rankint sa[N], tsa[N]; //这里tsa[]用来保存第二关键字排序后的结果int Top, n; //Top基数排序出现的极大值void ini() //初始化&#123; n=strlen(s); memset(sum,0,sizeof(sum)); memset(rank,0,sizeof(rank)); memset(sa,0,sizeof(sa)); Top=128;&#125;void sorting(int k)&#123; int p = 1; //利用sa排序第二关键字，求出tsa for(int i = n-k+1; i &lt;= n; i++) tsa[p++]=i; //超出范围的下标按下标顺序置其名次为最低 for(int i = 1; i &lt;= n; i++) if(sa[i]&gt;=k+1) tsa[p++]=sa[i]-k; //按sa排名从小到大遍历，如果排名为i的某后缀(sa[i])位置在k后面，那么这个后缀肯定会在第二关键字排序 //(tsa[])中出现并且排在比较靠前的位置，这个后缀对应tsa[]中的位置为sa[i]-k memset(sum,0,sizeof(sum)); for(int i = 1; i &lt;= n; i++)sum[rank[i]]++; for(int i = 1; i &lt;= Top; i++)sum[i]+=sum[i-1]; for(int i = n; i &gt; 0; i--)sa[sum[rank[tsa[i]]]--]=tsa[i];//这句话应该是关键 //如果把tsa[i]换成i，那么就是对rank[i]按第一关键字排序求sa[]，事实上这段程序也是这样， //不过把i换成tsa[i]后会多一个功能，就是按第一关键字排序rank相同的情况下，会按第二关键字排， //这也正符合我们最终要求的sa[](2k)。具体方法是从tsa[n]到tsa[1]，让在第二关键字排名靠后的优先 //取sum较大的，取一次，sum--，如果后边第二关键字排名靠前的某后缀在第一关键字下rank和它相同， //那么它的sum就较小，在sa中排名就会靠前，服从排序规律！&#125;void get_sa()&#123; int p; for(int i = 0; i &lt; n; i++) rank[i+1]=s[i]; //仔细想一下，其实此时rank就是rank[](1); for(int i = 1; i &lt;= n; i++) sum[rank[i]]++; for(int i = 1; i &lt;= Top; i++) sum[i]+=sum[i-1]; //sum[i] means the number of the &lt;= rank[i]; for(int i = n; i &gt; 0; i--) sa[sum[rank[i]]--]=i; //sa[](1)构造完成 for(int k = 1; k &lt;= n; k&lt;&lt;=1) &#123; sorting(k); //由sa[](k)和rank[](k) 求 sa[](2k) //求rank[](2k) trank[sa[1]]=1; p=1; for(int i = 2; i &lt;= n; i++) &#123; if((rank[sa[i]]!=rank[sa[i-1]])||(rank[sa[i]+k]!=rank[sa[i-1]+k]))p++; trank[sa[i]]=p; &#125; for(int i = 1; i &lt;= n; i++)rank[i]=trank[i]; if(p&gt;=n)break; //rank[1,2……n]已经唯一了，即后缀大小已经唯一确定了，不需要继续执行了 Top=p;//下次基数排序的最大值 &#125;&#125;int height[N];void get_height()&#123; for(int i = 1, j = 0; i &lt;= n; i++) &#123; if(rank[i]==1)continue; for(;s[i+j-1]==s[sa[rank[i]-1]+j-1];)j++;//i从1开始，所以在原串中要-1 height[rank[i]]=j; if(j&gt;0)j--; &#125;&#125; int *RMQ=height;int mm[N];int best[20][N];void initRMQ(int n)&#123; int i,j,a,b; for(mm[0]=-1,i=1; i&lt;=n; i++) mm[i]=((i&amp;(i-1))==0)?mm[i-1]+1:mm[i-1]; for(i=1; i&lt;=n; i++)best[0][i]=i; for(i=1; i&lt;=mm[n]; i++)for(j=1; j&lt;=n+1-(1&lt;&lt;i); j++) &#123; a=best[i-1][j]; b=best[i-1][j+(1&lt;&lt;(i-1))]; if(RMQ[a]&lt;RMQ[b])best[i][j]=a; else best[i][j]=b; &#125;&#125;int askRMQ(int a,int b)&#123; int t; t=mm[b-a+1]; b-=(1&lt;&lt;t)-1; a=best[t][a]; b=best[t][b]; return RMQ[a]&lt;RMQ[b]?a:b;&#125;//求sufix(a)与sufix(b)的最长公共前缀长度，用上面的RMQ优化int lcp(int a,int b) //这里的a,b是字符串当中的位置，注意要从1开始&#123; int t; a=rank[a],b=rank[b]; if(a&gt;b) &#123; t=a; a=b; b=t; &#125; return height[askRMQ(a+1,b)];&#125; h[i] &gt;= h[i-1]-1的证明设suffix(k)是排在suffix(i-1)前一位的后缀，则它们的最长公共前缀显然是h[i-1]。那么，suffix(k+1)显然将排在suffix(i)的前面。并且，suffix(k+1)&amp;suffix(i) 相对于 suffix(k)&amp;suffix(i-1)来说就是同时去掉了第一位，即少了一位的匹配数。所以suffix(i)和前一名次后缀的最长公共前缀至少是h[i-1]-1 为什么是 至少h[i-1]-1的理解根据上面的证明我们可以得到suffix(k+1)&amp;suffix(i) == h[i-1]-1又因为suffix(k+1)显然将排在suffix(i)的前面所以 LCP(k+1,i) = h[i-1]-1 =min{ LCP(j-1, j) |k+1 ≤j ≤i } （LCP Theorem）&lt;= h[i]@(数据结构)]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Suffix Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博弈进行时]]></title>
    <url>%2Fgame-theory%2F</url>
    <content type="text"><![CDATA[前言这是 2012年12月04日 00:13:42 发表的博客，那时的我初生牛犊不怕虎，天天都在做final梦，在数据结构、动态规划还没整明白的情况下贸然开始搞博弈，实乃too simple。博弈现在基本上是忘到爪哇岛去了，投入的精力不知道还剩几斤几两。不过想想当年，确实挺热血的嘛！ 小知识点HDOJ2176 取(m堆)石子游戏 [输出第一步走法] 若给出的是必胜状态：a1^a2^…….^an=k,(其中k不为零)，那么我们的目的是要把必胜状态转化为必败状态从 而使得先手胜利。 若a1^a2^…^an!=0，一定存在某个合法的移动，将ai改变成ai’后满足a1^a2^…^ai’^…^an=0。 若a1^a2^…^an=k，则一定存在某个ai，它的二进制 表示在k的最高位上是1（否则k的最高位那个1是怎么得到的）。这时ai^k&lt;ai一定成立。 则我们可以将ai改变成ai’=ai^k，此时a1^a2^…^ai’^…^an=a1^a2^…^an^k=0 HDOJ1907&amp;&amp;POJ3480&amp;ZOJ3113 John [ANTI-SG]必胜策略： sg值为0 且 全为孤单堆； sg值不为0 且 不全为孤单堆； NP暴力打表||找规律HDOJ1525&amp;POJ2348 Euclid’s Game [找规律博弈]注意第一次出现a/b&gt;1的位置为必胜 HDOJ1564 Play a game [找规律]（重点是打表的方法，N P定理）打表发现奇偶性规律 HDOJ1404 Digital Deletions [SG博弈]（暴力打表，NP定理）老实说，这个表不太好打。从P-&gt;N一种是在让某一位增加；另一种是在后面补0 裸求sg值HDOJ3032 Nim or not Nim? [找规律]SG打表重点是sg函数的求法，把一堆变成两堆，有嵌套的感觉。 HDOJ1729 Stone Game [SG博弈]这题我开始打了个表，，什么规律都没找出来，还是得靠分析，，可我不会啊。。。。 我的理解：可以先模拟一下，初始为（27,2），子递归状态为（4,2）然后返回2，结束。然后我从前往后推，A先拿2个，变为（27,4）状态，然后无论B怎么拿都无法拿完，所以B很保守的只拿一个，变为（27,5）状态，然后A轻松拿完；所以A获胜了。所以我的理解是子递归状态返回值的意义在于 先手能否成功到达必败点，让后手必败。 那么对于这题对于每一堆，放石子放满就想当于满的时候取s-c个，反向只是让我理解题意更深。 首先我们知道（S，S）这个局面是必败局面。 对于每一堆能加的数量有限，而当c的值（大于或者等于）D=sqrt(s) 或者 D=sqri（s）+1的时候就可以一次完成，就是说可以从当前局面到达（S，S）的局面，所以当前局面是必胜局面。 而这种情况下，你能造成的局面有集合A={0,1,2,…,s-c-1}；因为你可以去s-c，s-c-1，s-c-2，…..，1；那么对应mex(x)函数（即A中未出现的最小的一个数字），那么自然该局面的SG值就是s-c了； 另外当c的值小于D的时候，是不可能一下子加满的，因为c*c+c绝对是小于s的；那么小于D的局面一定能够是必输的吗？很显然不是的。 对于（S，D-1）这个局面，一定是必输，因为他能到的局面都是必胜！现在c小于D，那么如果（S，C）这个局面能到（S，D）；就代表这个局面是必胜的。所以现在SG值要在新集合（D，C）中求，而求法与上面的相同求新的D，所以可以用递归函数：当C&gt;D时，返回（S-C） 差不多就是这样。 其实D = sqrt(s);这里算是个加速，要不然就要：while(d*d+d &lt; S) d++;这样会很慢的。 思路：这题明显的sg函数。可惜我纠结了半天没想起思路来。设当前的箱子容量为si，求出一个t满足：t + t * t &lt; si，若是当前箱子里有ci颗石头， ci &gt; t 则必胜； ci == t 则必败； ci &lt; t不可立即断定，将t作为si递归调用函数。 当满足ci &gt; t时，return si - ci 作为当前状况的sg值。因为： 当ci在si点时，为有向图的端点，出度为0，也就是必败点，所以sg值为0； 当ci 位于si - 1时，ci的端点可能的sg值构成的凑集为{0}，所以当前sg值 为1； 当ci 位于si - 2 时，ci的端点可能的sg值构成的凑集为{0， 1}，所以当前的sg值为2； 可得，ci地点地位的sg值为si - ci； [TopCoder]SRM 561 DIV1 500：CirclesGame题意：给你n&lt;=50个不相交的圆（可能包含）。Alice 和Bob博弈，Alice先手，每次每人都可以选择一个没有红点的圆，在圆内放置一个红点，最后没法放红点的一方失败。 算法分析：圆与圆没有相交，故可以把每个圆看成一个结点，直接包含看成点与点之间的边，于是得到一个森林。原题变成在这个森林里博弈，每次选取一个结点，可以干掉他和他的所有祖先结点，干掉最后一个点的获胜。首先我们来考虑每颗树的sg函数。我们可以选择这棵树上任意一个结点操作，去掉这个结点以及其祖先结点后剩下的树的sg函数取异或便是当前操作所得状态的sg值，去掉这些sg值，剩下sg值的最小值即为这棵树的sg函数（看不懂的先好好学学博弈论）。求出每颗树的sg函数，最后取异或便是最终的sg函数，结果不等于0先手胜否则后手胜。 （这题不太好写，子状态神马递归容易出错） starcaseHDOJ3389 Game [Staircase Nim]变形要注意这类博弈的特点：一定是奇堆移动到偶堆，偶堆移动到奇堆。 题意：有N堆石子，每堆石子都有一定的石子数，Alice和Bob轮流玩一个游戏，游戏的规则是，每回合一个人可以选择1-n堆石子中的某一堆进行操作，操作是：假设选择操作的那堆石子的编号为A，现在还要选择一堆石子B，满足B&lt;A &amp;&amp; (A+B)%2==1 &amp;&amp; (A+B)%3==0，然后可以将A中至少一颗石子移到B中去，第一个不能进行合法操作的人输，问谁能赢。 思路：这是一个阶梯博弈的题目，首先我们可以发现，只有1 ，3 ，4 三个数是没有前缀的， 也就是terminal状态，其余的每个状态我们都可以计算出每个编号到这几个terminal的步数（当然有的编号的步数并不唯一，但是奇偶性是唯一的）。接下去我们就会发现，每次从一个奇数步的点，一定是要移到到 一个偶数步的点上去，也就是说每次只能移到奇数步，这个通过编号自身的奇偶性就可以证明了。这样就转化为了阶梯博弈的类型了，我们只需要关注奇数步编号出的石子的数量就可以了，如果奇数步编号处的石子的Nim和为0，则必败，不为0则必胜。接下去我们就简单地证明一下这个策略：如果移动的是奇数步位置的石子，则一定是移动到偶数步的位置，这时候我们只需要按照Nim博弈的必胜策略进行游戏就可以赢；如果移动的是偶数步数编号位置的石子，则移动是移动到奇数步的位置，这时候我们只需要将刚刚移动过来的石子移动到下一个偶数步位置，原来的Nim局面并没有变化，变化的只是偶数步石子的数量。这样我们就证明了原游戏可以转化为Nim游戏。 POJ1704 Georgia and Bob [阶梯博弈] (下面一题是这个的加强版，直接把这种最纯粹的看做模型)我们把棋子按位置升序排列后，从后往前把他们两两绑定成一对。如果总个数是奇数，就把最前面一个和边界（位置为0）绑定。在同一对棋子中，如果对手移动前一个，你总能对后一个移动相同的步数，所以一对棋子的前一个和前一对棋子的后一个之间有多少个空位置对最终的结果是没有影响的。于是我们只需要考虑同一对的两个棋子之间有多少空位。这样一来就成了N堆取石子游戏了. HDOJ4315 Climbing the Hill [阶梯博弈] （有待提高）这题压力山大，看了好久： 此题的简化版本是不考虑King的存在，双方一直走到不能走的一方为负。此时的解法是根据人数的奇偶性：把人从上顶向下的位置记为a1,a2,…an, 如果为偶数个人，则把a(2i-1)和a(2i)之间的距离当做一个Nim堆，变成一共n/2堆的Nim游戏；如果为奇数个人，则把山顶到a1的距离当做一个Nim堆，a(i2)到a(i2+1)的距离当做Nim堆，一共(n+1)/2堆。 考虑King的情况和上述版本几乎一致，只要把King当作普通人一样处理即可。除了两种特殊情况：1. 当King是第一个人时，Alice直接胜 2. 当King是第二个人且一共有奇数个人时，第一堆的大小需要减1。 题意如上图所示：有 n 个球分别在 n 个不同的位置，Alice 和 Bob 依次选择一个球向上移动，上面有球不能越过，谁最后把红球移出谁就赢！ 分析1、n 为偶数时：问题简化一下，假设全都是黄球，谁把最后一个球移出谁就赢（a1,a2） (a3,a4) …… ( a(2n-1) , a(2n) ) ……（a(n-1),an）其中第 i 个球与第 i+1 个球是相邻的，i 为基数，谁面对这个状态谁就必输。理由很简单，先手移动第 i 个球，后手移动第 i+1 个球，使之仍然保持必赢状态。回到原问题谁先移出红球谁就赢，假设红球不是第一个球（因为第一个球Alice直接就赢了）很显然如果红球在偶数位置后手必赢，如果在基数 i 位置，则只需将 第 i-1 个球移到第一个位置就ok了。所以与红球位置无关。至于产生这个状态（a1,a2） (a3,a4) …… ( a(2n-1) , a(2n) ) ……（a(n-1),an），那么就是简单的 Nim问题了2、n 为基数时：假设红球是第1、2个球。（a1） (a2,a3)（a4,a5）…… (a(2n),a(2n+1)) …… (a(n-1),an) 谁面对这个状态必赢！理由是先手直接把 a1 取走然后就变成上面的情况了，如果红球在第 2 个位置那么就是必输状态。 与staircase的联系：对于这个题目，从后向前两两划分成一组，组内相当于奇数阶梯上的石子，组间相当于偶数阶梯上的石子，移动组内的前面石子一定能够通过移动当前组的后面石子相同步数达到平衡态，移动组内后面的石子一定能够通过移动其他组后面石子达到平衡态，具体因为King的原因需要处理细节，上面已经说清楚了，不再赘述。 翻硬币游戏HDOJ3537 Daizhenyang’s Coin[翻硬币游戏]SG打表 （待解决）这题表我是打出来了，规律没找出来（注意还要对编号排序判重） HDOJ3951 Coin Game [找规律]题意给你n个硬币排成一圈，编号1-n,只能翻转连续的1~k个的硬币。翻最后一枚硬币者赢。 思路博弈 若k=1,则一次只能去翻一枚，奇数先手赢，偶数后手赢。 若k&gt;1: 先手一次翻完，先手赢； 先手不能翻完，第一次必定断环。只要后手一次翻完，或将其分为相等数量的两段， 之后先手怎么操作后手就怎么操作，后手必赢。 树&amp;&amp;删边HDOJ3094 A tree game[有向无环树形图SG博弈]最基础的删边 HDOJ3197 Game[树形SG博弈]砍树树的删边游戏，把多棵树的根异或起来就行了 HDOJ3590 PP and QQ[树的删边游戏+ANTI-SG]先手必胜当且仅当 游戏的SG函数不为0且游戏中某个单一游戏的SG函数大于1 游戏的SG函数为0且游戏中没有单一游戏的SG函数大于1 POJ3710 Christmas Game[无向图删边]Tarjan算法找出环，处理环之后，便是经典的删边游戏。拥有奇数条边的环可简化为一条边，偶数条边的环可简化为一个节点。]]></content>
      <categories>
        <category>Game theory</category>
      </categories>
      <tags>
        <tag>Nim</tag>
        <tag>SG</tag>
      </tags>
  </entry>
</search>
